%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Grundlegende Anwendungsgebiete}
\label{trends} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\chapterauthor{Isabel Fritz, Felix Goos, Martin Maier, David Richard, Sabine Weigand,
Lola Bankai, Mareike Rennebaum, Siri Wandel, Hüma Yilmaz}

\abstract{some abstract}

\section{Quantum Machine Learning: Anwendungen und Perspektiven}
\subsection{Einordnung, Motivation und Relevanz}

\subsection{Technologischer Überblick}


\subsubsection{Technologische Grundlagen}

\noindent\textbf{1. Grundlagen des Maschinellen Lernens für QML}  

\noindent  
Maschinelles Lernen (ML) ist ein Teilgebiet der Künstlichen Intelligenz, das darauf abzielt, Computersysteme auf der Basis von Beispieldaten dazu zu befähigen, Muster zu erkennen, Vorhersagen zu treffen und Entscheidungen zu fällen, ohne dass dafür explizit programmierte Regeln erforderlich sind. Der zugrunde liegende Gedanke besteht darin, dass Algorithmen aus Erfahrung lernen – ein Prinzip, das insbesondere dann zur Anwendung kommt, wenn Probleme zu komplex sind, um sie analytisch zu modellieren, jedoch ausreichend Datenmaterial vorliegt. Ziel ist es, Modelle zu erzeugen, die aus bekannten Datenstrukturen generalisierbare Zusammenhänge ableiten und auf neue, unbekannte Daten anwendbar sind \cite{alpaydin2022,kreutzer2019}.  

Für den Einsatz maschinellen Lernens lassen sich zwei zentrale Lernformen unterscheiden: das überwachte Lernen (supervised learning) und das unüberwachte Lernen (unsupervised learning). Beim überwachten Lernen wird ein Modell auf Basis gelabelter Daten trainiert. Es lernt, Eingabemuster mit bekannten Ausgabewerten zu verknüpfen, und kann dieses Wissen auf neue Daten übertragen. Typische Anwendungen sind Klassifikations- oder Regressionsaufgaben. Im unüberwachten Lernen hingegen stehen keine Zielwerte zur Verfügung. Stattdessen versucht der Algorithmus, selbstständig Strukturen oder Cluster in den Daten zu identifizieren, etwa durch Ähnlichkeitsmaße oder Dimensionsreduktion \cite{weber2020}.  

Ein besonders bedeutender Teilbereich des ML ist das Deep Learning (DL). Es basiert auf mehrschichtigen künstlichen neuronalen Netzen, die in der Lage sind, komplexe und hierarchische Merkmale aus großen Datensätzen automatisch zu extrahieren. Der Begriff „deep“ bezieht sich dabei auf die Tiefe des Netzwerks, also die Anzahl der Verarbeitungsschichten. DL hat sich in vielen datenintensiven Anwendungsfeldern wie der Bild- und Spracherkennung als besonders leistungsfähig erwiesen. Viele aktuelle Entwicklungen im Quantum Machine Learning (QML) – etwa Quantum Neural Networks (QNN), Quantum Autoencoder oder Quantum Convolutional Networks – orientieren sich strukturell und funktional an Deep-Learning-Architekturen und übertragen deren Konzepte in den quantenmechanischen Kontext \cite{kreutzer2019}.  

Neben neuronalen Netzwerken existieren zudem klassische ML-Verfahren, die in der QML-Forschung als Vergleichsmodelle, für das Feature Engineering oder zur Vorverarbeitung verwendet werden. Support Vector Machines (SVM) gehören zu den wichtigsten überwachten Lernverfahren. Sie trennen Klassen durch eine optimal platzierte Hyperplane und maximieren dabei den Abstand zwischen den Klassen, was die Generalisierungsfähigkeit des Modells erhöht. SVMs sind besonders bei hochdimensionalen Daten effektiv und werden in der QML-Forschung durch Quantum Support Vector Machines (QSVM) erweitert \cite{janiesch2021}.  

Ein weiteres zentrales Verfahren ist K-Means, ein unüberwachter Lernalgorithmus zur Clusteranalyse. Er partitioniert die Daten in eine vorab festgelegte Anzahl $k$ von Gruppen, wobei jeder Datenpunkt dem nächstgelegenen Clusterzentrum zugeordnet wird. Die Clusterzentren werden iterativ angepasst, bis sich die Gruppenzugehörigkeit stabilisiert \cite{springer2021}.  

Schließlich ist die Principal Component Analysis (PCA) als Verfahren zur Dimensionsreduktion von besonderer Relevanz. PCA projiziert Daten auf wenige unkorrelierte Hauptachsen, welche die maximale Varianz erklären. Sie erleichtert damit sowohl die Visualisierung als auch die Verarbeitung komplexer Datensätze. In der QML-Forschung wird PCA häufig durch Quantum PCA (qPCA) ersetzt, um quantenspezifische Merkmale effizient zu extrahieren \cite{springer2021}.  

Diese klassischen Verfahren sind nicht nur für die Analyse und Vorverarbeitung von Daten im klassischen ML unerlässlich, sondern bilden auch methodische und konzeptionelle Grundlagen für viele QML-Architekturen. Clustering ist eine grundlegende Aufgabe im unüberwachten maschinellen Lernen, bei der Datenpunkte in Gruppen ähnlicher Eigenschaften eingeteilt werden. Der klassische K-Means Algorithmus ist dafür weit verbreitet, leidet jedoch bei großen Datensätzen unter zunehmender Rechenkomplexität. Eine quantenmechanische Erweiterung dieses Verfahrens stellt der \textit{q-means} Algorithmus dar, der auf effizienten quantenmechanischen Operationen zur Distanzmessung und Gruppenzuweisung basiert \cite{kerenidis2019}.  

Im Gegensatz zum klassischen Verfahren wird im q-means Algorithmus die Distanz zwischen einem Datenpunkt und einem Clusterzentrum nicht numerisch berechnet, sondern über die Überlappung quantenmechanischer Zustände geschätzt. Dies geschieht durch \textit{quantum state preparation}, bei der sowohl Datenpunkte als auch Clusterzentren in Amplituden kodiert werden, sowie durch sogenannte \textit{inner product estimation circuits}, die diese Überlappung bestimmen \cite{kerenidis2019}.  

Der q-means Algorithmus folgt dabei einem iterativen Dreischritt:  
\begin{enumerate}
  \item \textbf{Initialisierung:} Die Clusterzentren werden klassisch gewählt und in quantenmechanische Zustände überführt.
  \item \textbf{Zuweisungsschritt:} Datenpunkte werden jenen Zentren zugeordnet, zu denen ihre quantenmechanische Zustandsüberlappung (gemessene Distanz) minimal ist. Dies nutzt effiziente Quantenschaltkreise zur inneren Produktabschätzung.
  \item \textbf{Update-Schritt:} Die neuen Clusterzentren werden als gewichtete Mittelwerte neu berechnet – ebenfalls in quantenmechanischer Form.
\end{enumerate}  

Der q-means Algorithmus zeigt, wie sich klassische Clustering-Ansätze durch quantenmechanische Optimierung beschleunigen lassen. Die theoretische Effizienz basiert auf der Annahme eines \textit{Quantum Random Access Memory} (QRAM), das den parallelen Zugriff auf quantenkodierte Daten ermöglichen würde – eine Technologie, die derzeit jedoch noch nicht praktisch verfügbar ist. q-means gilt als konzeptioneller Vorläufer für weiterführende Verfahren wie \textit{quantum Principal Component Analysis} (qPCA) und quanteninspirierte Anomalieerkennung \cite{kerenidis2019}.  

\vspace{0.5cm}
\noindent\textbf{2. Datenkodierung und quantenmechanische Vorverarbeitung}  

\noindent
\textbf{Datenkodierung (State Preparation)}\\
Um klassische Daten mit Quantenalgorithmen verarbeiten zu können, müssen sie zunächst in Quantenzustände überführt werden – ein Prozess, der als \textit{State Preparation} bezeichnet wird. Anders als bei klassischen Algorithmen, bei denen Daten direkt in numerischer oder symbolischer Form eingehen, müssen Quantencomputer Informationen in den Amplituden, Phasen oder Basiszuständen von Qubits kodieren. Die Wahl der Kodierung beeinflusst dabei direkt den Ressourcenbedarf, die Trainingskomplexität und die Fehleranfälligkeit eines QML-Modells. Daher ist sie nicht nur ein Vorverarbeitungsschritt, sondern ein zentrales Designkriterium für jede QML-Anwendung \cite{schuld2018}.  

\vspace{0.3cm}
\noindent
\textbf{Kodierungsstrategien}  

\vspace{0.2cm}
\noindent
\textbf{Angle Encoding}\\
Beim \textit{Angle Encoding} werden einzelne Datenpunkte in Rotationswinkel für bestimmte Quantengates übersetzt – typischerweise über Rotationen um die $Y$- oder $Z$-Achse. Diese Methode ist besonders gut für aktuelle NISQ-Hardware geeignet, da sie nur flache Schaltungen erfordert und vergleichsweise robust gegenüber Rauschen ist. Sie ist daher in vielen aktuellen Demonstratoren der Standardansatz, auch wenn sie nicht speichereffizient ist: Jede Datenkomponente benötigt ihre eigene Qubit-Rotation \cite{schuld2018, bsi2025}.  

\vspace{0.2cm}
\noindent
\textbf{Amplitude Encoding}\\
\textit{Amplitude Encoding} speichert die Datenwerte direkt in den Amplituden eines Quantenzustands. Dabei lassen sich $2^n$ Datenpunkte theoretisch mit nur $n$ Qubits abbilden, was diese Methode extrem speichereffizient macht. In der Praxis ist sie jedoch technisch anspruchsvoll: Die initiale Zustandspräparation ist komplex, besonders bei verrauschten oder realitätsnahen Datensätzen. Die entstehende Schaltungstiefe macht sie für viele aktuelle NISQ-Geräte nur eingeschränkt nutzbar – insbesondere bei Echtzeitanwendungen. Dennoch ist Amplitude Encoding von hohem theoretischem Interesse, z.~B. für generative Modelle oder komplexe Feature-Räume \cite{schuld2018, zoufal2019}.  

\vspace{0.2cm}
\noindent
\textbf{Quantenverschränkung in Embedding-Schichten}\\
Ein zentrales Merkmal moderner QGAN-Architekturen ist der Einsatz von \textit{entangled embeddings}. Dabei werden mehrere Qubits in einem verschränkten Zustand vorbereitet, um hochdimensionale und nichtlineare Abhängigkeiten zwischen den Inputmerkmalen abzubilden. Niu et al. (2022) zeigen, dass der Einsatz solcher Entanglement-Schichten in der Eingabekodierung die Fähigkeit des Generators signifikant verbessert, komplexe Wahrscheinlichkeitsverteilungen zu erlernen – insbesondere im Vergleich zu rein tensorproduktbasierten Kodierungen. Diese Methode wird als vielversprechend für Bilddaten, medizinische Signale oder molekulare Repräsentationen eingestuft \cite{niu2022}.  

\vspace{0.5cm}

\noindent\textbf{3. Schlüsselalgorithmen im QML}  

\vspace{0.3cm}
\noindent
\textbf{Quantum Neural Networks (QNNs) und Variational Quantum Circuits (VQCs)}\\
Quantum Neural Networks sind Quantenmodelle, die von der Architektur klassischer neuronaler Netze inspiriert sind. In der Praxis kommen fast ausschließlich hybride Varianten zum Einsatz, sogenannte \textit{Variational Quantum Circuits}. Diese bestehen aus einem parametrisierten Quantenanteil, der durch klassische Optimierungsalgorithmen trainiert wird. Dabei übernehmen klassische Systeme typischerweise die Gradientenberechnung, während die quantenmechanischen Schaltungen die eigentliche Datenverarbeitung und Mustererkennung übernehmen. Die zentrale Herausforderung liegt im Training dieser Modelle: Der Optimierungsprozess ist anfällig für sogenannte \textit{barren plateaus}, also Parameterregionen mit extrem flachen Gradienten, die effektives Lernen stark erschweren. QNNs gelten dennoch als Schlüsseltechnologie, insbesondere bei Klassifikationsaufgaben mit hohem Strukturgehalt \cite{abbas2021}.  

\vspace{0.3cm}
\noindent
\textbf{Quantum Autoencoder (QAE)}\\
Quantum Autoencoder sind die quantenmechanische Erweiterung klassischer Autoencoder und dienen der komprimierten Repräsentation von Quantenzuständen. Ziel ist es, irrelevante Komponenten eines Quantenzustands gezielt zu verwerfen, um eine effizientere, latente Repräsentation zu erzeugen. Ein Maß für die Qualität der Kompression ist die \textit{TrashState-Fidelity} – ein Maß für die Entfernung zur Referenz \cite{ngairangbam2022}. Im Gegensatz zu klassischen Autoencodern steht bei QAEs nicht die vollständige Rekonstruktion des ursprünglichen Zustands im Vordergrund, sondern die Reduktion redundanter Information im Quantenraum. Dies erlaubt insbesondere bei verrauschten oder hochdimensionalen Daten effizientere Modellierungen \cite{schuld2018}.  

\vspace{0.3cm}
\noindent
\textbf{Quantum Support Vector Machines (QSVM)}\\
Eine der vielversprechendsten Anwendungen quantenmechanischer Methoden im Bereich des überwachten Lernens ist die Quantum Support Vector Machine (QSVM). Analog zur klassischen SVM verfolgt auch die QSVM das Ziel, Datenpunkte durch eine optimale Trennungshyperfläche zu klassifizieren. Der zentrale Unterschied liegt jedoch in der Nutzung von quantenmechanischen Feature Maps, um die Daten in einen hochdimensionalen Hilbertraum zu überführen, in dem eine lineare Trennung möglich wird \cite{kavitha2024}. Der sogenannte \textit{Quantum Kernel Trick} basiert auf der Idee, die Ähnlichkeit zweier klassischer Datenpunkte nicht direkt, sondern über die Überlappung ihrer quantenmechanischen Zustände zu bestimmen. Formal wird der Kernelwert durch das Quadrat des inneren Produkts zweier Zustände beschrieben:
\[
K(x, z) = \left| \langle \Phi(x) \mid \Phi(z) \rangle \right|^2
\]
In der Praxis wird dieser Kernelwert über quantenmechanische Messungen ermittelt – etwa durch Verfahren wie den \textit{SWAP-Test} oder über spezielle Pauli-basierte Feature Maps. Diese nutzen verschiedene Kombinationen von Gattern (z.\,B. X, Y, Z), um die Daten in den Hilbertraum zu übertragen. Eine YZ-basierte Map erzielte z.\,B. auf dem Wine-Datensatz eine Genauigkeit von 98{,}11\,\% – höher als bei der klassischen RBF-SVM mit 96{,}15\,\% \cite{kavitha2024}.  

Die Umsetzung der QSVM erfolgt in drei Schritten:  
\begin{enumerate}
  \item Preprocessing mit Standardisierung und ggf. PCA zur Dimensionsreduktion,
  \item Generierung der Kernelmatrix durch Anwendung quantenmechanischer Feature Maps,
  \item Klassifikation, wobei der Kernelwert als Grundlage für die Entscheidung dient.
\end{enumerate}  
Ein zentrales Forschungsziel ist die Optimierung geeigneter Feature Maps, da diese maßgeblich über die Effizienz und Generalisierungsfähigkeit des Modells entscheiden. Darüber hinaus zeigt sich, dass QSVMs vor allem bei kleinen bis mittleren Datensätzen unter Nutzung aktueller NISQ-Hardware praktikabel sind. Ihre Anwendung steht somit im direkten Zusammenhang mit der verfügbaren Qubit-Anzahl und der Wahl speichereffizienter Kodierungsverfahren.  

\vspace{0.3cm}
\noindent
\textbf{Generative Verfahren in der QML}  

\vspace{0.2cm}
\noindent
\textit{Quantum Generative Adversarial Networks (qGANs)}\\
Quantum Generative Adversarial Networks übertragen das Prinzip klassischer GANs – ein Konkurrenzspiel zwischen Generator und Diskriminator – in den quantenmechanischen Kontext. In der Regel wird der Generator als parametrisierte Quantenschaltung (PQC) realisiert, die durch Rotation, Superposition und Entanglement synthetische Zustände generiert \cite{zoufal2019}. Die Trainingsdaten werden dabei häufig mittels \textit{Amplitude Encoding} in einen kompakten Quantenzustand überführt, was eine exponentielle Datenkomprimierung ermöglicht \cite{schuld2018}. Der Diskriminator bleibt meist klassisch, operiert jedoch auf den Ergebnissen quantenmechanischer Messungen, wodurch quantenspezifische Muster erkennbar werden \cite{braccia2021}. Aktuelle Forschungsansätze zeigen, dass der Einsatz von verschränkten Eingabezuständen (\textit{entangled embeddings}) die Ausdrucksstärke des Generators verbessern kann \cite{niu2022}.  

\vspace{0.2cm}
\noindent
\textit{Quantum Diffusion Models (QDM) \& unitäre Dynamik}\\
Ein zentrales Konzept zur Beschreibung der Zeitentwicklung geschlossener Quantensysteme ist die \textit{unitäre Dynamik}, wie sie durch die zeitabhängige Schrödingergleichung beschrieben wird:
\[
i\hbar \frac{\partial \psi(x, t)}{\partial t} = \hat{H} \psi(x, t)
\]
Diese Dynamik bewahrt die Norm des Zustandsvektors und ist reversibel. In erweiterten QML-Modellen, insbesondere bei \textit{Quantum Diffusion Models} (QDM), wird diese unitäre Entwicklung ergänzt durch dissipative Effekte, um realistische Transportphänomene in quantenoffenen Systemen zu modellieren. Degond et al. (2004) stellen dazu ein quantenmechanisches Drift-Diffusionsmodell auf Basis der Wigner–BGK-Gleichung vor. Diese enthält Kollisionsterm-Approximationen und führt zu makroskopischen Gleichungen mit \textit{quantum corrections}, insbesondere dem Bohmschen Potential und quantenkorrigierten Drucktermen. Solche Erweiterungen sind essenziell für QML-Modelle, die auf realistischen, nicht-unitären Dynamiken basieren \cite{degond2004}.  

\vspace{0.5cm}


\noindent\textbf{4. Lernstrategien und Optimierungsverfahren}  

\vspace{0.2cm}
\noindent
\textbf{Transfer Learning} bezeichnet die Wiederverwendung zuvor gelernter Repräsentationen auf neue, verwandte Aufgaben – z.\,B. durch Reinitialisierung von Modellparametern oder -architekturen. Diese Idee lässt sich auch in Quantum Machine Learning übertragen, etwa durch Wiederverwendung optimierter Parametrisierungen von VQCs oder Pretraining auf synthetischen Daten.

\vspace{0.2cm}
\noindent
\textbf{Meta-Learning} hingegen zielt darauf ab, Lernsysteme selbst „anpassungsfähiger“ zu machen. Laut Hospedales et al. (2022) umfassen klassische Meta-Learning-Strategien:
\begin{itemize}
  \item \textit{Metriklernen} (z.\,B. Vergleich neuer Beispiele mit Referenzklassen),
  \item \textit{Optimierungslernen} (z.\,B. durch modifizierte Gradientenverfahren),
  \item \textit{Modellinitialisierung} (z.\,B. MAML).
\end{itemize}
Diese Konzepte gewinnen auch in der QML zunehmend an Bedeutung – etwa bei der automatisierten Auswahl von Encoding-Strategien oder architekturabhängigen Optimierungsansätzen \cite{hospedales2022}.  

\vspace{0.4cm}
\noindent\textbf{5. Mathematische Werkzeuge im QML}  

\vspace{0.2cm}
\noindent
\textbf{Quantum Kernel Methods}\\
Quantum Kernel Methods übertragen den aus der klassischen ML bekannten „Kernel-Trick“ auf den Quantenraum: Anstatt Daten explizit in höherdimensionale Feature-Räume zu transformieren, nutzt man quantenmechanische Zustände, um innere Produkte direkt zu messen. Die Kernel-Werte entsprechen dabei typischerweise der \textit{Fidelity} zwischen zwei kodierten Zuständen, was mithilfe des \textit{SWAP-Tests} implementiert werden kann.  

Jerbi et al. (2023) unterscheiden zwei Klassen:
\begin{enumerate}
  \item Explizite Modelle, die mit Observablen auf einzelnen Zuständen arbeiten,
  \item Implizite Modelle, die nur über Kernel-Funktionen (Fidelity, Projective Measurements) operieren.
\end{enumerate}
Die Autoren zeigen auch, dass Kernelmethoden zwar gute Trennungen im Feature-Raum ermöglichen, aber oft eine hohe Sample-Komplexität aufweisen – ein zentrales Thema im praktischen QML \cite{jerbi2023}.

\vspace{0.2cm}
\noindent
\textbf{Shadow Tomography}\\
\textit{Shadow Tomography} ist eine Technik zur kompakten Repräsentation eines Quantenzustands durch Stichprobenmessungen. Dabei werden sogenannte \textit{Classical Shadows} erstellt – kleine Informationspakete über den Zustand, die eine effiziente Approximation von Observablen erlauben. Der Vorteil gegenüber vollständiger Quanten-Tomografie liegt im exponentiellen Effizienzgewinn bei vielen Qubits. Jerbi et al. (2023) zeigen, dass solche Methoden besonders für \textit{post hoc}-Analysen in Typ-III-QML-Modellen wichtig sind, z.\,B. zur Messung von Feature-Overlaps, Kernelwerten oder Verifikation von Outputzuständen.

\vspace{0.4cm}
\noindent\textbf{6. Technologischer Rahmen und Implementierung}  

\vspace{0.2cm}
\noindent
\textbf{NISQ-Hardware}\\
Aktuelle Quantencomputer befinden sich im sogenannten \textit{NISQ}-Zeitalter (\textit{Noisy Intermediate-Scale Quantum}), einem Begriff, der von John Preskill geprägt wurde. NISQ-Systeme zeichnen sich durch eine mittlere Qubit-Anzahl (etwa 50–100 Qubits) und eine hohe Fehleranfälligkeit aus. Die zentrale Herausforderung besteht darin, dass Quantenzustände extrem empfindlich auf Störungen reagieren – sogenannte Dekohärenz –, was die korrekte Ausführung tiefer Schaltungen stark einschränkt. Zudem fehlt es an praxistauglichen Fehlerkorrekturverfahren, da diese selbst enorme Ressourcen erfordern würden.  

Für Quantum Machine Learning bedeutet das: Nur Modelle mit geringer Schaltungstiefe, robuster Kodierung und hybrider Architektur sind derzeit realistisch umsetzbar. Der Hardwarebezug ist damit nicht nur technischer Rahmen, sondern direktes Gestaltungskriterium für alle gegenwärtigen QML-Ansätze \cite{preskill2018}.

\vspace{0.2cm}
\noindent
\textbf{Transpilation \& Readout}\\
Bevor ein Quantenalgorithmus auf realer Hardware ausgeführt werden kann, muss er in eine für das jeweilige Quantenprozessor-Layout ausführbare Form übersetzt werden – dieser Vorgang wird \textit{Transpilation} genannt. Dabei wandelt ein Compiler abstrakte logische Quantenoperationen in konkrete, hardwarekompatible Gatterfolgen um. Da reale Quantenprozessoren meist nur bestimmte Gatter und begrenzte Qubit-Verbindungen unterstützen, sind diese Anpassungen notwendig – sie können aber auch zu unbeabsichtigten Veränderungen der Modellstruktur führen.

In sicherheitskritischen Szenarien ist dieser Schritt deshalb besonders sensibel – etwa im Hinblick auf gezielte Manipulationen oder Compiler-Fehlverhalten. Beim \textit{Readout} schließlich werden die Qubits gemessen, um ein klassisches Ergebnis zu erhalten. Auch hier besteht eine potenzielle Schwachstelle: Die Messung ist probabilistisch und anfällig für Fehler oder systematische Verzerrungen, insbesondere bei häufiger Wiederholung (\textit{Sampling}) oder starker Rauschbelastung. Beide Prozesse – Transpilation und Readout – sind daher nicht nur technische Zwischenschritte, sondern sicherheitsrelevante Komponenten im gesamten QML-Workflow \cite{wille2019}.

\vspace{0.5cm}


\subsubsection{Stand der Technik und Forschung}
\noindent
Die Forschung im Bereich des Quantum Machine Learning (QML) erlebt seit einigen Jahren eine bemerkenswerte Dynamik. Während die Grundlagen des Quantencomputings bereits in den 1990er Jahren durch Algorithmen wie Shor und Grover etabliert wurden, konzentriert sich die gegenwärtige Forschung zunehmend auf die Anwendung quantenmechanischer Rechenverfahren in datengetriebenen Lernprozessen. Das Ziel ist, über klassische Grenzen hinausgehende Lern- und Analysekapazitäten zu schaffen – insbesondere in Bereichen wie Klassifikation, Mustererkennung, Vorhersage und Entscheidungsunterstützung. Zwischen 2017 und 2023 ist ein signifikanter Anstieg an wissenschaftlichen Publikationen im Themenfeld QML zu verzeichnen, in denen verschiedene algorithmische Ansätze entwickelt, simuliert und teilweise bereits auf realer Quantenhardware implementiert wurden (Peral-García et al., 2024).

\vspace{1em}
\noindent\textbf{Modellklassen und Typen}

\noindent
Die derzeitige Forschung im Bereich QML lässt sich anhand drei unterschiedlicher Modellarchitekturen strukturieren, wobei insbesondere der Grad der quantenmechanischen Integration ein zentrales Unterscheidungskriterium darstellt. Zwei etablierte Klassifikationen (– von )Chen et al. (2023) und Uddin et al. (2024)) schlagen dabei jeweils eine Dreiteilung vor, die sich inhaltlich stark überschneiden, aber unterschiedliche Perspektiven einnehmen: Chen et al. fokussieren primär auf die strukturelle Zusammensetzung der Modelle, während Uddin et al. den Funktionalitäts- und Integrationsgrad betonen.
Zur besseren Übersicht wird im Folgenden eine kombinierte Darstellung beider Ansätze gewählt, um die zentralen Architekturen konsistent und vergleichbar einzuordnen.

\subparagraph{Typ I / Fully Quantum Models}
Diese Modellklasse führt alle Verarbeitungsschritte – von der Datenkodierung über die Merkmalsextraktion bis zur Entscheidung – vollständig auf Quantenhardware durch. Dazu zählen z.B. Quantum Support Vector Machines (QSVM) mit vollständig quantenbasiertem Kernelansatz (Chen et al., 2023, S.4–5) sowie Quantum Hopfield Networks oder vollständig quantisierte QNNs (Uddin et al., 2024, S.10).
Trotz ihres theoretischen Potenzials – etwa durch maximale Parallelisierung – sind diese Modelle derzeit nur für sehr kleine Problemräume einsetzbar. Sie gelten als stark hardwareabhängig und in der Optimierung hochkomplex (Uddin et al., 2024, S.10–11).

\subparagraph{Typ II / Hybride Quantum-Classical Models}
In diesen Architekturen werden klassische und quantenmechanische Verarbeitungseinheiten kombiniert. Typisch ist eine klassische Vorverarbeitung (z.B. Feature Engineering, Normalisierung) in Verbindung mit einem quantenmechanischen Kernmodul wie VQCs oder QNNs (Chen et al., 2023, S.4–5; Uddin et al., 2024, S.9).
Diese Modelle sind derzeit die am weitesten verbreitete Architektur in der praktischen Forschung, da sie mit den Limitierungen heutiger NISQ-Geräte vereinbar sind. Sie bieten eine höhere Modellflexibilität, sind vergleichsweise leicht trainierbar und erlauben modulare Schnittstellen zwischen klassischen und quantenbasierten Komponenten (Uddin et al., 2024, S.9–10).

\subparagraph{ Typ III / Quantum-enhanced Classical Models}
Hierbei handelt es sich um klassisch dominierte Architekturen, in die gezielt einzelne quantenmechanische Komponenten – etwa zur Kernelberechnung oder Dimensionsreduktion – integriert werden. Ein prominentes Beispiel ist die Quantum PCA oder die QSVM mit quantenmechanischem Kernel (Chen et al., 2023, S.5; Uddin et al., 2024, S.8–9).
Solche Modelle versprechen Vorteile insbesondere bei großen Datensätzen, da sie eine Reduktion der Zeitkomplexität ermöglichen – etwa von O(n²) auf O(logn), sofern eine geeignete Datenkodierung vorhanden ist (Uddin et al., 2024, S.11). Gleichzeitig sind sie vergleichsweise hardwaretolerant, da nur ausgewählte Verarbeitungsschritte auf Quantenhardware erfolgen.


\vspace{1em}
\noindent\textbf{Funktionale Gliederung nach Lernziel}

\vspace{0.5em}
\noindent
Chen et al. (2023) unterscheiden laufende Forschungsprojekte zusätzlich anhand des funktionalen Lernziels. Diese systematische Gliederung umfasst drei zentrale Kategorien:
\subparagraph{Klassifikation:}
Der größte Anteil aktueller QML-Anwendungen zielt auf die Kategorisierung strukturierter oder unstrukturierter Daten. Hierfür kommen unter anderem VQCs, Quantum Neural Networks (QNN) sowie quantum-inspirierte CNN-Varianten zum Einsatz. Domänen sind u.a. Bilderkennung, Sprachverarbeitung und Biosignalverarbeitung (Chen et al., 2023, S.8).
\subparagraph{Regression:}
In regressiven Szenarien werden kontinuierliche Zielgrößen vorhergesagt. Dies findet Anwendung z.B. in der molekularen Simulation, der Finanzmarktmodellierung oder der physikalischen Prozessprognose. Verwendet werden hierfür insbesondere hybride Netzwerke mit quantenmechanischen Hidden-Layers und klassischem Output (Chen et al., 2023, S.9).
\subparagraph{Clustering und Dimensionsreduktion:}
Diese Aufgaben werden vorrangig zur explorativen Datenanalyse eingesetzt. Algorithmen wie Quantum PCA, Quantum k-Means oder QSFA (Quantum Slow Feature Analysis) kommen hier zur Anwendung. Oftmals wird der SWAP-Test verwendet, um quantenmechanische Distanzmaße zwischen Zuständen zu evaluieren (Chen et al., 2023, S.10).

\vspace{1em}
\noindent
Diese funktionale Gliederung reflektiert zentrale Anwendungsfelder gegenwärtiger QML-Modelle und dient der gezielten Auswahl geeigneter Architekturtypen je nach Aufgabe.


\vspace{1.5em}
\noindent\textbf{Fully Quantum Models (Typ I)}

\noindent
Vollständig quantenmechanische Modelle (Typ I) stellen den theoretisch reinsten Ansatz im QML dar. In diesen Architekturen werden sämtliche Rechenschritte – von der Datenkodierung über die Merkmalsextraktion bis zur Entscheidung – ausschließlich auf Quantenhardware durchgeführt. Damit versprechen Typ I-Modelle eine maximale Ausnutzung quantenmechanischer Prinzipien wie Superposition, Verschränkung und unitäre Evolution.
Gleichzeitig sind sie jedoch mit erheblichen Herausforderungen konfrontiert: Die extreme Hardwareabhängigkeit, hohe Schaltungstiefe und das Problem der Barren Plateaus erschweren bislang eine skalierbare Implementierung. Dieses Kapitel beleuchtet exemplarisch aktuelle Entwicklungen auf diesem Gebiet – von innovativen Modellarchitekturen wie Quantum Diffusion Models bis hin zu fortschrittlichen Trainingsstrategien zur Überwindung grundlegender Optimierungsbarrieren.

\subparagraph{Quantum Diffusion Models (QDMs) – Ein deterministisches Generationsparadigma}
Ein besonders innovativer Ansatz innerhalb der generativen QML-Verfahren sind die Quantum Diffusion Models (QDMs), die ein deterministisches Gegenmodell zu klassischen stochastischen Diffusionsprozessen darstellen. Anders als klassische Diffusionsmodelle – wie die populären Denoising Diffusion Probabilistic Models (DDPMs) – verzichten QDMs vollständig auf stochastische Rauschprozesse. Stattdessen nutzen sie die deterministische, reversibel kontrollierbare Dynamik der quantenmechanischen Schrödingergleichung, um Bilddaten zu generieren.
Das von Zhang et al. (2023) vorgestellte Framework basiert auf dem Prinzip, eine Bildinformation nicht durch inkrementelles Hinzufügen von Rauschen zu transformieren, sondern sie durch unitäre Quantenoperationen in einen hochentropischen Zustand zu überführen. Dieser „Vorwärtsprozess“ erfolgt durch eine zeitlich gesteuerte unitäre Evolution, die sich direkt aus der Schrödingergleichung ableitet:
dψ(t)/dt = –i · H(t) · ψ(t) 
Hierbei ist H(t) ein zeitabhängiger Hamiltonoperator, der gezielt gewählt wird, um den Bildzustand in einen Zustand maximaler Unordnung zu überführen. Das ursprüngliche Bild wird zuvor in einem quantenmechanisch kodierten Amplituden- oder Phasenzustand abgelegt.
Der eigentliche Innovationspunkt liegt im Rückführungsprozess, der – anders als bei klassischen Modellen – nicht iterativ durch stochastische Subtraktion von Rauschen erfolgt, sondern durch einen trainierten inversen unitären Operator. Dieser wird mittels eines parametrisierten quantenmechanischen Netzwerks approximiert, sodass er den „entropischen“ Endzustand deterministisch zurück in einen strukturierten Bildzustand überführen kann (Zhang et al., 2023, S.3–4).
Der gesamte Generierungsprozess basiert somit auf vollquantitativen, verlustfreien Operationen, ohne dass klassische Sampling-Mechanismen oder diskrete Zwischenschritte notwendig wären. Damit eröffnen QDMs die Perspektive auf eine neue Klasse von generativen Quantenmodellen, die durch ihre deterministische Natur besonders effizient trainierbar und hardwarekompatibel sind.
Zur praktischen Validierung wurde das Modell auf mehreren Datensätzen getestet, darunter insbesondere Fashion-MNIST und ein quantenmechanisch kodiertes CIFAR10-Subset (S-CIFAR10). Das Training des QDM-Netzwerks erfolgte simuliert auf Quantenprozessoren, wobei die Trainingsdaten zunächst in quantenkodierte Bildzustände überführt wurden.
Die Evaluation zeigt, dass das QDM-Framework in Bezug auf Bildqualität, Strukturerhaltung und Modellstabilität mit klassischen Diffusionsmodellen vergleichbar ist – bei zugleich deutlich reduziertem Ressourcenaufwand. Während klassische DDPMs teils mehrere Tausend Schritte zur Bildgenerierung benötigen, arbeitet das QDM mit einer festen, nicht-iterativen Rückführung basierend auf einer trainierten unitären Approximation (Zhang et al., 2023, S.6–8).
Die Ergebnisse deuten darauf hin, dass QDMs nicht nur eine theoretische Möglichkeit zur Bildgenerierung bieten, sondern eine praxisrelevante Alternative für generative QML-Aufgaben darstellen – insbesondere in Bereichen, in denen deterministische und physikalisch reversible Systeme bevorzugt werden.
Trotz solcher Fortschritte bei Modellarchitekturen bleiben Trainingsprozesse eine der größten Herausforderungen vollquantitativer QML-Modelle – insbesondere aufgrund der berüchtigten Barren Plateaus. Der folgende Abschnitt beleuchtet einen neuartigen Trainingsansatz, der dieses Problem gezielt adressiert.

\subparagraph{Trainingsherausforderungen: Barren Plateaus und QuantumTrain}
Ein bislang unterschätztes Hindernis bei der Entwicklung leistungsstarker QML-Modelle ist die Schwierigkeit, tiefere PQC-Architekturen effizient zu trainieren. Klassische Gradientendeszente wie SPSA oder COBYLA geraten hier schnell an ihre Grenzen – insbesondere aufgrund des Problems sogenannter Barren Plateaus, bei denen sich der Gradient im Parameterraum global auslöscht. Um dieses Problem zu umgehen, präsentieren Lui et al. in ihrer QuantumTrain-Reihe einen innovativen Trainingsansatz, der auf dem Prinzip der Parameter-Perturbation in Verbindung mit kontrollierter Initialisierung basiert (Lui et al., 2024a; 2024b).
Kernidee ist, den Trainingsprozess nicht auf zufälliger Initialisierung basieren zu lassen, sondern durch systematische Pre-Training-Strukturen (u.a. mit layerweise kontrollierter Parametrisierung und rekursivem Fine-Tuning) bereits vorab eine stabile Gradientenlandschaft zu erzeugen. Ergänzt wird dies durch ein Meta-Training-Verfahren, das aus vorherigen Trainingsläufen lernt und diese Erfahrungen zur besseren Initialisierung neuer Modelle einsetzt – eine Art quantenmechanisches Pendant zum Transfer Learning (Lui et al., 2024b, S.4–5).
Besonders bemerkenswert ist, dass QuantumTrain auch Hardware-bewusste Architekturen unterstützt: Anstatt rein theoriebasierte PQCs zu trainieren, erfolgt die Optimierung unter realistischen Einschränkungen wie Rauschprofilen und begrenzter Gate-Fidelity. Erste Simulationen auf IBM Q zeigen, dass tiefere Netze durch QuantumTrain signifikant schneller und stabiler konvergieren, insbesondere in Bildklassifikationsaufgaben (z.B. Fashion-MNIST und Quantum MNIST).


\vspace{1.5em}
\noindent\textbf{Hybride Quantum-Classical Models (Typ II)}

\noindent
Hybride Quantum-Classical Models (Typ II) bilden gegenwärtig das Herzstück anwendungsorientierter QML-Forschung. Sie kombinieren klassische Vorverarbeitungs- und Nachbearbeitungsschritte mit quantenmechanischen Kernkomponenten wie parametrisierten Quanten-Schaltkreisen (PQC) oder Quantum Neural Networks (QNN). Diese Architekturen gelten als besonders praxistauglich, da sie sich mit den Limitierungen aktueller NISQ-Geräte vereinbaren lassen und zugleich die Vorteile beider Paradigmen vereinen.
Im Folgenden werden zentrale Modellbeispiele, Kodierungsmethoden sowie hardwarebezogene Implementierungsstrategien dieser Modellklasse vorgestellt und kritisch eingeordnet.


\subparagraph{Modellbeispiele}
Ein besonders interessantes hybrides QML-Modell ist das sogenannte LSTM-QNN (Long Short-Term Memory – Quantum Neural Network), das klassische rekurrente Architekturprinzipien mit einer quantenmechanischen Endschicht kombiniert. Die Modellarchitektur besteht aus zwei aufeinanderfolgenden klassischen LSTM-Schichten mit jeweils 32 Zellen, gefolgt von einer vollständig verbundenen klassischen Zwischenschicht. Daran anschließend folgt eine QNN-Schicht, die auf zehn Qubits basiert.
Die Quantenschicht wird mithilfe Strongly Entangling Layers implementiert – einer Schaltungsstruktur, die systematisch Rotation und Verschränkung kombiniert, um eine hohe Expressivität bei gleichzeitig flacher Tiefe zu erreichen. Zusätzlich kommt der Instantaneous Quantum Polynomial-Time (IQP)-Ansatz zum Einsatz, bei dem nicht-kommutierende Gates zur Erzeugung komplexer Quantenzustände verwendet werden. Diese Architektur erlaubt es, auch nichtlineare, sequentielle Muster im Datenfluss zu lernen, ohne auf klassische Tiefenstapel zurückgreifen zu müssen.
Ein klassisches Dense-Layer dient dabei als „Schnittstelle“ zwischen LSTM-Ausgabe und QNN-Eingabe – notwendig, da aktuelle Quantenhardware keine direkte Verarbeitung kontinuierlicher Repräsentationen erlaubt. Erste empirische Studien zeigen, dass diese Form hybrider Architektur besonders gut für sequentielle Klassifikationsaufgaben geeignet ist, etwa in der Verarbeitung zeitlicher Sensordaten oder natürlicher Sprache (Peral-García et al., 2024, S.10–11).
Ein weiteres Modell, das die Stärken hybrider Architekturen demonstriert, ist der Quantum Autoencoder (QAE). Dieser basiert auf dem bekannten Encoder-Decoder-Paradigma, wird jedoch teilweise durch Quantenschaltungen realisiert. Der Encoder besteht aus einem parametrisierten PQC, der aus einem Hilbertraum $H \otimes \mathcal{H}_A \otimes \mathbb{N}$ in einen Raum $V_c \subset \mathbb{R}^k \setminus V_c \subset \mathbb{R}^k$ transformiert. Dabei wird gezielt eine rauscharme, rauschresistente Zustandscodierung angestrebt.
Der Decoder, ebenfalls als PQC implementiert, versucht anschließend, aus dem latenten Zustand die ursprüngliche Information zu rekonstruieren. Dieses Verfahren ist besonders relevant für datenarme Anwendungsdomänen, in denen eine Reduktion redundanter Information – etwa bei Bilddaten, molekularen Fingerprints oder medizinischen Zeitreihen – von großem Vorteil ist.
In der Praxis wird häufig eine klassische Vorverarbeitung vorgeschaltet, um kontinuierliche oder verrauschte Eingaben zunächst zu stabilisieren, bevor sie quantenmechanisch kodiert werden. Diese hybride Vorstrukturierung erlaubt eine robuste und hardwareeffiziente Implementierung (Peral-García et al., 2024, S.5).
Eine weitere innovative Klasse hybrider Modelle sind die Quantum Generative Adversarial Networks (QGANs). Sie übertragen das Prinzip klassischer GANs – bestehend aus Generator und Diskriminator – in den quantenmechanischen Kontext. Der Generator wird hierbei als parametrische Quanten-Schaltung (PQC) realisiert, die durch gezielte Rotation, Superposition und Verschränkung Zustände erzeugt, die klassisch nur schwer simulierbar sind.
Der Diskriminator bleibt in der Regel klassisch, erhält jedoch als Input die Messergebnisse des Quantenprozessors. Diese Konfiguration erlaubt eine leistungsfähige Unterscheidung zwischen echten und synthetisch erzeugten Daten, bei gleichzeitig reduzierter Komplexität auf der Quantenebene.
QGANs wurden bereits erfolgreich zur Bildgenerierung, zur Synthese komplexer Wahrscheinlichkeitsverteilungen sowie zur Simulation physikalischer Systeme eingesetzt. Ihre Kombination aus generativer Flexibilität und hardwarefreundlicher Umsetzung macht sie zu einem besonders vielversprechenden Kandidaten für Anwendungen in den Bereichen Medizin, Materialforschung und Cybersicherheit (Peral-García et al., 2024, S.11–12).

\subparagraph{Kodierungsmethoden und Schnittstellen}
Ein zentrales Element quantenbasierter ML-Systeme ist die effiziente Übertragung klassischer Daten in quantenmechanisch verarbeitbare Zustände – ein Prozess, der unter „Quantum Data Encoding“ oder „State Preparation“ bekannt ist. Uddin et al. (2024) analysieren drei dominierende Kodierungstechniken, die aktuell in QML-Anwendungen zum Einsatz kommen
(Uddin et al., 2024, S.6–8).
Amplitude Encoding speichert die Werte klassischer Vektoren direkt in den Amplituden eines Quantenzustands. Diese Methode ist speichereffizient, da ein Vektor mit $2^{n_2} \wedge 2^{n_n}$ Dimensionen bereits mit $n \cdot n$ Qubits kodiert werden kann. Ihr Nachteil liegt jedoch in der hohen Komplexität der Zustandsvorbereitung, insbesondere bei verrauschten oder komplex strukturierten Eingabedaten (Uddin et al., 2024, S.7).
Angle Encoding (auch Phase Encoding) übersetzt Datenwerte in Rotationswinkel einzelner Qubits, etwa über $R_y$, $R_x$- oder $R_z$-Gates. Diese Technik ist besonders gut für NISQ-Hardware geeignet, da sie flache Quantenschaltungen ermöglicht. Sie ist jedoch weniger speichereffizient, da jede Datenkomponente einen eigenen Qubit oder eine eigene Rotation benötigt (Uddin et al., 2024, S.7).
Basis Encoding wiederum weist diskrete Datenwerte direkt bestimmten Basiszuständen zu (z.\,B. $\ket{00}$, $\ket{01}$)). Diese Methode ist zwar konzeptionell einfach, skaliert jedoch sehr schlecht mit der Datenmenge und eignet sich daher eher für symbolische oder stark klassifizierende Aufgaben – weniger für umfangreiche ML-Probleme mit kontinuierlichen Daten
(Uddin et al., 2024, S.7–8).
Die Wahl des Encodings beeinflusst direkt die Modellarchitektur: Sie bestimmt den Qubit-Bedarf, die Schaltungstiefe und die Empfindlichkeit gegenüber Hardwarefehlern. So ist Angle Encoding etwa robuster gegenüber Qubit-Rauschen, während Amplitude Encoding theoretisch eine exponentielle Datenkomprimierung bietet – allerdings auf Kosten tief verschachtelter Vorbereitungsschaltkreise, was es auf heutigen NISQ-Geräten nur eingeschränkt praktikabel macht
(Uddin et al., 2024, S.8).
Darüber hinaus hat das Encoding auch Auswirkungen auf Trainingszeiten und Modellkomplexität. Komplexere Kodierungen führen häufig zu längeren Ladezeiten und erhöhen die Anzahl erforderlicher Gate-Operationen – was wiederum das Fehlerpotenzial und das Risiko von Overfitting steigern kann. Besonders bei hybriden QML-Ansätzen mit klassischer Vorverarbeitung ist die Wahl der Kodierung entscheidend für Trainingsstabilität und Effizienz
(Uddin et al., 2024, S.11–12).

\subparagraph{Einsatz auf NISQ-Systemen}
Ein zentrales Forschungsthema im Bereich der hybriden QML-Modelle ist die Optimierung für NISQ-Geräte (Noisy Intermediate-Scale Quantum), also Quantenprozessoren mit begrenzter Qubitzahl und fehleranfälliger Hardware. Gujju et al. (2024) betonen, dass die Einschränkungen realer Quantencomputer (z.B. kurze Kohärenzzeiten, beschränkte Konnektivität, Gate-Fehler) eine gezielte algorithmische Anpassung erforderlich machen. Als besonders relevante Ansätze identifizieren sie in ihrer Analyse zwei Klassen:
\begin{itemize}
  \item Variational Quantum Classifiers (VQCs): Diese Modelle beruhen auf trainierbaren parametrisierten Quantenschaltkreisen, deren Parameter durch klassische Optimierungsalgorithmen iterativ angepasst werden. Sie sind besonders geeignet für Klassifikationsprobleme kleiner bis mittlerer Komplexität und erlauben eine flexible Modellierung quantenmechanischer Entscheidungsgrenzen.
  \vspace{0.5em}
  \item Data-Reuploading Circuits: Hierbei handelt es sich um Schaltungen, bei denen klassische Eingabedaten mehrfach in das PQC eingespeist werden. Diese Wiederverwendung von Eingaben erhöht die effektive Ausdrucksstärke des Netzwerks, ohne die Tiefe der Quantenschaltung zu erhöhen – ein wesentlicher Vorteil im Kontext fehlerbehafteter Hardware. Durch diese Technik lassen sich auch komplexere Entscheidungsgrenzen realisieren, selbst wenn die zugrunde liegenden Quantenoperationen flach bleiben (Gujju et al., 2024, S.3–5).
\end{itemize}
Beide Modellklassen zielen darauf ab, mit möglichst einfachen, hardwarekompatiblen Schaltungen dennoch eine leistungsfähige Modellierung zu ermöglichen. Gujju et al. stellen heraus, dass diese Strategien insbesondere für niedrigdimensionale Klassifikationsaufgaben wie Iris oder Wine Dataset bereits vielversprechende Resultate erzielen. Im Vergleich zu klassischen SVMs oder kleinen neuronalen Netzen zeigen sich keine gravierenden Leistungsunterschiede – insbesondere dann nicht, wenn die Featurekodierung effizient implementiert ist (Gujju et al., 2024, S.5).
Ein besonderer Fokus der Arbeit liegt auf der Hardware-Freundlichkeit: Um die negativen Effekte realer Quantenhardware zu minimieren, wird in vielen Studien bewusst auf tiefe Netzwerke, mehrschichtige PQCs oder hohe Anzahl an Gattern verzichtet. Stattdessen werden gezielt sogenannte hardware-efficient ansätze genutzt – also Schaltungen, die an die physikalischen Eigenschaften der Zielplattform (z.B. native Gate-Sets, Connectivity Maps) angepasst sind. Diese Anpassungen sind laut Gujju et al. zwar mathematisch nicht optimal, aber in der Praxis unverzichtbar, um überhaupt zuverlässige Ergebnisse auf NISQ-Prozessoren erzielen zu können (Gujju et al., 2024, S.9).

\vspace{1.5em}
\noindent\textbf{Quantum-enhanced Classical Models (Typ III)}

\noindent
Die Kombination quantenmechanischer Rechenprinzipien mit klassischen Lernverfahren zielt darauf ab, die inhärente Parallelität von Qubits für die Modellierung komplexer Datenräume nutzbar zu machen. Quantum-enhanced Modelle wie der Quantum Support Vector Machine (QSVM) basieren auf der Idee, dass Quantencomputer durch Superposition und Verschränkung simultan eine Vielzahl möglicher Lösungsräume repräsentieren können. Diese Eigenschaft eröffnet insbesondere bei der Merkmalsraum-Transformation (Feature Mapping) neue Möglichkeiten, nichtlinear separierbare Probleme effizient zu modellieren (Kavitha und Kaulgud, 2024, S.820).
Im Vergleich zu klassischen Systemen, die pro Rechenschritt stets nur einen Lösungsweg verfolgen, ermöglichen Quantenarchitekturen durch exponentielle Zustandsrepräsentation (2ⁿ Zustände mit n Qubits) einen hohen Grad an Parallelität. Dies ist insbesondere bei rechenintensiven Algorithmen wie SVMs von Vorteil, deren klassische Varianten bei wachsender Datenkomplexität an ihre Grenzen stoßen. In solchen Fällen kann die Einbindung quantenmechanischer Feature Maps zu einer verbesserten Trennschärfe und beschleunigten Konvergenz beitragen.

\subparagraph{Modellbeispiele}
Ein zentrales Beispiel für quantum-enhanced klassische Modelle ist die QSVM. Hierbei werden Eingabedaten durch parametrische Quantenschaltungen (z.B. mit Pauli-Operatoren) in einen hochdimensionalen Merkmalsraum transformiert. Der daraus resultierende quantum kernel bildet die Ähnlichkeitsstruktur der Daten ab, die klassisch kaum zugänglich wäre (Chen et al., 2023, S.20).
Der eigentliche SVM-Algorithmus bleibt klassisch, arbeitet jedoch auf Basis des quantenbasierten Kernels. Diese Trennung erlaubt eine modulare Architektur mit quantenmechanischer Feature Map.
Kavitha und Kaulgud (2024) zeigen dies an realen Datensätzen (u.a. Wine, HCV, Electric Grid). Beim Wine-Datensatz erzielen sie mit einer Kombination aus Pauli-Y/Z-Gattern und zwei Wiederholungen eine Genauigkeit von 98{,}11\% – höher als die klassische SVM mit RBF-Kernel (96{,}15\%) – bei zugleich reduzierter Laufzeit (S.823, Tab.1).
Die QSVM eignet sich besonders für hochdimensionale Daten. Eine vorgelagerte PCA reduziert dabei den Qubit-Bedarf und erleichtert die Umsetzung auf NISQ-Hardware (S.821). 
QKE dient oft als Grundlage für QSVMs. Dabei wird ein fidelity-basierter Kernel berechnet, der die Überlappung zweier Quantenzustände misst – im Gegensatz zu klassischen, geometrischen Metriken (Chen et al., 2023, S.21).
Diese Technik kann komplexe, nichtlinear separierbare Strukturen erkennen. Das Lernverfahren selbst bleibt klassisch, wodurch sich QKE leicht in bestehende ML-Pipelines integrieren lässt. Der Quantenanteil beschränkt sich auf die Merkmalsraumkodierung. 
Shadow Models kombinieren quantenmechanisches Modelltraining mit klassischer Inferenz. Zunächst wird ein parametrisiertes Quantenmodell trainiert, das in einen Quantenzustand $\rho(\Theta)$ überführt wird. Mittels Shadow Tomography entsteht daraus eine klassisch speicherbare Näherung $p(\Theta)$, die später ohne Quantenhardware nutzbar ist. Voraussetzung ist, dass relevante Ausgaben durch messbare Observablen beschrieben werden können.
Die Methode eignet sich für produktive KI-Systeme mit hohem Inferenzbedarf und begrenzter Hardware, etwa im industriellen IoT.
Herausforderungen bestehen v.a. bei hoher Eingabedimensionalität, da der Messaufwand exponentiell steigt. Shadow Models sind daher derzeit nur für kleine bis mittlere Datensätze praktikabel.
In der Komplexitätstheorie gelten sie als Mittelweg: leistungsfähiger als klassische Modelle, aber unterhalb der Ausdrucksstärke vollständig quantenmechanischer Systeme. Dennoch bieten sie eine vielversprechende Brücke zur Praxis – etwa in Medizin, Materialwissenschaft oder Finanzanalyse.

\vspace{1.5em}
\noindent\textbf{Synthetische Bewertung aller Typen}

\noindent
Um die drei dominanten Architekturtypen im QML vergleichend einzuordnen, werden sie anhand zentraler Kriterien bewertet. Die folgende Übersicht verdichtet die wesentlichen Merkmale zu einer praxisorientierten Entscheidungsgrundlage:


\begin{table}[ht]
\centering
\small
\begin{tabular}{|>{\bfseries}m{2.5cm}|m{3cm}|m{3cm}|m{3cm}|}
\hline
Kriterium & Typ I – \textbf{Fully Quantum} & Typ II – \textbf{Hybride Modelle} & Typ III – \textbf{Quantum-simuliert Klassisch} \\
\hline

Expressivität & Sehr hoch (qubitige Modellierung, vollständige Superposition) & Hoch (kombiniert klassische Vorverarbeitung mit PQC) & Mittel bis hoch (abhängig vom Quantum-Kernel) \\
\hline

Trainierbarkeit & Niedrig (Barren Plateaus, hardwarelimitiert) & Mittel (Kombination aus klassischen und PQC-Optimierungen) & Hoch (meist klassisches Training) \\
\hline

Fehlerresistenz & Gering (hohe Schaltungsstiefe, fehleranfällig) & Mittel (robust gegenüber see PQC-Modelle) & Hoch (nur selektive Quantenkomponenten) \\
\hline

Hardware-
abhängigkeit & Sehr hoch (vollständige Quantenprozessorabhängigkeit) & Mittel (anpassbar an NISQ-Bedingungen) & Gering (oft klassische Ausführung möglich) \\
\hline

Skalierbarkeit & Niedrig (nur wenige Qubits praktikabel) & Mittel (optimiert für NISQ-Systeme) & Hoch (insbesondere bei Shadow Models) \\
\hline

Rechenkomplexität & Theoretisch minimal ($O(\log n)$), praktisch & Reduziert durch Modularität & Klassisch dominiert, jedoch \\
\hline

Einsatzreife & \textit{Proof-of-Concept} & Prototypisch in Praxisfeldern & Teilweise produktionsnah (v.a. bei Shadow Models) \\
\hline
\end{tabular}
\caption{Vergleich verschiedener Typen von Quantum-Modellen}
\end{table}


\subparagraph{Kontextuelle Einsatzempfehlungen}
Basierend auf den empirisch erprobten Stärken und Schwächen lassen sich folgende Empfehlungen für die Auswahl geeigneter Modelltypen je nach Anwendungsszenario ableiten:
\begin{itemize}
  \item Typ I (Fully Quantum) ist ideal für explorative Grundlagenforschung und die Entwicklung neuer, vollständig quantenbasierter Lernparadigmen – insbesondere bei kleinen Datensätzen und hoher theoretischer Innovationsdichte. Der Einsatz ist auf Emulatoren beschränkt; reale Hardware limitiert derzeit Trainingstiefe und Modellgröße.
  \vspace{0.5em}
  \item Typ II (Hybride Modelle) eignet sich besonders für anwendungsnahe Forschungsprojekte auf NISQ-Hardware. Sie bieten den besten Kompromiss aus Expressivität und Implementierbarkeit, etwa bei sequentiellen Klassifikationsaufgaben, Moleküldesign oder Sensorik. Beispiele wie das LSTM-QNN oder Quantum Autoencoder-Modell zeigen konkrete Umsetzbarkeit.
  \vspace{0.5em}
  \item Typ III (Quantum-enhanced Classical) ist derzeit der vielversprechendste Ansatz für erste reale Einsätze, etwa in der Finanzanalyse, medizinischen Diagnostik oder der Bildverarbeitung. Die Kombination aus klassischer Robustheit und quantenbasierter Merkmalsextraktion erlaubt praktikable Integrationen. Shadow Models und QSVMs stehen hier im Fokus.
\end{itemize}


\vspace{1.5em}
\noindent\textbf{Synthetische Bewertung aller Typen}

\noindent
\subparagraph{Klassifikation}
Ein dominierender Anwendungsbereich in der QML-Forschung ist die Klassifikation von Bild- und Sensordaten, wobei eine Vielzahl unterschiedlicher Modellansätze zur Anwendung kommt. So wird etwa das Quantum Support Vector Machine (QSVM)-Modell eingesetzt, um klassische Merkmalsdaten durch ein nichtlineares Mapping in hochdimensionale Hilberträume zu überführen. Die Kernidee hierbei ist die Verwendung quantenmechanischer Zustände als Feature-Embeddings, wodurch sich insbesondere komplexe, nichtlinear separierbare Datensätze mit hoher Effizienz klassifizieren lassen. Die dafür verwendeten Quantenkernel basieren auf spezifisch konstruierten unitären Transformationen, etwa rotationsbasierten PQCs oder Instantaneous Quantum Polynomial (IQP) Circuits (Peral-García et al., 2024, S.6).
Dabei erfolgt zunächst eine Transformation der Eingabedaten mittels quantenmechanischer Feature Maps – z.B. durch Pauli-Rotationsoperatoren – in einen quantenmechanisch erzeugten Merkmalsraum. Anschließend wird ein quantenbasierter Kernel berechnet, der in eine klassisch ausgeführte Support-Vector-Maschine eingespeist wird. Kavitha und Kaulgud (2024) demonstrieren dieses Verfahren anhand realer Datensätze wie Wine, HCV (Hepatitis C Virus) und Electric Grid Stability. Für den Wine-Datensatz zeigt sich, dass QSWM mit Pauli-Y- und Pauli-Z-Kodierung eine Genauigkeit von 98{,}11\% erreicht – gegenüber 96{,}15\% beim klassischen RBF-Kernel-SVM. Zugleich wurde die Trainingszeit erheblich reduziert. Sie empfehlen zusätzlich die Anwendung klassischer Preprocessing-Schritte wie PCA, um den Anforderungen gegenwärtiger NISQ-Geräte mit limitierter Qubit-Anzahl zu begegnen (Kavitha \& Kaulgud, 2024, S.\,821–823).
Ein weiterer relevanter Ansatz ist Quantum Reservoir Computing (QRC). In diesen Systemen wird ein quantendynamischer Zustand über zeitabhängige CPTP-Transformationen aktualisiert, wobei ein memory effect entsteht – eine Eigenschaft, die insbesondere für sequenzielle Klassifikations- und Vorhersageaufgaben im Zeitbereich genutzt wird (Peral-García et al., 2024, S.5).
Ergänzend dazu bieten Quantum Convolutional Neural Networks (QDCNNs) eine Möglichkeit zur bildbasierten Klassifikation. Diese Netzwerke übertragen das Prinzip klassischer CNNs auf quantenmechanische Register, wobei Bilddaten in einem QRAM-ähnlichen System kodiert und anschließend durch verschränkende Operationen propagiert werden. Die finale Entscheidung erfolgt entweder auf Quantenebene oder durch einen klassischen Postprozess (Peral-García et al., 2024, S.9–10).
Einen flexiblen Ansatz stellt der Variational Quantum Classifier (VQC) dar, der auf parametrisierten Quantenschaltungen mit trainierbaren Parametern basiert. Optimiert wird mit klassischen Algorithmen wie SPSA oder COBYLA, wobei sowohl klassische Kostenfunktionen (z.B. Cross-Entropy) als auch quantenspezifische Metriken (z.B. Fidelity) zum Einsatz kommen. Die Kodierung der Eingabedaten erfolgt entweder über Angle Encoding oder Amplitude Encoding, wobei letzteres eine exponentielle Datenkompression erlaubt, jedoch hardwareseitig derzeit schwerer realisierbar ist (Gujju et al., 2024, S.3–6). In Anwendungen auf Standarddatensätzen wie Iris, Wine oder Breast Cancer zeigen VQCs bei geeigneter Initialisierung vergleichbare Leistungen zu klassischen SVMs – wenn auch mit höherem Trainingsaufwand.
Ein besonders aktueller Forschungsansatz ist die Übertragung von Konzepten wie Transfer Learning auf den QML-Kontext. Dabei können vortrainierte PQC-Modelle für neue Aufgaben wiederverwendet werden – insbesondere, wenn die Aufgaben ähnliche Strukturen aufweisen (z.B. Bildklassifikation). Erste Studien mit „warm-start“-Initialisierungen zeigen deutliche Vorteile bei Konvergenz und Stabilität (Gujju et al., 2024, S.7–8).

\subparagraph{Regression}
Im Bereich der Regression bietet QML ebenfalls erste erfolgversprechende Ansätze, wenngleich die Forschung hier weniger weit fortgeschritten ist als bei der Klassifikation. Wie Chen et al. (2024) betonen, ist Quantum Regression insbesondere in Bereichen relevant, in denen hochdimensionale Eingabedaten mit kontinuierlichen Zielgrößen verarbeitet werden müssen. Ein Beispiel hierfür ist der Quantum Least Squares Fitting Algorithm, bei dem ein lineares Gleichungssystem zur Anpassung der Modellparameter durch eine quantenbasierte Matrixinversion (z.B. mittels des HHL-Algorithmus) gelöst wird.
Zudem kommen auch VQCs mit kontinuierlichen Lossfunktionen wie Mean Squared Error (MSE) zur Anwendung. Diese kombinieren parametrische PQCs mit klassischer Rückpropagation, wobei sich insbesondere bei kleineren Regressionsproblemen mit begrenztem Datensatz (z.B. polynomiale Fits) Vorteile gegenüber klassischen Methoden zeigen – etwa in Bezug auf Robustheit gegenüber Rauschen und Modellkompression (Chen et al., 2024, S.13–14).
Gleichzeitig ist jedoch zu beachten, dass viele quantenbasierte Regressionsansätze eine sehr hohe Rechenkomplexität in der Initialisierung und Messung aufweisen, was ihre praktische Umsetzung auf heutigen NISQ-Systemen limitiert. In Zukunft könnten Fortschritte bei Qubit-Fidelity und Circuit Depth hier jedoch deutliche Verbesserungen bringen.

\subparagraph{Clustering und Dimensionsreduktion}
Im Bereich des unüberwachten Lernens richten Gujju et al. den Fokus auf Quanten-Clustering, insbesondere den Quantum K-Means Algorithmus. Dabei wird die Distanz zwischen Datenpunkten mittels SWAP-Tests im Hilbertraum ermittelt. Dies ermöglicht effizienteres Clustering, insbesondere bei nichtlinear separierbaren Strukturen – vorausgesetzt, die Initialisierung der Clusterzentren ist ausreichend präzise (Gujju et al., 2024, S.5–6).
Ein weiteres Forschungsfeld ist die Quantum-enhanced Principal Component Analysis (qPCA), die auf Quantum Phase Estimation basiert und theoretisch eine exponentiell schnellere Zerlegung großer Kovarianzmatrizen als klassische PCA erlaubt. Der praktische Nutzen ist jedoch aufgrund begrenzter Qubit-Ressourcen derzeit auf Proof-of-Concept-Anwendungen beschränkt (ebd., S.6).
Zunehmend erforscht werden hybride Verfahren, die klassische Methoden wie PCA oder t-SNE mit quantenmechanischer Kodierung und anschließendem Quantum-Clustering kombinieren. Diese kommen v.a. in Bereichen wie medizinischer Diagnostik, Materialwissenschaft oder Genomik zum Einsatz, wo hochdimensionale Daten effizient transformiert und segmentiert werden müssen.

\vspace{1.5em}
\noindent\textbf{Sicherheit und Kryptografie}

\noindent
Quantum Machine Learning (QML) gilt als Schlüsseltechnologie an der Schnittstelle von maschinellem Lernen und Quanteninformatik. Besonders im sicherheitskritischen Bereich bietet QML Potenzial: Quantenunterstützte Lernverfahren können frühzeitig Muster in Netzwerkverkehr, Malware oder Spam erkennen. Zugleich ergeben sich neue sicherheitstechnische Herausforderungen, da QML-Modelle spezifische Verwundbarkeiten aufweisen. Das Bundesamt für Sicherheit in der Informationstechnik (BSI) betont daher die Notwendigkeit einer frühzeitigen Sicherheitsbewertung – trotz derzeit begrenzter Praxistauglichkeit (QML in the Context of IT Security, S.1–2).
Zur wissenschaftlichen Fundierung initiierte das BSI mit Capgemini und dem Fraunhofer IAIS eine Grundlagenstudie, gefolgt von der vertiefenden SecQML-Studie mit Fraunhofer IKS, adesso SE und Quantagonia GmbH. Im Fokus standen Risiko- und Angriffsszenarien wie adversariale Eingaben, Modellmanipulationen und Inferenzangriffe (Security Aspects of Quantum Machine Learning, S.3–7).
Die Studie zeigt, dass klassische Bedrohungen wie adversariale Inputs oder Datenvergiftung auch in QML auftreten – oft mit quantenspezifischen Effekten. So können manipulierte Eingabewinkel beim Angle-Encoding Fehlklassifikationen auslösen. Auch Transpilation und Quantenmessungen bieten Angriffsflächen (ebd., S.8–11).
Experimente mit QNNs unter Rauscheinflüssen und QSVMs unter Eingabemanipulationen belegten, dass schon kleine Änderungen in Encoding oder Schaltung die Modellstabilität beeinträchtigen (ebd., S.12–14). Als Gegenmaßnahmen werden u.a. Encoding-Validierung, Transpilations-Randomisierung und Monitoring des Readouts vorgeschlagen. Ein zentrales Hindernis bleibt der Mangel an Standards, Zertifizierungen und normierten Tests (ebd., S.14–16).
Zur Weiterentwicklung startete das BSI 2025 das Projekt QML-ESA (Quantum Machine Learning – Extended Security Analysis). Es untersucht realitätsnahe Angriffe bei komplexeren Daten, realistischen Rauschprofilen und fortgeschritteneren Modellen. Im Fokus stehen u.a. Bedrohungen beim Encoding, Readout und Training sowie geeignete Schutzmaßnahmen. Die Ergebnisse sollen 2026 veröffentlicht werden (bsi.bund.de/QML, Stand: März 2025).
Ein Kernziel ist die Entwicklung standardisierter, fehlertoleranter Methoden zur sicheren Datenkodierung. Auch die Übertragung klassischer Intrusion-Detection-Verfahren auf QML-Systeme wird geprüft. Das BSI positioniert sich damit als Impulsgeber für vertrauenswürdige QML-Anwendungen (ebd., Abschnitt „Zielsetzung QML-ESA“).
Abschließend erprobte das BSI einen QSVM-Demonstrator zur Spam-Erkennung: Ein klassischer Datensatz wurde per Angle-Encoding auf einem Quantenemulator verarbeitet. Die QSVM erzielte eine vergleichbare Klassifikationsleistung wie das klassische Modell, zeigte aber höhere Robustheit gegenüber Störungen – bedingt durch strukturelle Eigenschaften des Quantum Kernels (QML Demonstrator, S.3–9). Trotz vielversprechender Ergebnisse bleibt es ein Proof of Concept, da reale Quantenhardware derzeit noch nicht skalierbar ist (ebd., S.10–12).



\subsubsection{Chancen, Herausforderungen und Grenzen von QML}
Quantum Machine Learning gilt als vielversprechender Ansatz zur Lösung komplexer Lernaufgaben, die mit klassischen Algorithmen nicht effizient lösbar sind. Erste Studien zeigen Potenziale in Bereichen wie Klassifikation, Dimensionsreduktion und quantenphysikalischer Simulation \citep{jerbiShadowsQuantumMachine2024} \cite{peralGarcia2024}. Zugleich stehen QML-Modelle noch vor erheblichen technischen, algorithmischen und anwendungsspezifischen Herausforderungen, die ihren praktischen Einsatz einschränken \cite{uddin2024, gujju2024} \citep{gujjuQuantumMachineLearning2024}.


\vspace{1em}
\noindent\textbf{Potenziale und Chancen}

\noindent
Quantum Machine Learning (QML) bietet ein faszinierendes Potenzial zur Lösung von Problemen, die mit klassischen Verfahren entweder nur ineffizient oder gar nicht lösbar sind. Die theoretische Grundlage dafür liefern die quantenmechanischen Prinzipien der Superposition und Verschränkung, welche es ermöglichen, exponentiell große Zustandsräume gleichzeitig zu adressieren. Diese Eigenschaft erlaubt nicht nur eine parallele Datenverarbeitung, sondern eröffnet grundsätzlich neue Rechenparadigmen, die über die Grenzen klassischer Computer hinausgehen \cite{peralGarcia2024}.

Ein zentrales Beispiel hierfür ist die Vorhersage von Grundzustandseigenschaften stark wechselwirkender Quantensysteme, eine Aufgabe, die klassisch als unlösbar gilt. Jerbi et al. \cite{jerbi2024} betonen, dass QML-Modelle, insbesondere sogenannte Shadow Models, in der Lage sind, genau solche lerntechnischen Herausforderungen zu adressieren \cite[1]{jerbi2024}. Diese Modelle zeigen darüber hinaus die Fähigkeit, quantenbasiert nützliche Informationen zu berechnen, die in einem klassischen Evaluationskontext weiterverwendet werden können – und dies für Aufgaben, die rein klassisch nicht lösbar wären \cite[6]{jerbi2024}.

Konkret zeigen Studien zu QSVM, dass geeignete Feature-Mappings auf Quantenebene eine effiziente Trennung hochdimensionaler Daten erlauben können. Kavitha und Kaulgud \cite{kavitha2024} demonstrieren, dass durch gezielte Auswahl und Konfiguration von Quanten-Kernels eine signifikante Verbesserung gegenüber klassischen SVMs erzielt werden kann, insbesondere in Szenarien mit komplexer Merkmalsverteilung. Diese Ergebnisse legen nahe, dass QML nicht nur eine höhere Rechenleistung bietet, sondern auch eine qualitativ neuartige Art der Datenrepräsentation und Klassifikation ermöglicht.

Darüber hinaus zeigen empirische Arbeiten von Gujju et al. \cite{gujju2024}, dass bereits erste realweltliche Anwendungen in Domänen wie Hochenergiephysik, Finanzwesen und medizinischer Bilddiagnostik möglich sind. QML-Ansätze wurden hier erfolgreich zur Mustererkennung, Vorhersage und Merkmalsextraktion eingesetzt – teilweise in hybrider Form, wobei klassische Vorverarbeitung mit quantenbasierter Verarbeitung kombiniert wurde. Dies deckt sich auch mit der Einschätzung von Jerbi et al. \cite[1]{jerbi2024}, dass durch klassische Deployment-Techniken eine breite Anwendbarkeit dieser Modelle in praktischen Kontexten möglich wird.

Ein weiterer vielversprechender Forschungsstrang liegt in der Beschleunigung spezifischer ML-Verfahren, insbesondere im Bereich der Dimensionsreduktion, des Clustering und der Generierung. So beschreibt Zhang et al. \cite{zhang2024} in ihrer Arbeit zu Quantum Denoising Diffusion Probabilistic Models (QuDDPMs), wie durch quantenmechanisch angereicherte Trainingsprozesse Daten mit hoher struktureller Komplexität modelliert werden können. Auch Quantum-enhanced PCA und Quantum GANs gelten als aussichtsreiche Ansätze zur Verarbeitung komplexer Datenverteilungen, die klassisch nur mit großem Rechenaufwand darstellbar wären.

Trotz der Einschränkungen heutiger Hardware konnten bereits vielversprechende Resultate auf sogenannten NISQ-Systemen erzielt werden. Diese Geräte verfügen zwar noch nicht über vollständige Fehlertoleranz oder große Qubit-Zahlen, eignen sich jedoch für experimentelle Validierungen und erste Produktivprototypen \cite{peralGarcia2024, gujju2024}.

Zusammenfassend lässt sich festhalten, dass Quantum Machine Learning nicht nur theoretisch bedeutsame Vorteile bietet, sondern auch erste empirische Erfolge in praxisnahen Anwendungen zeigt. Die größten Potenziale liegen aktuell in Nischenanwendungen mit hoher algorithmischer Komplexität, strukturell tiefen Daten und begrenztem Volumen. Die Kombination aus neuer algorithmischer Struktur, quantenphysikalischer Parallelität und klassischer Nutzbarkeit macht QML zu einem der vielversprechendsten Felder moderner Computerwissenschaften.


\vspace{1em}
\noindent\textbf{Technologische und algorithmische Herausforderungen}

\noindent
Trotz vielversprechender Potenziale steht Quantum Machine Learning (QML) noch immer vor einer Vielzahl technischer und algorithmischer Hindernisse, die eine breite praktische Umsetzung erschweren. Zentrale Herausforderungen betreffen sowohl die physikalische Hardware als auch die zugrundeliegenden Trainingsverfahren, Modellarchitekturen und Bewertungsmethoden.

Ein wesentlicher limitierender Faktor ist der derzeitige Stand der Quantenhardware. Aktuelle Quantenprozessoren befinden sich noch im sogenannten NISQ-Zeitalter (Noisy Intermediate-Scale Quantum) und verfügen typischerweise über weniger als 100 Qubits. Diese Systeme sind nicht fehlertolerant, stark verrauscht und haben nur kurze Kohärenzzeiten. Das bedeutet, dass komplexe QML-Modelle – insbesondere solche mit vielen Gatterebenen oder parametrischen Schaltkreisen – entweder instabil laufen oder gar nicht ausführbar sind \cite[S.~13]{uddin2024, gujju2024, peralgarcia2024}. \cite[S.~10--11]{tychola2023} weisen zudem darauf hin, dass Dekohärenz und zunehmende Messrauschen mit wachsender Schaltkreiskomplexität zu erheblichen Verzerrungen führen. Fehlerkorrekturverfahren existieren zwar theoretisch, sind aber gegenwärtig zu rechenintensiv für den praktischen Einsatz \cite[S.~10]{tychola2023}.

Eine weitere technische Barriere liegt im Training quantenbasierter Modelle wie Variational Quantum Circuits (VQCs) oder Quantum Neural Networks (QNNs). Diese Architekturen leiden unter dem sogenannten \textit{barren plateau}-Problem: In bestimmten Regionen des Parameterraums verschwinden die Gradienten nahezu vollständig, was die Optimierung des Modells durch klassische Gradientenverfahren praktisch unmöglich macht \cite{liu2024, tychola2023}. Auch Shadow Models, die eine hybride Auswertung ermöglichen, sind hiervon nicht ausgenommen. \cite[S.~4]{jerbi2024} warnen vor einer exponentiellen Sample-Komplexität bei hochdimensionalen Eingabedaten, was die Anwendbarkeit in realen Szenarien stark einschränken kann.

Ein weiterer kritischer Aspekt betrifft die Auswahl geeigneter Quantum Feature Maps. \cite{kavitha2024} zeigen, dass die Leistungsfähigkeit von QSVMs stark von der Wahl und Konfiguration dieser Features abhängt. Es fehlen bislang systematische Methoden, um optimale Mappings für unterschiedliche Datensätze zu identifizieren. Gleichzeitig existieren keine standardisierten Trainingsprotokolle oder Optimierer, die hardwareübergreifend zuverlässig funktionieren \cite[S.~14]{uddin2024}.

Zudem mangelt es an einheitlichen Benchmarks zur Bewertung der Leistungsfähigkeit von QML-Ansätzen. Während klassische ML-Forschung auf bewährte Vergleichsdatensätze wie MNIST, ImageNet oder GLUE zurückgreifen kann, existieren im Bereich QML bislang kaum standardisierte Testumgebungen. Viele Studien arbeiten mit eigens generierten Datensätzen und simulierten Umgebungen, was die Vergleichbarkeit und Reproduzierbarkeit erheblich einschränkt \cite[S.~14--15]{uddin2024, gujju2024}. Letztere fordern deshalb die Entwicklung einer \textit{Quantum Learning Suite} mit standardisierten Klassifikations- und Clusteringbenchmarks, definierten Trainingsprotokollen und klaren Spezifikationen zu Featurekodierung und Hardwareeinsatz.

Nicht zuletzt sind auch Fragen der Interpretierbarkeit ungelöst. Während in der klassischen KI Explainable AI (XAI) zunehmend an Bedeutung gewinnt, fehlt es im QML-Bereich an entsprechenden Konzepten. Die komplexe Natur quantenmechanischer Zustände erschwert es, Entscheidungsprozesse nachvollziehbar zu machen. \cite[S.~15]{uddin2024} plädieren daher für die Etablierung eines neuen Forschungsfeldes: Explainable Quantum AI (XQAI), um Vertrauen und Sicherheit im praktischen Einsatz zu stärken. Auch \cite[S.~2--3]{jerbi2024} machen deutlich, dass nicht alle quantenbasierten Modelle für eine klassische Evaluation geeignet sind. Insbesondere Shadow-Modelle sind nur für bestimmte Klassen von Zuständen und Observablen effizient einsetzbar.



\vspace{1em}
\noindent\textbf{Anwendungsbezogene Risiken und offene Fragen}

\noindent
Neben den technischen Hürden bestehen auch auf Anwendungsebene eine Reihe offener Fragen und Risiken, die den produktiven Einsatz von QML aktuell einschränken. Ein zentrales Problem besteht in der unklaren Vorteilhaftigkeit vieler quantenbasierter Modelle gegenüber klassischen Alternativen. \cite{bowles2024} betonen, dass zahlreiche Studien ihre quantenmechanischen Verfahren auf kleinen, künstlichen Datensätzen testen, ohne deren Vorteil gegenüber etablierten Machine-Learning-Verfahren hinreichend zu belegen. Auch \cite{jerbi2024} weisen darauf hin, dass klassische Modelle in manchen Fällen eine vergleichbare Leistung erzielen können wie Shadow Models – und das ohne den Aufwand einer quantenmechanischen Implementierung \cite[S.~2]{jerbi2024}.

Ein weiteres Risiko betrifft die adversariale Robustheit. Die Fragilität quantenmechanischer Zustände macht QML-Systeme potenziell anfällig für gezielte Störimpulse oder manipulierte Eingabedaten. \cite{gujju2024} sehen hier einen dringenden Forschungsbedarf, insbesondere im Hinblick auf hybride Architekturen, die sowohl klassische als auch quantenbasierte Komponenten enthalten. Auch \cite{uddin2024} fordern systematische Robustheitsanalysen analog zu denen in der klassischen KI-Forschung \cite[S.~15]{uddin2024}.

Hinzu kommt die mangelnde Interpretierbarkeit vieler QML-Modelle. Die intransparenten Rechenprozesse innerhalb quantenmechanischer Systeme machen es schwierig, Entscheidungen nachzuvollziehen oder zu validieren. \cite{tychola2023} verweisen darauf, dass die Interpretation der Ergebnisse tiefgreifende Fachkenntnisse erfordert und sich eine breite Anwendung dadurch erschwert \cite[S.~11]{tychola2023}.

Auch der Ressourcenbedarf bleibt eine praktische Hürde. Trotz verfügbarer Cloud-Zugänge über Plattformen wie IBM Q oder Xanadu ist der Zugriff auf Quantenhardware begrenzt und mit erheblichen Rechenzeiten verbunden \cite{peralgarcia2024}. \cite{zhang2020} merken zudem an, dass die klassischen Ein- und Ausgabestrukturen in vielen Fällen den theoretischen Speedup durch QML zunichtemachen können \cite[S.~19]{zhang2020}. Das Einlesen großer Datenmengen und die Extraktion quantenmechanischer Ergebnisse sind nicht nur teuer, sondern in manchen Szenarien sogar praktisch unmöglich.

Abschließend bleibt festzuhalten, dass viele QML-Modelle zwar ein hohes theoretisches Potenzial aufweisen, ihr Reifegrad jedoch noch begrenzt ist. Besonders Gate-basierte Ansätze sind stark simulationsbasiert und abhängig von spezifischer Hardware. Die Skalierung auf größere Systeme bleibt herausfordernd. Dennoch wird einzelnen Verfahren wie der QSVM mit effizientem Kernel-Encoding ein mittelfristiger Anwendungsvorteil bei strukturierten Klassifikationsaufgaben zugeschrieben \cite[S.~10--12]{uddin2024}.


\subsection{Fazit}

\vspace{2em}

\section{Kryptographie}
\subsection{Einleitung}
Kryptographische Verfahren sind integraler Bestandteil moderner digitaler Infrastrukturen. Insbesondere asymmetrische Kryptosysteme wie RSA, Diffie-Hellman und elliptische Kurvenkryptographie basieren auf der angenommenen Schwierigkeit mathematischer Probleme wie der ganzzahligen Faktorisierung oder der Berechnung diskreter Logarithmen. Diese Probleme gelten im klassischen Rechenmodell als nur mit exponentiellem Aufwand lösbar, wodurch die genannten Verfahren als sicher eingestuft werden. Symmetrische Kryptographie, beispielsweise AES, bietet hingegen Sicherheitsgarantien auf Basis der Schlüsselraumgröße und ist gegenwärtig gegenüber klassischen Angriffen hinreichend resistent.

Diese Sicherheitsannahmen werden jedoch durch die Entwicklung leistungsfähiger Quantencomputer substanziell infrage gestellt. Mit Shors Algorithmus (Shor 1994) wurde erstmals ein quantenmechanisches Verfahren vorgestellt, das die Faktorisierung großer Zahlen sowie die Berechnung diskreter Logarithmen in polynomialer Zeit ermöglicht und somit die Grundlage nahezu aller etablierten asymmetrischen Verfahren kompromittiert. Ergänzend dazu führt Grovers Algorithmus zu einer quadratischen Beschleunigung von Suchproblemen, was insbesondere symmetrische Verfahren betrifft, bei denen dadurch eine effektive Halbierung der Schlüssellänge notwendig wird, um das ursprüngliche Sicherheitsniveau zu erhalten (Grover 1996; vgl. auch Shor u Grover, Überblick in [63]).

Diese Algorithmen stellen keine abstrakten theoretischen Bedrohungen dar, sondern begründen bereits heute reale Risiken, insbesondere im Kontext sogenannter Harvest-now-decrypt-later-Angriffe. In diesem Szenario werden verschlüsselte Kommunikationsinhalte langfristig gespeichert, um sie bei Verfügbarkeit eines hinreichend leistungsfähigen Quantencomputers retrospektiv zu entschlüsseln (Mosca 2018). Die praktische Relevanz ergibt sich dabei aus der Latenz zwischen dem heutigen Einsatz kryptographischer Verfahren und der potentiellen Verfügbarkeit skalierbarer Quantencomputer. Michele Mosca beschreibt diese Problematik entlang dreier Größen: der angestrebten Geheimhaltungsdauer (x), der Migrationszeit zu quantenresistenten Verfahren (y) und der geschätzten Zeit bis zum Bruch aktueller Verfahren (z). Ist x + y > z, besteht akuter Handlungsbedarf, da zukünftige Angreifer auf heute übertragene Daten zugreifen können, ohne dass dies zum Zeitpunkt der Kommunikation verhindert werden kann (Mosca S. 38f.).

Diese Bedrohungslage hat eine breite wissenschaftliche, industrielle und regulatorische Reaktion ausgelöst. Insbesondere die vom U.S. National Institute of Standards and Technology (NIST) koordinierte Standardisierung postquantenkryptographischer Verfahren zielt darauf ab, praktikable Alternativen zu heute verbreiteten Algorithmen zu identifizieren, zu evaluieren und langfristig zu etablieren. Im Rahmen der vierten Wettbewerbsrunde wurden Verfahren wie CRYSTALS-Kyber, CRYSTALS-Dilithium und SPHINCS+ als zukünftige Standards ausgewählt \cite{alagic_status_2025}.

Das vorliegende Kapitel analysiert diese Entwicklung aus kryptographischer Perspektive. Es beginnt mit der detaillierten Darstellung der durch Quantenalgorithmen verursachten Bedrohung für symmetrische und asymmetrische Kryptosysteme und ordnet das Harvest-now-decrypt-later-Szenario systematisch ein. Daran anschließend werden aktuelle technische, normative und strategische Reaktionen untersucht. Ziel ist es, die Implikationen für die Gestaltung zukunftsfähiger Sicherheitssysteme darzustellen und die Notwendigkeit einer rechtzeitigen und koordinierten Umstellung auf quantenresistente Verfahren herauszuarbeiten.

\subsection{Technologische Grundlagen}
\subsubsection{Post-Quantum-Cryptography}
\noindent\textbf{Einleitung und Motivation}

\noindent
Die Sicherheit klassischer kryptographischer Verfahren basiert auf Problemen wie der Faktorisierung großer Zahlen oder der Berechnung diskreter Logarithmen – Aufgaben, die für klassische Rechner schwer, aber für zukünftige Quantencomputer effizient lösbar sind. Insbesondere Shors Algorithmus (1994) kann sowohl RSA als auch elliptische Kurven in polynomialer Zeit brechen, während Grovers Algorithmus (1996) symmetrische Verfahren wie AES durch quadratische Beschleunigung schwächt. Diese Entwicklungen bedrohen die langfristige Vertraulichkeit digitaler Kommunikation – besonders in Bereichen, in denen Daten über Jahrzehnte hinweg sicher bleiben müssen, etwa im Gesundheits- oder Militärwesen.
 (Chen et al., 2016, Sec.1–2; Mosca, 2018, S.38–39)
Post-Quantum Cryptography (PQK) verfolgt daher das Ziel, neue kryptographische Verfahren zu entwickeln, deren Sicherheit auch gegenüber leistungsfähigen Quantencomputern erhalten bleibt. Dabei handelt es sich um rein klassische Algorithmen, die auf mathematischen Problemen beruhen, für die bislang keine effizienten quantenbasierten Angriffe bekannt sind. Die zentrale Motivation hinter PQK liegt also nicht in kurzfristigen Leistungsverbesserungen, sondern in der Absicherung digitaler Infrastrukturen gegen zukünftige Bedrohungen – ein Paradigmenwechsel von „komplexitätsbasierter“ zu „quantenresistenter“ Sicherheit.
(Bernstein et al., 2009, Kap.1; Chen et al., 2016, Sec.3; Mosca, 2018, S.39–40)


\vspace{1em}
\noindent\textbf{Abgrenzung und Einordnung}

\noindent
Post-Quantum Cryptography (PQK) darf nicht mit Quantenkryptographie verwechselt werden. Während Quantenkryptographie wie Quantum Key Distribution (QKD) physikalische Quanteneffekte und spezielle Hardware (z.B. Photonenkanäle) nutzt, ist PQK vollständig klassisch: Die Verfahren laufen auf herkömmlichen Computern und beruhen auf mathematischen Problemstellungen, die auch Quantencomputer voraussichtlich nicht effizient lösen können.
 (Chen et al., 2016, Sec.2.2; Mosca, 2018, S.39)
PQK verfolgt somit einen softwarebasierten Ansatz, der sich weitgehend in bestehende Infrastrukturen integrieren lässt – etwa durch Austausch von Schlüsselaustausch- oder Signaturalgorithmen in Protokollen wie TLS, SSH oder S/MIME. Im Gegensatz zu QKD, das lediglich den Schlüsselaustausch absichert, können PQK-Verfahren alle Aufgaben moderner Kryptographie abdecken: Vertraulichkeit, Authentizität und Integrität.
 (Bernstein et al., 2009, Kap.1; Chen et al., 2016, Sec.4; National Academies, 2019, S.95–97)
Die beiden Ansätze – PQK und Quantenkryptographie – gelten daher nicht als Konkurrenz, sondern als komplementär: PQK bietet eine pragmatische Übergangslösung mit globaler Skalierbarkeit, während QKD auf langfristige physikalische Sicherheit zielt, aber hohe infrastrukturelle Anforderungen stellt.
 (Mosca, 2018, S.40; National Academies, 2019, S.97–98)


\vspace{1em}
\noindent\textbf{Grundlagen der PQK}

\noindent
Im Kern zielt Post-Quantum Cryptography darauf ab, kryptographische Verfahren zu entwickeln, die auch durch Quantencomputer nicht effizient gebrochen werden können. Dabei basieren PQK-Verfahren auf mathematischen Problemstellungen wie Gitterproblemen, Code-Fehlerkorrektur oder Hashfunktionen, für die es bislang weder klassische noch quantenbasierte effiziente Algorithmen gibt. Diese Probleme sind in vielen Fällen sogar NP-schwer oder lassen sich auf solche reduzieren.
 (Chen et al., 2016, Sec.3; Bernstein et al., 2009, Kap.1)
Ein großer Vorteil von PQK liegt darin, dass keine neue Hardware erforderlich ist. Die Algorithmen können als Software auf bestehenden Systemen laufen und sind dadurch schnell skalierbar und kosteneffizient einsetzbar. Das macht PQK besonders attraktiv für Szenarien, in denen langfristige Datensicherheit gewährleistet sein muss – etwa im Kontext von Cloud-Infrastrukturen, öffentlicher Verwaltung oder medizinischer Archivierung.
 (Mosca, 2018, S.39–40; National Academies, 2019, S.92–93)
Insgesamt ermöglicht PQK einen schrittweisen Übergang von heute verbreiteten Verfahren zu quantensicheren Alternativen – ohne Brüche in Protokollen oder Softwarearchitekturen. Dies ist entscheidend, um Sicherheitssysteme rechtzeitig vor dem breiten Einsatz von Quantencomputern zu schützen.
 (Chen et al., 2016, Sec.4; Bernstein et al., 2009, Kap.1)

\vspace{1em}
\noindent\textbf{Problemklassen und Verfahren}
\subparagraph{Gitterbasierte Verfahren}
Gitterbasierte Kryptographie basiert auf geometrischen Problemen im mehrdimensionalen Raum, insbesondere auf dem Learning-With-Errors-Problem (LWE) und dem Shortest Vector Problem (SVP). Diese gelten sowohl für klassische als auch für Quantencomputer als hart. Die Verfahren zeichnen sich durch Effizienz, breite Anwendbarkeit (z.B. Signaturen, Schlüsselaustausch) und gute Sicherheitsnachweise aus. Prominente Vertreter sind Kyber (Key Encapsulation Mechanism) und Dilithium (Signaturverfahren), die von NIST standardisiert wurden.
 (Regev, 2005; Alagic et al., 2023; NIST, 2024 – FIPS 203)
 
\subparagraph{Codebasierte Verfahren}
Diese Ansätze nutzen die Schwierigkeit, zufällige lineare Fehlerkorrekturcodes zu dekodieren. Das klassische McEliece-System, das bereits 1978 vorgestellt wurde, ist bis heute gegen bekannte Angriffe – einschließlich quantenbasierter – resistent. Neuere Varianten wie MDPC-McEliece verringern den Speicherbedarf und ermöglichen effizientere Implementierungen. Hauptvorteil: extrem lange kryptographische Erfahrung; Nachteil: große Schlüssellängen.
 (Misoczki et al., 2013; Bernstein et al., 2009, Kap.10)
 
\subparagraph{Hashbasierte Verfahren}
Diese Verfahren stützen sich allein auf die Sicherheit kryptographischer Hashfunktionen. Bekannteste Vertreter sind Lamport-Signaturen, Merkle-Bäume und darauf basierende Standards wie SPHINCS+. Da sie keine algebraische Struktur ausnutzen, gelten sie als sehr robust gegen strukturelle Angriffe – allerdings sind die Signaturen oft lang und benötigen komplexes Management für Schlüsselzustände.
 (Lamport, 1979; Bernstein et al., 2009, Kap.5; Chen et al., 2016, Sec.3.2)


\vspace{1em}
\noindent\textbf{Standardisierung und Status quo}

\noindent
Im Jahr 2016 startete das National Institute of Standards and Technology (NIST) einen mehrstufigen Auswahlprozess zur Standardisierung quantenresistenter Kryptographie. Ziel war es, aus Hunderten international eingereichter Vorschläge robuste, breit einsetzbare und quantensichere Verfahren für digitale Signaturen und Schlüsselaustausch zu identifizieren.
 (Alagic et al., 2023, S.1–2)
Nach drei Wettbewerbsrunden wurden 2022 drei Verfahren für die Standardisierung ausgewählt: Kyber (Key Encapsulation), Dilithium (digitale Signaturen) und SPHINCS+ (hashbasiertes Signatursystem). Im März 2024 veröffentlichte NIST den ersten offiziellen PQC-Standard – FIPS 203, der Kyber spezifiziert. Die Standards zu Dilithium und SPHINCS+ folgen zeitnah.
 (NIST, 2024 – FIPS 203)
Die Relevanz für Wirtschaft und Staat ist hoch: Durch die Softwarekompatibilität dieser Verfahren ist ein schrittweiser Migrationspfad möglich – etwa in TLS, VPNs, Firmware-Updates oder digitaler Kommunikation in Verwaltung und Industrie. Unternehmen wie IBM, Cloudflare oder Amazon Web Services haben erste PQK-Pilotprojekte in Betrieb genommen.
 (Alagic et al., 2023, S.3–5)
 

\subsubsection{Quantum-Key-Distribution}
\noindent\textbf{Einleitung und Motivation}

\noindent
Die Verteilung kryptographischer Schlüssel zählt zu den zentralen Herausforderungen moderner Informationssicherheit. Klassische Verfahren stützen sich auf die angenommene Schwierigkeit mathematischer Probleme wie der Faktorisierung großer Zahlen oder diskreter Logarithmen. Diese gelten zwar aktuell als sicher, könnten jedoch durch leistungsfähige Quantencomputer kompromittiert werden.
Quantum Key Distribution (QKD) verfolgt einen grundlegend anderen Ansatz: Sie nutzt nicht mathematische Komplexität, sondern quantenmechanische Prinzipien wie die Unschärferelation, Verschränkung und das No-Cloning-Theorem, um Sicherheit zu garantieren (Scarani et al., 2009).
Ein QKD-Protokoll erlaubt es zwei Parteien – klassisch Alice und Bob –, über einen Quantenkanal einen gemeinsamen geheimen Schlüssel zu erzeugen. Dieser beruht darauf, dass jede Messung durch einen Angreifer (Eve) zwangsläufig den Quantenzustand verändert und damit erkennbar stört (Bennett und Brassard, 1984).
Der Schlüssel wird anschließend über einen klassischen Kanal authentifiziert und genutzt. Da jeder Abhörversuch Spuren hinterlässt, gilt QKD theoretisch als informationstheoretisch sicher – unabhängig von technischer oder rechnerischer Kapazität (Ekert, 1991).


\vspace{1em}
\noindent\textbf{Abgrenzung und konzeptionelle Einordnung}

\noindent
Quantum Key Distribution (QKD) dient ausschließlich der sicheren Verteilung kryptographischer Schlüssel – die eigentliche Datenübertragung erfolgt weiterhin klassisch und muss gesondert verschlüsselt werden. QKD allein stellt also kein vollständiges Sicherheitssystem dar (Scarani et al., 2009).
Zudem erfordert QKD stets eine Authentifikation der Kommunikationspartner, um Angriffe wie „Man-in-the-Middle“ zu verhindern. Diese Authentifikation kann durch klassische Verfahren oder durch Post-Quantum-Kryptographie (PQK) erfolgen, die auf mathematisch schwer lösbaren Problemen basiert (Mosca, 2018).
QKD und PQK verfolgen dabei unterschiedliche Ansätze: Während PQK softwarebasiert ist und klassische Systeme absichert, nutzt QKD physikalische Gesetze zur Sicherheit. PQK ist leichter zu integrieren, QKD bietet im Idealfall theoretisch unbedingte Sicherheit, erfordert aber quantenphysikalische Infrastruktur (National Academies, 2019).
QKD ist damit ein spezialisierter Baustein, kein Ersatz für klassische oder PQK-gestützte Systeme, sondern ein ergänzender Bestandteil quantensicherer Kommunikation.


\vspace{1em}
\noindent\textbf{Grundlagen der Protokolle}

\subparagraph{BB84-Protokoll}
Das BB84-Protokoll wurde 1984 von Bennett und Brassard entwickelt und gilt als das erste konkrete Verfahren zur Quantum Key Distribution. Die zentrale Idee besteht darin, einzelne Photonen in zufälligen Polarisationszuständen zu senden – etwa horizontal/vertikal (|, ⎯) und diagonal (⧬, ×).
Der Sender (Alice) wählt dabei zufällig eine Basis (z.B. rectilinear oder diagonal) und sendet ein entsprechend polarisiertes Photon. Der Empfänger (Bob) misst das Photon ebenfalls in einer zufällig gewählten Basis. Nur wenn Sender und Empfänger dieselbe Basis verwendet haben, stimmen die Messergebnisse überein und können als Teil des gemeinsamen Schlüssels übernommen werden.
Ein Abhörversuch durch einen Dritten (Eve) würde das Photon stören und damit statistisch erkennbare Fehler erzeugen – was Grundlage der theoretischen Sicherheit ist.
 (Bennett & Brassard, 1984)
 
\subparagraph{E91-Protokoll}
Das E91-Protokoll, vorgeschlagen von Ekert (1991), basiert nicht auf Polarisation und Basiswahl, sondern auf quantum-entanglement (Verschränkung). Dabei wird ein Paar verschränkter Photonen erzeugt, von denen eines zu Alice und eines zu Bob geschickt wird.
Die beiden messen ihre Photonen in jeweils unterschiedlichen, zufällig gewählten Messrichtungen. Aufgrund der Quantenverschränkung sind die Messergebnisse korreliert – stärker, als es nach klassischer Physik möglich wäre. Diese Korrelationen können mit Bell-Tests überprüft werden und dienen als Sicherheitsnachweis: Wird die Bell-Ungleichung verletzt, ist ein Abhörversuch ausgeschlossen.
(Ekert, 1991)

\subparagraph{Vergleich BB84 vs. E91}
Beide Protokolle ermöglichen die sichere Generierung gemeinsamer Schlüssel, unterscheiden sich aber im Sicherheitsmodell:
BB84 stützt sich auf die Unmöglichkeit einer störungsfreien Messung und erfordert Vertrauen in die Quelle und die Geräte.
E91 nutzt dagegen verschränkte Zustände und erlaubt die Überprüfung der Sicherheit durch beobachtbare Bell-Korrelationen – potenziell sogar device-independent, also ohne vollständiges Vertrauen in die Hardware.

\vspace{1em}
\noindent\textbf{Erweiterte QKD-Konzepte}

\noindent
Um theoretisch sichere QKD-Protokolle an reale Bedingungen anzupassen, wurden mehrere Erweiterungen entwickelt, die bekannte Schwachstellen adressieren.
Decoy-State QKD
In der Praxis ist es oft technisch schwierig, wirklich einzelne Photonen zu senden. Viele Systeme verwenden daher abgeschwächte Laserpulse, bei denen gelegentlich mehrere Photonen gleichzeitig ausgesendet werden. Ein Angreifer könnte diese ausnutzen, indem er einen Photonenteil abzweigt und das restliche Signal weitersendet – ein sogenannter Photon-Splitting-Angriff.
Decoy-State QKD löst dieses Problem, indem zusätzlich zum eigentlichen Signal auch sogenannte Täuschungspulse („decoy states“) mit variabler Intensität gesendet werden. Diese erlauben es, den Kanal zu testen und solche Angriffe zu erkennen, ohne die Schlüsselerzeugung zu stören.
 (Scarani et al., 2009, Abschn. III.C)
Device-Independent QKD (DI-QKD)
Bei herkömmlichen Protokollen wird vorausgesetzt, dass die verwendeten Geräte korrekt und vertrauenswürdig arbeiten. In der Realität kann das aber durch Manipulation oder Fertigungsfehler nicht immer garantiert werden.
DI-QKD umgeht dieses Problem, indem es die Sicherheit direkt aus den beobachteten Messdaten ableitet – unabhängig von der internen Funktionsweise der Geräte. Möglich wird das durch quantenmechanische Bell-Tests, mit denen gezeigt werden kann, dass echte Nichtlokalität vorliegt und somit ein Abhören ausgeschlossen ist. Damit ist DI-QKD theoretisch besonders robust – aber experimentell sehr anspruchsvoll.
 (Scarani et al., 2009, Abschn. V.C)

\vspace{1em}
\noindent\textbf{Sicherheitsprinzipien und Angriffsmodelle}

\noindent
Die Sicherheit von QKD beruht auf fundamentalen Prinzipien der Quantenmechanik. Ein zentrales Element ist die Messstörung: Wird ein Quantenobjekt – etwa ein Photon im BB84-Protokoll – vom Angreifer gemessen, verändert sich sein Zustand. Dadurch entstehen Fehler in der Bitfolge, die von den Kommunikationspartnern erkannt werden können. Zudem verhindert das No-Cloning-Theorem, dass ein unbekannter Quantenzustand exakt kopiert werden kann. Damit ist es einem Angreifer prinzipiell unmöglich, eine perfekte Kopie der übertragenen Information zu erstellen.
 (Scarani et al., 2009)
Trotz dieser theoretischen Sicherheit existieren Angriffsmodelle, die Schwächen im Protokoll oder im Ablauf ausnutzen – etwa durch Ausnutzung falsch gewählter Basen oder durch die Manipulation von Messprozessen, z.B. durch das sogenannte Basis-Guessing. Auch Detektor-Manipulationen werden diskutiert, sofern Annahmen über das ideale Verhalten der Geräte nicht erfüllt sind.
 (Scarani et al., 2009)
Insgesamt gilt QKD als informationstheoretisch sicher, sofern alle theoretischen Annahmen erfüllt sind. In der Praxis jedoch hängt die tatsächliche Sicherheit stark von der korrekten Implementierung und der Kontrolle externer Fehlerquellen ab. Die Diskrepanz zwischen Theorie und Realität ist daher ein aktives Forschungsfeld, insbesondere im Hinblick auf Standards, Geräteverifikation und Modellierung von Angriffen.
 (Mosca, 2018)


\subsubsection{Quantum-Random-Number-Generator}
\noindent\textbf{Einleitung und Motivation}

\noindent
Ein Quantum Random Number Generator (QRNG) erzeugt echte Zufallszahlen auf Basis quantenmechanischer Prozesse. Im Gegensatz zu klassischen Zufallszahlengeneratoren, die deterministisch aus einem Startwert („Seed“) arbeiten, ist das Ergebnis bei QRNGs fundamental unvorhersagbar, da es nicht kausal vorherbestimmt ist.
 (Herrero-Collantes & Garcia-Escartin, 2017, Kap.I)
Pseudo-Random Number Generators (PRNGs) erzeugen zwar scheinbar zufällige Folgen, doch sind diese vollständig durch ihren Startwert bestimmt. Wird dieser bekannt oder erraten, lässt sich die gesamte Folge rekonstruieren – ein kritisches Risiko etwa bei Schlüsselgenerierung oder Challenge-Response-Verfahren.
 (NIST SP 800-90B, 2018, Sec.3.1)
QRNGs bieten hier einen klaren Vorteil: Da sie auf quantenphysikalischer Messung beruhen – etwa bei der Entscheidung, ob ein Photon reflektiert oder transmittiert wird – entsteht das Ergebnis erst im Moment der Messung. Selbst der Hersteller kann die Bitfolge nicht vorhersagen, was sie einzigartig und nicht rekonstruierbar macht.
 (Ma et al., 2016, S.2)
In der Kryptographie ist echter Zufall essenziell. Wird ein Schlüssel durch einen schwachen Zufallsprozess erzeugt, kann er unter Umständen rekonstruiert werden. Ein bekanntes Beispiel ist der Debian OpenSSL-Bug (2006–2008), bei dem fehlerhafte Zufallszahlen zu wiederverwendbaren Schlüsseln führten. QRNGs vermeiden solche Risiken, da sie eine verlässliche, nicht-reproduzierbare Entropiequelle liefern.
 (Herrero-Collantes & Garcia-Escartin, 2017, Kap.III)


\vspace{1em}
\noindent\textbf{Einsatzmöglichkeiten in der Kryptographie}

\noindent
QRNGs finden vor allem Anwendung in zwei Bereichen:
\smallskip
In Quantum Key Distribution (QKD): Hier dienen sie zur Erzeugung der zufälligen Bitfolgen, die später verschlüsselt oder verglichen werden. Da die Sicherheit von QKD-Protokollen wie BB84 maßgeblich von der Qualität des Zufalls abhängt, sind QRNGs hier ein kritischer Bestandteil. Ohne echten Zufall wäre z.B. ein Angriff durch Wiederverwendung von Schlüsseln möglich.
\smallskip
In klassischen kryptographischen Protokollen: Auch in herkömmlichen Systemen wie TLS, S/MIME oder SSH kann ein QRNG eine sichere Hardware-Entropiequelle für einen Pseudozufallszahlengenerator (PRNG) sein. Typischerweise wird das QRNG genutzt, um einen PRNG zu „seeden“, also mit einer Startentropie zu versorgen. So wird das Beste aus beiden Welten kombiniert: Echter Zufall + hohe Geschwindigkeit durch PRNGs.
\smallskip
(NIST SP 800-90B, 2018, Sec. 3.1)
Auch moderne Hardware-Sicherheitsmodule (HSMs) und Smartcards integrieren zunehmend QRNGs – sowohl zur Erhöhung der kryptographischen Sicherheit als auch zur Einhaltung regulatorischer Standards. In sensiblen Sektoren wie der Finanzbranche oder beim Militär ist eine verifizierbare Quelle für echten Zufall mittlerweile eine Anforderung.
(Sanguinetti et al., 2014)


\vspace{1em}
\noindent\textbf{Abgrenzung und Rolle in der Kryptographie}

\noindent
Quantum Random Number Generators (QRNGs) sind zentrale Bausteine sicherer Systeme, jedoch keine vollständigen Kryptosysteme. Ihre Aufgabe besteht ausschließlich in der Erzeugung echter, nicht vorhersagbarer Zufallszahlen – sie übernehmen weder Verschlüsselung noch Authentifikation. Als Grundlagentechnologie ersetzen oder ergänzen sie klassische Zufallsquellen wie Pseudozufallszahlengeneratoren (PRNGs).
 (Herrero-Collantes & Garcia-Escartin, 2017, Kap.IV)
QRNGs werden meist mit anderen Verfahren kombiniert. In der Quantenkryptographie, etwa bei Quantum Key Distribution (QKD), liefern sie die nötigen Zufallsbits. Auch in klassischen Anwendungen wie TLS oder VPNs dienen sie als sichere Entropiequelle zur Initialisierung von PRNGs. Moderne Hardware wie Security Modules (HSMs) oder Smartcards nutzt QRNGs zunehmend zur Stärkung kryptographischer Prozesse und zur Einhaltung regulatorischer Standards.
 (Ma et al., 2016, S. 4; Sanguinetti et al., 2014)
Trotz dieser Vorteile sind QRNGs nicht automatisch sicher. Bei kompromittierter oder fehlerhafter Hardware kann ein Angreifer Einfluss nehmen. Ma et al. (2016) unterscheiden daher drei Vertrauensmodelle: praktische QRNGs, die voll auf ihre Hardware angewiesen sind; semi-self-testing QRNGs mit interner Überprüfung; und fully self-testing QRNGs, bei denen quantenmechanische Tests (z. B. Bell-Tests) die Korrektheit unabhängig von der Gerätevertrauenswürdigkeit belegen. Letztere, auch als Device-Independent QRNGs (DI-QRNGs) bekannt, gelten als langfristiges Ziel kryptographischer Forschung.
 (Ma et al., 2016, S.5–6)
QRNGs bieten somit das Potenzial für neue, robuste Sicherheitssysteme – ihre Wirksamkeit entfaltet sich jedoch nur im Zusammenspiel mit weiteren kryptographischen Komponenten. Ihre korrekte Einbindung ist entscheidend für die Integrität moderner Sicherheitsarchitekturen.


\vspace{1em}
\noindent\textbf{Funktionsprinzipien von QRNG}

\noindent
Die Funktionsweise von Quantum Random Number Generators (QRNGs) beruht auf einem fundamentalen Prinzip der Quantenmechanik: Das Ergebnis einer Messung ist nicht im Voraus bestimmbar – es entsteht erst im Moment der Messung. Genau dieses Verhalten macht sich die QRNG-Technologie zunutze, um echte, nicht reproduzierbare Zufallszahlen zu erzeugen.
 (Herrero-Collantes und Garcia-Escartin, 2017, Kap. II)
Ein klassisches Beispiel für dieses Prinzip ist der Strahlteiler-Ansatz (Beam Splitter), bei dem einzelne Photonen auf einen halbtransparenten Spiegel treffen. Das Photon wird dabei entweder reflektiert oder transmittiert – mit einer Wahrscheinlichkeit von jeweils 50 Prozent. Welchen der beiden Wege es nimmt, ist nicht kausal festgelegt, sondern ergibt sich erst durch die Messung an einem der beiden Detektoren. Jeder Detektor steht für eine Bitentscheidung („0“ oder „1“) – und damit entsteht ein echter Zufallswert.
 (Jennewein et al., 2000)
Ein weiteres praktisches Prinzip ist die Nutzung von Laserrauschen – also der zufälligen Phasenschwankungen von Licht, die bei der Emission in einem Laser auftreten. Diese Schwankungen können mit optischen Detektoren erfasst und in digitale Bitfolgen umgewandelt werden. Auch hier ist das Messergebnis nicht vorhersagbar und basiert auf quantenmechanischen Effekten.
(Ma et al., 2016, Abschn. „Practical architectures“)
Neben diesen beiden Grundtypen existieren noch weitere physikalische Realisierungen, die auf unterschiedlichen Quellen quantenmechanischer Unbestimmtheit beruhen: etwa das Quantentunnelungsrauschen in Halbleiterbauelementen, Phasenrauschen in Oszillatoren, oder Spontanemission in optischen Verstärkern. Die konkrete Umsetzung hängt dabei von der jeweiligen Anwendung ab – ob hohe Geschwindigkeit, Miniaturisierung oder maximale Vertrauenswürdigkeit im Vordergrund steht.
(Herrero-Collantes und Garcia-Escartin, 2017, Kap. II und III)
Allen Ansätzen gemeinsam ist, dass sie im Unterschied zu klassischen Zufallsgeneratoren nicht deterministisch sind. Es ist physikalisch unmöglich, das Ergebnis vorherzusagen – selbst mit vollständigem Wissen über den Aufbau des Systems. Genau das macht QRNGs zu einer so wertvollen Komponente in sicherheitskritischen Bereichen der Kryptographie.


\vspace{1em}
\noindent\textbf{Anforderungen an sichere QRNGs}

\noindent
Ein Quantum Random Number Generator (QRNG) muss für den Einsatz in kryptographischen Systemen spezifische Anforderungen erfüllen. Diese betreffen nicht nur die physikalische Qualität der Zufallszahlen, sondern auch ihre Testbarkeit und Vertrauenswürdigkeit. Vier zentrale Kriterien sind entscheidend.
 (Herrero-Collantes und Garcia-Escartin, 2017, Kap.II; Ma et al., 2016, S.2–3)
Erstens: Nicht-Vorhersagbarkeit. Anders als bei PRNGs lässt sich bei QRNGs das Ergebnis nicht rekonstruieren – selbst mit vollständigem Wissen über Aufbau und Zustand. Die Ursache liegt in der quantenmechanischen Superposition: Ein Ergebnis entsteht erst im Moment der Messung, z.B. wenn ein Photon zufällig reflektiert oder transmittiert wird.
 (Herrero-Collantes und Garcia-Escartin, 2017, Kap.II; Ma et al., 2016, S.2–3)
Zweitens: Zufälligkeitstestbarkeit. Um die Gleichverteilung und Unabhängigkeit der erzeugten Bitfolgen zu prüfen, werden standardisierte statistische Tests eingesetzt – etwa die NIST Statistical Test Suite (STS) oder Dieharder. Ein QRNG muss diese regelmäßig bestehen, um weiterbetrieben werden zu dürfen.
 (NIST SP 800-90B, 2018, Sec.4)
Drittens: Entropiequellenanalyse. Die Entropie – also der tatsächlich enthaltene Zufall – muss ausreichend hoch und gegen technische Störungen abgesichert sein. Systeme müssen in der Lage sein, die effektive Entropie zu schätzen und im Zweifel zu verwerfen. Diese Prüfung ist Teil vieler Zertifizierungen, etwa FIPS 140-3.
 (Ma et al., 2016; NIST SP 800-90B, Abschn.5.2)
Viertens: Device Independence. Ideal ist ein QRNG, das auch dann sichere Zufallszahlen liefert, wenn die Hardware nicht vertrauenswürdig ist. Dies wird durch quantenmechanische Tests wie Bell-Tests erreicht, die Unabhängigkeit von internen Annahmen garantieren. Solche Geräte sind technisch anspruchsvoll, aber ein zentrales Ziel künftiger Entwicklungen.
 (Ma et al., 2016, Abschnitt „Classification of QRNGs“)
 

\subsection{Aktuelle Anwendungsprojekte}
\subsubsection{Post-Quantum-Cryptography}
\cite{}
Im Rahmen der Post-Quantum-Kryptographie-Standardisierung verfolgt das NIST das Ziel, kryptographische Algorithmen zu identifizieren, die auch gegenüber zukünftigen Quantenangriffen resistent sind. Die vierte Runde des PQC-Prozesses hat zur Auswahl von Verfahren für Public-Key-Verschlüsselung sowie digitale Signaturen geführt. Im Fokus stehen insbesondere die Verfahren CRYSTALS-Kyber, CRYSTALS-Dilithium, and SPHINCS+, welche bereits breite Beachtung in industriellen Pilotprojekten und Implementierungsstudien gefunden haben.

CRYSTALS-Kyber ist ein auf dem Module-Learning-with-Errors (Module-LWE)-Problem basierendes Verfahren zur sicheren Schlüsselaushandlung. Das Design legt besonderen Wert auf eine effiziente Implementierbarkeit über verschiedene Plattformen hinweg, darunter auch ressourcenbeschränkte Systeme wie eingebettete Geräte. Ein wesentliches Merkmal ist die Resistenz gegen Seitenkanalangriffe, insbesondere durch eine auf konstante Laufzeit ausgelegte Referenzimplementierung (vgl. NIST IR 8545, S. 4; kyber blog). Kyber wurde frühzeitig in praktische Anwendungen integriert. So testete Google gemeinsam mit Cloudflare den Algorithmus im Rahmen einer hybriden TLS-1.3-Implementierung, um dessen Praxistauglichkeit im Internetverkehr zu evaluieren. Auch Amazon Web Services (AWS) integrierte Kyber im Key Management Service als experimentellen Post-Quantum-Kandidaten \cite{sullivan_securing_2020, weibel_round_2020}.

Für digitale Signaturen wurde mit CRYSTALS-Dilithium ein weiteres Verfahren aus der CRYSTALS-Familie standardisiert. Dilithium basiert auf der kombinatorischen Härte des Short Integer Solution (SIS)- und des Module-LWE-Problems und verwendet das Fiat-Shamir with Aborts-Paradigma. Ein wesentliches Designziel ist die einfache und sichere Implementierbarkeit, insbesondere durch Verzicht auf zustandsbehaftete Operationen und durch Nutzung gleichmäßig verteilter Zufallswerte anstelle diskreter Gauss-Verteilungen, die als anfällig gegenüber Seitenkanalangriffen gelten (dilithium paper S. 3–5). Die Signaturen werden entweder deterministisch oder randomisiert erzeugt; Letzteres erhöht die Sicherheit in adversen Umgebungen, in denen Seitenkanalangriffe drohen. Zur Performanceoptimierung sind Implementierungen mit AVX2- und AES-Unterstützung verfügbar, die deutliche Geschwindigkeitsvorteile gegenüber Referenzimplementierungen zeigen (dilithium paper S. 8–9). Die Sicherheitsnachweise erfolgen sowohl im klassischen Random-Oracle-Modell (ROM) als auch im Quantum-ROM (QROM) unter Annahme der Härte der Module-LWE- und SIS-Probleme \cite[S. 6-7]{schwabe_dilithium_nodate}.

Das Verfahren SPHINCS+ verfolgt im Gegensatz zu Kyber und Dilithium einen hashbasierten Ansatz und setzt ausschließlich auf die Sicherheit kryptographischer Hashfunktionen. Dies ermöglicht ein besonders konservatives Sicherheitsmodell, das selbst unter Annahme fortgeschrittener algebraischer Quantenalgorithmen tragfähig bleibt \cite[s. 1-2]{schwabe_sphincs_2025}. Ein Alleinstellungsmerkmal von SPHINCS+ ist das stateless Design, welches im Gegensatz zu klassischen, zustandsbehafteten Hash-basierten Verfahren die Gefahr von Schlüsselverlusten durch fehlerhafte Zustandsverwaltung eliminiert. Die Signaturerzeugung basiert auf der Kombination mehrerer kryptographischer Bausteine, darunter Winternitz-One-Time-Signatures, Merkle-Bäume und HORST (eine Few-Time-Signature-Variante). Die daraus resultierenden Signaturen sind mit etwa 41KB vergleichsweise groß, jedoch bieten sie hohe Sicherheit und Resistenz gegen eine breite Klasse von Angriffen, inklusive solcher unter Einbeziehung von Quantenrechnern \cite[S. 4-5]{schwabe_sphincs_2025}. Aufgrund dieser Eigenschaften empfiehlt sich SPHINCS+ insbesondere für Anwendungen mit langfristiger Vertrauenswürdigkeit, wie etwa Firmware-Signaturen oder die Verifikation kritischer Softwarekomponenten.

Die Auswahl und Standardisierung dieser Verfahren wurde von NIST sowohl unter Berücksichtigung mathematischer Sicherheitsannahmen als auch praktischer Implementierbarkeit und Performance getroffen. Insbesondere wurde ein Augenmerk auf Diversität der zugrundeliegenden Problemklassen gelegt, um im Sinne der Kryptoagilität eine robuste Sicherheitsarchitektur gegenüber zukünftigen algorithmischen Durchbrüchen zu gewährleisten \cite[S. 4, 9]{alagic_status_2025}. CRYSTALS-Kyber und CRYSTALS-Dilithium basieren auf Gitterproblemen, während SPHINCS+ auf die Sicherheit von Hashfunktionen setzt.

Die ausgewählten Algorithmen sind mittlerweile Bestandteil offizieller NIST-Standards: CRYSTALS-Kyber als ML-KEM in FIPS 203, CRYSTALS-Dilithium als ML-DSA in FIPS 204 und SPHINCS+ als SLH-DSA in FIPS 205. Erste industrielle Anwendungen und Pilotprojekte zeigen eine zunehmende Integration in sicherheitskritische Kommunikationsprotokolle und Cloud-Infrastrukturen \cite{alagic_status_2025, sullivan_securing_2020, weibel_round_2020}. Insgesamt stellt die NIST-Standardisierung eines der zentralen Anwendungsprojekte im Bereich PQK dar und liefert eine belastbare Grundlage für die künftige Migration kryptographischer Systeme in das Post-Quantum-Zeitalter.

\subsubsection{Quantum-Key-Distribution}

Die Quantum Key Distribution (QKD) stellt eine der derzeit technologisch am weitesten entwickelten Anwendungen der Quantenkommunikation dar. Ihr Ziel ist die abhörsichere Verteilung kryptographischer Schlüssel basierend auf quantenmechanischen Prinzipien. Aufgrund fundamentaler physikalischer Eigenschaften erlaubt QKD die Erkennung von Abhörversuchen und bietet damit in bestimmten Szenarien informationstheoretische Sicherheit. Dennoch bestehen Herausforderungen hinsichtlich Reichweite, Infrastrukturkosten und Interoperabilität mit bestehenden Kommunikationssystemen. Zwei bedeutende internationale Anwendungsprojekte verdeutlichen die verschiedenen strategischen Ansätze im Umgang mit diesen Herausforderungen: das chinesische Satellitenprogramm um den Micius-Satelliten sowie die europäische EuroQCI-Initiative.

Im Jahr 2016 startete China mit dem Satelliten \cite{courtland_chinas_2016} das erste operative Weltraumexperiment zur quantenmechanischen Schlüsselverteilung \cite{courtland_chinas_2016}. Der in einer Umlaufbahn von ca. 500~km operierende LEO-Satellit demonstrierte die Realisierbarkeit satellitengestützter QKD zwischen weit entfernten Bodenstationen und legte damit den Grundstein für eine globale Quantenkommunikationsarchitektur \cite{wang_modeling_2021}. Im Vergleich zu optischen Glasfaserverbindungen, deren Reichweite durch Dämpfung und Fehlerraten physikalisch limitiert ist, ermöglicht die freie Ausbreitung im Vakuum des Alls verlustärmere Verbindungen über mehrere tausend Kilometer.

Parallel zur Satellitenentwicklung wurde ein nationales QKD-Netz realisiert, das sich von Beijing bis Shanghai über ca. 2000~km erstreckt und 32 terrestrische Knoten umfasst, die als trusted nodes operieren \cite{wang_modeling_2021}. Diese Architektur wirft jedoch grundlegende Fragen hinsichtlich Vertrauen und Skalierbarkeit auf, da kompromittierte Knoten die Gesamtsicherheit untergraben können. Obwohl sie aus technologischer Sicht einfacher zu realisieren sind als quantenmechanische Repeater, bleibt ihre Eignung für hochsichere Anwendungen begrenzt. Aus diesem Grund rückt in China zunehmend die Entwicklung von Satellitenkonstellationen mit Inter-Satellite-Links (ISLs) in den Fokus, die eine redundante und dynamisch steuerbare Verteilung von Schlüsseln ermöglichen sollen \cite{wang_modeling_2021}.

Die Europäische Union verfolgt mit der EuroQCI den Aufbau einer sicheren paneuropäischen Quantenkommunikationsarchitektur. Ziel ist ein hybrides Netz, das terrestrische QKD-Übertragungen über Glasfaser mit satellitengestützten Komponenten kombiniert \cite{noauthor_european_2020}. Die Initiative umfasst dabei sowohl nationale Quanten-Backbones als auch grenzüberschreitende Knoten und wird in enger Kooperation mit der European Space Agency (ESA) umgesetzt.

Ein zentrales Teilprojekt ist der Satellit \cite{noauthor_eagle-1_2025}, der ab 2024 getestet werden soll. Das Vorhaben zielt auf die Demonstration von QKD via Satellit im Kontext europäischer Sicherheitsinfrastrukturen und wurde speziell auf regulatorische und industrielle Bedarfe innerhalb der EU abgestimmt (eagle1). Im Gegensatz zum chinesischen Micius-Satelliten handelt es sich bei Eagle-1 explizit um ein technologievalidierendes Pilotprojekt, das vor allem Schnittstellen und Interoperabilität mit terrestrischen Netzen erprobt.

Erste Systemdesignstudien wurden unter anderem vom Institut de Ciències Fotòniques (ICFO) in Spanien koordiniert und adressieren die technischen Herausforderungen von Synchronisation, Detektion und Netzwerkmanagement im Kontext von Low Earth Orbit Systemen \cite{van_deventer_towards_2022}. Ein interessantes Detail ist die geplante Modularität des EuroQCI-Systems, das eine sukzessive Erweiterung über Mitgliedsstaaten hinweg vorsieht, statt eine zentralistische Netzarchitektur zu forcieren.

Im direkten Vergleich zeigt sich, dass China auf Demonstrationen mit operationellem Charakter und nationaler Souveränität fokussiert, während die europäischen Aktivitäten stärker kooperativ, regulativ eingebettet und netzarchitektonisch offen angelegt sind. Beide Strategien erscheinen unter ihren jeweiligen geopolitischen und technologischen Rahmenbedingungen plausibel. Eine kritische Reflexion muss jedoch auch die langfristige Wartbarkeit, Standardisierung und Kostenstruktur berücksichtigen. Gerade hier wird sich zeigen, ob der europäische Ansatz mit seiner Betonung auf Interoperabilität und systemischer Integration über den Prototypenstatus hinaus skaliert werden kann.

\subsubsection{Quantum-Random-Number-Generator}

Während großskalige Quantenkommunikationssysteme wie QKD-Netzwerke in der Regel erhebliche infrastrukturelle Investitionen und spezifische physikalische Rahmenbedingungen erfordern, lassen sich Komponenten wie Quantum Random Number Generators (QRNG) bereits heute in einer Vielzahl bestehender Systeme integrieren. QRNG fungiert dabei als grundlegende Bausteintechnologie in kryptographischen Anwendungen, indem es echte, nicht-deterministische Zufallszahlen auf Basis quantenmechanischer Prozesse erzeugt. Die Kommerzialisierung dieser Technologie erfolgt primär in Form eigenständiger Hardware-Module oder integrierbarer Chips, die sich durch geprüfte Entropiequellen, regulatorische Konformität und standardisierte Schnittstellen auszeichnen. Im Folgenden werden vier aktuelle Anwendungsprojekte vorgestellt, die unterschiedliche Implementierungsstrategien, Zielmärkte und technologischen Reifegrade illustrieren.

ID Quantique entwickelt seit mehreren Jahren miniaturisierte QRNG-Chips, die sich insbesondere für eingebettete Systeme, mobile Endgeräte und IoT-Infrastrukturen eignen. Die Quantis-QRNG-Chips basieren auf der quantenmechanischen Messung von Schussrauschen eines lichtempfindlichen Elements. Konkret wird dabei ein Verstärkerschaltkreis zur Erfassung der Quantenfluktuationen eines Photodetektors verwendet, aus denen durch nachgeschaltete Entropieextraktion bitweise Zufallsdaten generiert werden \cite{noauthor_quantis_2025}. Die verwendeten Verfahren stellen sicher, dass die erzeugten Bits nicht durch klassische Prozesse vorherbestimmt oder rekonstruierbar sind. Ziel ist eine kontinuierliche, rückführbare und stromsparende Generierung echter Zufallszahlen in Anwendungen, bei denen weder hohe Datenraten noch externe optische Komponenten verfügbar sind – etwa in Secure Elements oder als Entropiequelle in Embedded Devices. Die Chips sind für industrielle Zertifizierungsprozesse vorbereitet und werden mittlerweile in verschiedenen Mobilplattformen evaluiert, beispielsweise in vernetzten Fahrzeugkomponenten und sicherheitskritischen Sensornetzwerken.

Ein zentraler Treiber für die Akzeptanz und Skalierbarkeit von QRNG ist die Standardisierung der zugrundeliegenden Technologien. Im Rahmen des ETSI Industry Specification Group QKD arbeitet ein Konsortium unter Beteiligung von ID Quantique an der Formulierung technischer Spezifikationen für QRNG-Systeme. Die aktuelle Spezifikation GS QKD 014 legt Kriterien zur Charakterisierung von QRNG-Komponenten fest, insbesondere hinsichtlich Modellierung der quantenmechanischen Entropiequelle, Entropieextraktion, Testbarkeit und Systemarchitektur \cite{curran_idq_2023}. Zusätzlich wird zwischen „strong quantum“, „quantum-origin“, und „pseudo-random“ unterschieden, je nachdem, ob die Zufallsquelle rein quantenbasiert, hybrid oder deterministisch ist \cite{van_deventer_towards_2022}. Diese Kategorisierung dient nicht nur der Interoperabilität, sondern adressiert auch die Zertifizierbarkeit und Sicherheitsklassifikation in kritischen Infrastrukturen. Die ETSI-Initiative schlägt darüber hinaus vor, QRNGs künftig als definierte Komponenten in Public Key Infrastructures (PKI) zu integrieren, etwa zur Schlüsselinitialisierung oder als Seed für deterministische Algorithmen in hybriden Kryptosystemen.

Die von Toshiba entwickelte USB-QRNG-Serie richtet sich an Anwendungen in professionellen Desktop- und Rechenzentrumsumgebungen. Die Geräte nutzen eine optische Methode, bei der einzelne Photonen auf einen Strahlteiler gelenkt und anschließend detektiert werden. Die Entscheidung, ob ein Photon auf Detektor A oder B trifft, basiert auf fundamentaler Quantenunsicherheit, wodurch Bitfolgen mit maximaler Entropie erzeugt werden \cite{toshiba_europe_cambridge_research_laboratory_quantum_2025}. Der Vorteil dieser Architektur liegt in der unmittelbaren physikalischen Rückführbarkeit der Zufälligkeit sowie in der einfachen Integration über eine standardisierte USB-Schnittstelle. Die Bitraten liegen im Bereich einiger Mbit/s und eignen sich damit sowohl zur Initialisierung kryptographischer Prozesse (z.B. RSA-Schlüsselerzeugung, TLS-Handshakes) als auch als Entropiequelle für Betriebssysteme oder Sicherheitsmodule. Toshiba hebt besonders die Resistenz gegenüber externen Einflussgrößen hervor, da die Detektion in einem abgeschirmten optischen System erfolgt. Die Geräte wurden unter anderem in der britischen Post-Quantum-Initiative evaluiert und werden derzeit auch in Kombination mit PQC-Algorithmen getestet.

Ein umfassender Überblick über aktuelle und potenzielle QRNG-Anwendungen findet sich im Bericht „QRNG Report 2021“ von evolutionQ. Dort wird deutlich, dass sich die Einsatzfelder weit über klassische Kryptographie hinaus erstrecken. QRNG wird beispielsweise in der Tokenisierung von Finanztransaktionen, bei der Erzeugung von Seeds für Blockchain-Schlüsselpaare oder in der Protokollinitialisierung sicherer Multi-Party-Computing-Systeme eingesetzt \cite{??????} <- Piani et al. Besonders hervorgehoben wird die Rolle von QRNG in virtualisierten und containerisierten Cloud-Infrastrukturen, bei denen klassische Entropiequellen wie /dev/random nicht hinreichend isoliert oder manipulationssicher sind. QRNG kann hier als dedizierter Hardware-Entropieprovider über PCIe, USB oder Netzwerkprotokolle eingebunden werden. Ein weiteres Anwendungsfeld ist die Verwendung in sicherheitskritischer Hardware wie Hardware Security Modules (HSMs), wo QRNG zur Generierung nicht reproduzierbarer Einmalschlüssel beiträgt. Laut evolutionQ liegt der entscheidende Vorteil in der messbaren Entropieherkunft, die sowohl regulatorischen Anforderungen (z.B. eIDAS, FIPS) als auch zukünftigen Anforderungen an Post-Quantum-Resilienz gerecht wird.

Die vorgestellten Anwendungsprojekte zeigen, dass QRNG nicht mehr als isolierte Labortechnologie zu betrachten ist, sondern sich zunehmend als integraler Bestandteil moderner Sicherheitsarchitekturen etabliert. Während sich einzelne Implementierungen – etwa in Form von USB-Geräten oder Chips – technisch stark unterscheiden, eint sie das Ziel, vertrauenswürdige und physikalisch nachvollziehbare Zufallsquellen für kryptographische Kernfunktionen bereitzustellen. In der Gesamtschau ergibt sich ein technologieoffenes, aber klar sicherheitsgetriebenes Innovationsfeld: QRNG schließt eine zentrale Lücke in der Kette quantensicherer Systeme, indem es nicht nur den algorithmischen Teil, sondern auch die zugrunde liegende Entropiequelle absichert. Damit kommt dieser Technologie eine Schlüsselrolle im Übergang von klassischen zu postquantenresilienten Sicherheitsinfrastrukturen zu – und zwar nicht nur auf konzeptioneller, sondern bereits auf operativer Ebene.

\subsection{Fazit}
%Zusammenfassung:
Das vorliegende Kapitel hat drei komplementäre technologische Strategien zur Absicherung digitaler Kommunikation unter quantenbezogenen Bedrohungen systematisch analysiert: Post-Quantum Cryptography, Quantum Key Distribution und Quantum Random Number Generation. Diese lassen sich in ihrer Zielsetzung, Methodik und Einbettung in bestehende Infrastrukturen klar voneinander abgrenzen, bilden jedoch gemeinsam die Grundlage einer zukunftsfähigen Kryptographiearchitektur im Quantenzeitalter. Um ihre Rolle einzuordnen, bedarf es einer strukturierten Gesamtbewertung entlang der zugrunde liegenden Technologien, ihrer Anwendungsdomänen und ihrer strategischen Reichweite.

PQC verfolgt einen softwarebasierten Ansatz zur Absicherung klassischer Kommunikationssysteme. Die Verfahren sind auf mathematisch schwer lösbare Probleme (z.B. Gitterprobleme) gestützt und benötigen keine neue physikalische Infrastruktur. Ihre Hauptstärke liegt in der breiten Anwendbarkeit (Schlüsselaustausch, Signaturen) und der hohen Kompatibilität mit bestehenden Protokollen wie TLS, VPN oder S/MIME.

QKD bietet informationstheoretische Sicherheit auf Basis physikalischer Prinzipien und hebt sich durch Abhörerkennung und theoretisch unknackbare Sicherheit ab. Es erfordert jedoch eine spezifische Quanteninfrastruktur (Photonenkanäle, Detektoren, Synchronisation) und ist aktuell nur in spezialisierten Anwendungen (z.B. Regierungsnetze, Satellitenkommunikation) realistisch einsetzbar.

QRNG stellt keine eigenständige Verschlüsselungslösung dar, sondern liefert verifizierbaren echten Zufall für kryptographische Verfahren. Es wird in klassischen Systemen (z.B. zur Initialisierung von PRNGs) ebenso genutzt wie in QKD-Systemen. Aufgrund seiner Miniaturisierbarkeit und regulatorischen Akzeptanz (z.B. FIPS, eIDAS) gilt QRNG als sofort einsatzfähige Schlüsseltechnologie.

%Bewertung des Standes:

Die drei untersuchten Ansätze – Post-Quantum Cryptography (PQC), Quantum Key Distribution (QKD) und Quantum Random Number Generation (QRNG) – unterscheiden sich hinsichtlich ihrer methodischen Grundlagen, Implementierungsvoraussetzungen und Anwendungskontexte. Tabelle~\ref{tab:kategorische_einordnung} fasst die zentralen Merkmale dieser Technologien in vergleichender Form zusammen und bietet eine systematische Einordnung auf Basis etablierter Bewertungskriterien.

\begin{table}[htbp]
\centering
\caption{Kategorische Einordnung der betrachteten Technologien}
\label{tab:kategorische_einordnung}
\begin{tabular}{@{}p{3cm}p{4.2cm}p{2.8cm}p{2.8cm}p{3.2cm}@{}}
\toprule
\textbf{Technologie} & \textbf{Anwendungsbereich} & \textbf{Reifegrad} & \textbf{Skalierbarkeit} & \textbf{Sicherheitstyp} \\
\midrule
\textbf{PQC} & Breite IT-Systeme, Cloud-Infrastrukturen, öffentliche Kommunikation & Hoch (NIST-Standards) & Hoch (Softwareintegration) & Komplexitätsbasiert \\
\textbf{QKD} & Hochsicherheitsnetze, Satellitenkommunikation, Forschung & Mittel (Pilotprojekte) & Gering (infrastrukturintensiv) & Informationstheoretisch \\
\textbf{QRNG} & Schlüsselgenerierung, Hardware Security Modules, IoT & Hoch (kommerzielle Verfügbarkeit) & Hoch (Modulintegration) & Physikalisch/Entropiebasiert \\
\bottomrule
\end{tabular}
\end{table}

Die Tabelle zeigt, dass PQC derzeit die praktikabelste Technologie für breite Anwendungen darstellt, während QKD vor allem in hochsicherheitskritischen Szenarien mit spezialisierter Infrastruktur zum Einsatz kommt. QRNG ist eine unterstützende Basistechnologie mit hoher Marktreife und spielt eine Schlüsselrolle bei der Qualitätssicherung kryptographischer Zufallsquellen.

\vspace{2em}

\subsection{SWOT-Analyse quantensicherer Kryptographie}

Zur übergreifenden Bewertung der vorgestellten Technologien bietet sich eine SWOT-Analyse an. Diese strukturiert interne Eigenschaften (Stärken und Schwächen) sowie externe Einflüsse (Chancen und Bedrohungen), die für die nachhaltige Implementierung quantensicherer Kryptographie relevant sind. Tabelle~\ref{tab:swot} stellt die wichtigsten Aspekte in verdichteter Form dar.

\begin{table}[htbp]
\centering
\caption{SWOT-Analyse der quantensicheren Kryptographie}
\label{tab:swot}
\begin{tabular}{@{}p{4.2cm}p{5.6cm}p{5.6cm}@{}}
\toprule
\textbf{} & \textbf{Intern: Stärken / Schwächen} & \textbf{Extern: Chancen / Bedrohungen} \\
\midrule
\textbf{Stärken} &
\begin{itemize}
  \item Komplementarität von PQC, QKD und QRNG
  \item Standardisierungsfortschritte (NIST, ETSI)
  \item Mehrschichtige Sicherheitsarchitekturen realisierbar
\end{itemize}
&
\begin{itemize}
  \item Förderung digitaler Souveränität (z.\,B. EuroQCI)
  \item Industrielle Skalierung (insb. QRNG und PQC)
  \item Schutz gegen langfristige Angriffsmodelle (z.\,B. „Harvest-now, decrypt-later“)
\end{itemize}
\\
\textbf{Schwächen} &
\begin{itemize}
  \item Infrastrukturaufwand und begrenzte Reichweite bei QKD
  \item Unsicherheiten bzgl. PQC-Langzeitsicherheit
  \item Implementierungsrisiken bei QRNG und PQC
\end{itemize}
&
\begin{itemize}
  \item Fragmentierte Regulierungs- und Zertifizierungslandschaft
  \item Geopolitische Spannungen in Quantenkommunikation
  \item Mangel an qualifiziertem Fachpersonal in Quanten-IT
\end{itemize}
\\
\bottomrule
\end{tabular}
\end{table}

Die SWOT-Analyse macht deutlich, dass die quantensichere Kryptographie sowohl technologische als auch strategische Differenzierungspotenziale bietet. Gleichzeitig zeigt sich, dass deren nachhaltige Wirksamkeit entscheidend von einer systematischen Einbettung in regulative, wirtschaftliche und infrastrukturelle Kontexte abhängt.

%Fazit/Abschluss:

Das Zusammenspiel von PQC, QKD und QRNG markiert einen Übergang von rein algorithmisch motivierter Sicherheit zu physikalisch untermauerten Sicherheitsgarantien. In der Systemarchitektur zukünftiger Sicherheitstechnologien ist daher nicht von einem „Entweder-oder“, sondern von einem technologisch diversifizierten, redundanten Ansatz auszugehen. PQC stellt die breite Basis dar, QRNG sichert die Zufallsgenerierung und QKD deckt hochsicherheitskritische Schlüsselverteilungen ab.

Damit ergibt sich ein paradigmatischer Wandel in der Kryptographie: von komplexitätsbasierter, zentralisierter Sicherheit hin zu hybriden, verteilten, quantenresistenten Sicherheitsarchitekturen. Die Gestaltung dieser Systeme erfordert nicht nur technische Exzellenz, sondern auch normativen Konsens, wirtschaftliche Skalierung und strategisches Risikomanagement.

\printbibliography
