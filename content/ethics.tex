%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Ethische Aspekte}
\label{ethics} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\chapterauthor{Halima Albader Alhusini, Gizem Bulut, Adrian Hußfeldt Vazquez, Tayebeh Nazari}

\abstract{Quantencomputing bringt neuartige ethische Risiken, die über bestehende KI-Rahmen hinausgehen. Nationale Alleingänge und geopolitischer Wettbewerb verstärken Fragmentierung und Dual-Use-Gefahren. Kritische Felder sind fairer Zugang, reale Sicherheit (trotz QKD-Versprechen), hoher Ressourcenverbrauch, fehlende Transparenz und militärisches Missbrauchspotenzial. Ein spezifisches „Quantum-Ethics“-Regelwerk mit offenen Standards, Explainability-Forschung und internationaler Governance ist nötig, um Chancen zu nutzen, ohne neue Ungleichheiten und Sicherheitslücken zu schaffen.}

\section{Ethik-Grundlagen}

Die schnelle Entwicklung des Quantencomputings wirft weitreichende Fragen auf, die rein über technische oder ökonomische Fragestellungen hinausgehen. Als disruptive Technologie hat Quatencomputing das Potenzial, die globale Kommunikation, Sicherheitsmechanismen, politische und gesellschaftliche Machtverhältnisse sowie viele weitere Aspekte grundlegend zu verändern. Die potenziell weitreichenden Auswirkungen erfordern eine sorgfältige Auseinandersetzung auch mit ethischen Prinzipien und Maßstäben, um den Einsatz von Quatencomputing auch mit gesellschaftlichen Werten in Einklang zu bringen. Ethik fungiert dabei als normativer Kompass: Sie zeigt nicht nur technische Potenziale auf, sondern auch, was moralisch erstrebenswert ist, und wo moralisch Risiken liegen. Gerade im Kontext des Quantencomputings, dessen langfristige Folgen aktuell noch schwer abzuschätzen sind, ist es von Bedeutung, geeignete ethische Ansätze zu identifizieren und diese bereits in der Entwicklung solcher Technologien anzuwenden. Ein Beispiel für die frühe Entwicklung ethischer Leitlinien und deren Bedeutung wird beim Thema künstliche Intelligenz deutlich. So hat die EU beispielsweise bereits im Jahr 2018 durch Fachexperten der Technikethik ethische Leitlinien für vertrauenswürdige KI entwickeln lassen. (\cite{european_commission_directorate_general_for_communications_networks_content_and_technology_ethik-leitlinien_2019})
\\
\\
In diesem Kapitel werden zunächst zentrale Ethiktheorien vorgestellt und grundlegende Begriffe definiert. Aufbauend darauf wird in Kapitel 2 untersucht, welche Akteure (Stakeholder) im Quantencomputing entscheidend sind und wie Verantwortlichkeiten und Governance-Strukturen ausgestaltet werden. Im Anschluss werden in Kapitel 3  normative Spannungsfelder untersucht, in denen sich die Chancen und Risiken des Quantencomputings ergeben. Abschließend wird ein Fazit gezogen, sowie ein weiterer Ausblick gegeben.

\subsection{Überblick über zentrale Ethiktheorien}

Es existieren unterschiedliche ethische Ansätze, die als Bewertungsrahmen für technologische Innovation herangezogen werden können. Dieser Abschnitt soll einen Überblick über ethische Grundmodelle im Bereich der (Technik-)Ethik bieten, die zu unserer Thematik Anwendung finden können. In dieser Betrachtung werden die Kernelemente der jeweiligen Ethik betrachtet und ein Bezug zur Technikethik hergestellt.

\subsubsection{Utilitarismus}
Der Utilitarismus ist Teil der konsequentialistischen Ethikansätze. Er geht zurück auf Philosophen wie Jeremy Bentham und John Stuart Mill und setzt das Prinzip der kollektiven Nutzenmaximierung aller Betroffenen ins Zentrum. Das Ziel ist demnach das größtmögliche Glück für die größtmögliche Zahl zu erzeugen. Die Folgen einer Handlung oder von Innovationen werden demnach bilanziell aufgestellt und aggregiert. Das Ziel besteht darin, das maximale subjektive Wohlbefinden zu steigern. Der Utilitarismus fordert dabei auch ein Miteinbeziehen zukünftiger Generationen bei langfristigen Entscheidungen. Im Kontext von Technologien und Innovationen ist der Utilitarismus passend, da diese auch häufig die Charakteristik der Maximierung der Nützlichkeit aufweisen. Ebenfalls zeigt sich eine Ähnlichkeit in der utilitaristischen Ansicht, dass ein Eingriff in die Vorgabe der Natur zulässig ist, wenn eine technische Optimierung stattfindet. Es wird dabei rein auf die Folgen der "Optimierung" geachtet.  Jedoch existieren ebenfalls hier der Natur nähere "grüne"  Interpretationen des Utilitarismus, welche Demut, Achtsamkeit und Kooperation zur Erhaltung und Sicherung der Lebensgrundlagen als Kernelemente identifizieren. Bei der utilitaristischen Betrachtung technischer Entwicklungen müssen neben Chancen und Risiken auch Unsicherheit und Katastrophenpotenzial berücksichtigt werden. (\cite{grunwaldHandbuchTechnikethik2021}, Seite 160 - 164)

\subsubsection{Tugendethik}
Anders als bei den anderen großen Ethiktheorien (Deontologie und Konsequentialismus/Utilitarismus), bei denen Handlungen im Mittelpunkt stehen, wendet sich die Tugendethik nicht den einzelnen Handlungen, sondern der Persönlichkeit der handelnden Person zu, also dem jeweiligen Träger der moralischen Verantwortung. Im Zentrum stehen dabei Tugenden, also bestimmte Haltungen die moralischen Charakter auszeichnen, wie beispielsweise Klugheit ("phronesis" oder Verstandestugend), Mut und Gerechtigkeit, an welchen ein Mensch sein Handeln und Denken ausrichtet. In diesem Zusammenhang hat beispielsweise die Klugheit („phronesis”) allerdings eine andere Bedeutung als der Begriff heute üblicherweise hat. Im Kontext der Tugendethik bedeutet Klugheit nicht nur die gute Planung und Umsetzung eines Vorhabens, sondern bezieht auch die guten Handlungsmotive mit ein und setzt diese voraus. Moralisches Handeln wird hierbei nicht als Pflicht angesehen, sondern als Ausdruck eines guten Charakters. Dieser entwickelt sich aus dem Studium des richtigen Handelns. Historisch hatte die Tugendethik, welche auf Aristoteles zurückgeht, ein tugendhaftes, also erfülltes Leben, zum Ziel. Im Kontext der Technologieentwicklung sind Ansätze erforderlich, um die Tugendethik in die Ausbildung und das Studium der Entwickler und anderer Entscheidungsträger zu integrieren und technisches Wissen mit verantwortungsvollem Handeln in komplexen und unsicheren Situationen zu verknüpfen. Verbinden lässt sich dies, da technische Innovation meist ebenfalls eine Verbesserung des menschlichen Lebens zum Ziel hat. (\cite{grunwaldHandbuchTechnikethik2021}, Seite 165 - 169; \cite{corcilius_phronesis_2021},Seite 351 - 352)


\subsubsection{Deontologie}
Die Deontologie gehört zu den bekanntesten und am weitesten verbreiteten ethischen Denkschulen. Einer ihrer bedeutendsten Vertreter und Begründer war Immanuel Kant. Über die Jahre hinweg haben sich in Bezug auf die Deontologie sowohl verschiedene Interpretationen als auch Definitionen ergeben, diese stehen zwar meist eng in Verbindung, sind allerdings nicht deckungsgleich. Im Vergleich zur Tugendethik steht bei der deontologischen Ethik nicht der Charakter des Handelnden, sondern die moralische Beurteilung der Handlung selbst im Vordergrund. Zentral ist die Vorstellung, dass bestimmte Handlungen aus Prinzip verboten oder geboten sind, unabhängig von den Konsequenzen. In der Ethik nach Kant bildet der \textbf{kategorische Imperativ} dabei den Kern: Jede Handlung muss so gestaltet sein, dass ihre zugrunde liegende Maxime als allgemeines Gesetz gelten kann. Damit weist Kant allen rationalen Subjekten universelle Pflichten zu, unabhängig davon, welchen Nutzen oder Schaden das konkrete Ergebnis dieser Handlung haben könnte. Im Laufe des 20. Jahrhunderts wurde die kantische Pflichtethik in zahlreichen Varianten und Ergänzungen weiterentwickelt. In der Technikethik stellt die deontologische Perspektive einen grundsätzlichen Bruch mit rein zweck-mittelorientierten Entscheidungslogiken. Gemäß der Standarddefinition deontologischer Ethiken wird die moralische Richtigkeit technischer Handlungsoptionen nicht allein an ihrer Effizienz bei der Zielerreichung gemessen, sondern auch an Kriterien, die sich jeder reinen Nutzen- bzw. Risikoabwägung entziehen. So lassen sich Gerechtigkeitsüberlegungen zur Risikoverteilung oder Rechte wie beispielsweise das Recht auf Privatsphäre nicht einfach als Variable in eine Optimierung einbauen. Sie müssen vielmehr als vorrangige Normen gelten, die technische Lösungen einschränken. (\cite{grunwaldHandbuchTechnikethik2021}, Seite 171 - 174; \cite{plegerGuteLebenEinfuehrung2017}, Seite 94 - 101; \cite{neuhaeuserHandbuchAngewandteEthik2023}, Seite 67 - 73)

\subsection{Ethische Chancen \& Risiken}

Im Zusammenhang mit Quantencomputing unterscheiden wir zwischen technologischen Potenzialen und ethischen Chancen. Technologische Potenziale beschreiben primär die funktionalen Möglichkeiten einer Technologie, beispielsweise in den verschiedenen Anwendungsfeldern der Kryptographie oder Medizintechnik. Andere Sichtweisen beziehen das technologische Potenzial auf die strategischen Möglichkeiten für Unternehmen. Ethische Chancen beziehen sich darüber hinaus auf die positiven Beiträge, also den positiven Wert, den eine Technologie beispielsweise zum menschlichen Wohlergehen, zur sozialen Gerechtigkeit oder zur ökologischen Nachhaltigkeit leisten kann. Ein ethisch wertvolles Innovationsziel ist demnach nicht allein die technische Machbarkeit und die Erzielung von Profit gegeben, sondern auch die Stärkung gesellschaftlicher Werte wie Teilhabe, Solidarität und Langzeitgerechtigkeit muss darin enthalten sein. Im Unterschied zu reinem Fortschrittsoptimismus oder wirtschaftlichen Vorteilen kommen im Zusammenhang mit ethischen Chancen explizit Fragen auf, welche gesellschaftlichen Werte durch eine Technologie gefördert werden und wie diese Förderung über die eigentliche Funktionalität hinausgeht. Aus ethischer Perspektive gilt ein Fortschritt erst dann als Chance, wenn er nicht nur Profit generiert, sondern auch den kollektiven Nutzen in Form von Sicherheit, Gesundheit oder Umweltintegrität und weiteren Aspekten messbar erhöht. In diesem Sinne entspricht unsere Auffassung und Definition von ethischen Chancen in dieser Arbeit dem Ansatz der utilitaristischen Technikethik, die welchem aggregierten gesellschaftlichen Nutzen in den Mittelpunkt stellt. (\cite{hoferPotentialbasedTechnologyPlanning2019} ;\cite{brusoniEthicsTechnologyOrganizational2017} ;\cite{bednarPowerEthicsUncovering2024})
\\
\\
Ethische Risiken erfassen potenzielle Schäden, die über rein technische Fehlfunktionen oder betriebswirtschaftliche Verluste hinausgehen und gesellschaftliche Werte wie Autonomie oder Gerechtigkeit nachteilig für die Mehrheit beeinflussen. Im Kontext der künstlichen Intelligenz definieren Douglas et al. (2024, S. 1 ff.) ethische Risiken als „jedes mit einer KI verbundene Risiko, das dazu führen kann, dass die Beteiligten eine oder mehrere ihrer ethischen Verpflichtungen gegenüber anderen Beteiligten verletzen.“ Angesichts der vergleichbaren potenziellen disruptivität und Reichweite der Technologie lässt sich diese Definition direkt auf das Quantencomputing übertragen. Demnach umfasst das ethische Risiko des Quantencomputings alle (ethisch) negativen Folgen, die durch dessen Einsatz für die verschiedenen Stakeholder – von Nutzern über Entwickler bis hin zu betroffenen Dritten – entstehen können. Hierzu zählen beispielsweise Dual-Use oder der zukünftige möglicherweise eingeschränkte Zugang zu Quantentechnologie. Diese Definition bietet ebenfalls, wie die ethischen Chancen, eine Ähnlichkeit mit der utilitaristischen Sichtweise der Technikethik und der Betrachtung des möglichen aggregierten "Leids", welches durch die Technologie oder deren Nutzung für die Stakeholder hervorgeht. (\cite{douglasEthicalRiskAI2025}) 
\\
\\
Die ethischen Chancen und Risiken werden im Rahmen dieser Arbeit in Abschnitt 3 „Normative Spannungsfelder” in Bezug auf das Quantencomputing näher betrachtet. Dabei werden verschiedene Anwendungen und Spannungsfelder hinsichtlich ihrer ethischen Risiken und Chancen analysiert.


\subsection{Verantwortung}
In der Angewandten Ethik zählt die Verantwortung als ein ethischer Grundbegriff. Einerseits dient dieser dazu, zu klären, welche Akteure überhaupt als moralische Subjekte gelten können. Andererseits beschreibt er, wie abstrakte Rechte und Pflichten konkret auf Handelnde Akteure verteilt werden. Gerade in Anwendungsfeldern wie der Umwelt- oder Medizinethik sowie im Bereich fundamentaler technologischer Innovationen stellt sich immer wieder die Frage, wer welche Pflichten trägt und wofür diesbezüglich Rechenschaft abzulegen ist. Im Bereich der Ethik lässt sich die Verantwortung meist als dreistellige Relation darstellen mit den zentralen Fragestellungen: (1) Wer ist verantwortlich? (2) Wofür ist jemand verantwortlich? (3) Wem gegenüber wird die Verantwortung getragen? Diese Struktur ermöglicht es, die Komplexität von Verantwortungszuschreibungen abzubilden, während zugleich die Übersichtlichkeit beibehalten wird. Zur ersten Frage „Wer ist verantwortlich?” – wird zwischen allgemeiner Verantwortungsfähigkeit und konkreter Verantwortungszuschreibung unterschieden. verantwortungsfähig sind nur Akteure, die über \textbf{Willensfreiheit} verfügen, \textbf{handlungsfähig} sind und einen \textbf{moralischen Standpunkt einnehmen} können. Erst auf dieser Grundlage kann in einer konkreten Situation einem Individuum Verantwortung zugeschrieben werden. Bezüglich konkreter Verantwortung wird diese zugeschrieben, wenn eine Person sich der Verantwortung freiwillig bekennt, ihr die Verantwortung automatisch zufällt (z. B. wenn der diensthabende Notarzt in der Notaufnahme automatisch die Verantwortung für die Versorgung eines Patienten übernimmt) oder diese zugewiesen wird (letzteres muss in der Verantwortungsethik genauer betrachtet werden). Die zweite zentrale Fragestellung nach dem "Wofür jemand verantwortlich ist" differenziert zwischen \textbf{haftender Verantwortung} für Folgen eigener Handlungen und \textbf{sorgender Verantwortung} für bereits eingetretene oder künftige Ereignisse. Haftende Verantwortung bezieht sich auf die eigenen Handlungen und deren konkrete Folgen. Die sorgende Verantwortung hingegen fordert präventives, aktives Eingreifen, um mögliche Schäden abzuwenden oder zu mindern, auch wenn der Schaden nicht auf das eigene Handeln zurückgeht, sondern auf andere Personen oder natürliche Vorgänge. Die dritte zentralen Fragestellung befasst sich damit, wem gegenüber die Verantwortung getragen wird. Hierbei kann eine Verantwortung einerseits gegenüber anderen Personen entstehen oder gegenüber bestimmter normativer (Werte-)Maßstäbe (Recht, moralische Verantwortung). Die Verantwortung gegenüber Personen oder spezifischen Gruppen umfasst beispielsweise Patientinnen und Patienten, deren Angehörige, betroffene Bürgerinnen und Bürger sowie spezifische Stakeholder, wie etwa Anwohner bei einem Industriebauprojekt. Dies muss in der jeweiligen Situation beurteilt werden. In diesem Sinne richtet sich die Rechenschaftspflicht direkt an diejenigen, die unter einer Handlung leiden oder profitieren. Es besteht die normative Erwartung, dass der Verantwortungsträger die Interessen und Rechte dieser konkreten Betroffenen angemessen berücksichtigt und zum Ausdruck bringt - etwa durch Partizipation an einem Entscheidungsprozess oder ähnliches. Die zweite Dimension bezieht sich auf die Verantwortung gegenüber normativen Maßstäben. Dabei steht nicht eine konkrete Person im Zentrum, sondern die Einhaltung allgemein akzeptierter Regeln und Prinzipien, wie beispielsweise rechtlicher Vorgaben, ethischer Leitlinien oder wissenschaftlicher Standards. Der Verantwortungsträger muss demnach nicht nur seinen Mitmenschen Rede und Antwort stehen, sondern auch sicherstellen, dass sein Handeln mit übergeordneten Normen im Einklang steht. (\cite{neuhaeuserHandbuchAngewandteEthik2023}, Seite: 215 - 221)


\subsection{Ethik-Leitlinien}

Angesichts der potenziell disruptiven Wirkung und der schwer abzuschätzenden Tragweite des Quantencomputings ist es notwendig, ethisch fundierte Leitlinien zu definieren. Diese sollen verantwortliches Handeln in Forschung, Entwicklung und Anwendung der Technologie voraussetzen. Die im Folgenden vorgeschlagenen Leitlinien stützen sich dabei hauptsächlich auf zwei ethische Denkschulen, den Utilitarismus und die Verantwortungsethik, die in den Kapiteln 1.1 und 1.1.3 vorgestellt wurden. Der Utilitarismus bietet hierbei einen Rahmen für die Abwägung von potenziellem Nutzen und Schaden, während die Verantwortungsethik die Perspektive der Verantwortung der Stakeholder – also Entwickler, Unternehmen, Staaten usw. – miteinbezieht. Die Leitlinien sollen dabei als Orientierung für die verantwortungsvolle und ethische Entwicklung und Anwendung des Quantencomputings dienen. Die Prinzipien sind als Ausgangspunkt für Diskussionen und weitere Entwicklung zu verstehen. Sie sollen das Bewusstsein für ethische Herausforderungen schärfen und die Debatte innerhalb von Wissenschaft, Politik und Gesellschaft anregen, ohne einen abschließenden Charakter zu haben. (\cite{kop_quantum-elspi_2023}; \cite{kop_establishing_nodate};  \cite{european_commission_directorate_general_for_communications_networks_content_and_technology_ethik-leitlinien_2019}):
\\
\\
\textbf{1. Maximiere den gesellschaftlichen Nutzen und minimiere den Schaden.}
Quantencomputing soll in erster Linie zur Förderung des gemeinschaftlichen Wohlergehens eingesetzt werden. Dies Bedeutet, dass Anwendungen, die globale Herausforderungen adressieren, priorisiert werden sollen, während potentielle negative Risiken proaktiv minimiert werden müssen. 
\\
\\
\textbf{2. Verantwortungsvolle Zukunftsgestaltung }
Akteure im Bereich des Quantencomputings tragen eine besondere Verantwortung: Sie müssen sowohl die haftende Verantwortung für die Folgen des eigenen Handelns als auch die sorgende Verantwortung für die gegenwärtigen und langfristigen Konsequenzen ihres Handelns und das anderer übernehmen, auch wenn potenzielle Schäden nicht auf das eigene Handeln zurückgehen.
\\
\\
\paragraph{\textbf{3. Wahrung der Menschenwürde und Autonomie}}
Die Achtung der Menschenwürde, die Sicherung individueller Autonomie sowie der Schutz fundamentaler Rechte aller beteiligten Stakeholder müssen integrale Bestandteile der Entwicklung und Anwendung quantentechnologischer Systeme sein.
\\
\\
\textbf{4. Fairer Zugang und Förderung der Inklusion}
Um den potenziellen Nutzen der Technologie zu maximieren dürfen die Vorteile des Quantencomputings nicht nur privilegierten Akteuren und Unternehmen vorbehalten sein. Eine Demokratisierung des Zugangs muss aktiv gefördert werden um die öffentliche Teilhabe sicherzustellen.
\\
\\
\textbf{5. Transparenz und Rechenschaftspflicht}
Transparenz im Bereich der Forschung und Entwicklung im Quantenbereich müssen gefördert werden. Dies umfasst die Offenlegung relevanter Vorgänge sowie klare Verantwortlichkeitsstrukturen, die eine angemessene Rechenschaftspflicht gegenüber der Gesellschaft und den Rechtssystemen gewährleisten.
\\
\\
\textbf{6. Datenschutzkonformität}
Die Quantentechnologie wird im Einklang mit geltenden Datenschutzrechten und den Datenschutz-Grundprinzipien in allen Technologielebenzyklusphasen entwickelt und eingesetzt. Damit sollen der Schutz sensibler Informationen, die Privatsphäre aller Stakeholder und die Resilienz gegen Datenmissbrauch gewährleistet werden.
\\
\\
\textbf{7. Förderung öffentlicher Aufklärung und Teilhabe}
Der gesellschaftliche Diskurs über die Chancen und Risiken des Quantencomputing ist unerlässlich. Bildungsprogramme und öffentliche Debatten sollen sicherstellen, dass die Bürger in die Gestaltung der technologischen Zukunft eingebunden werden.
\\
\\
\textbf{8. Kontinuierliche ethische Reflexion}
In Anbetracht der rasanten Entwicklung des Quantencomputings und der breiten Einsatzmöglichkeiten ist eine regelmäßige Überprüfung und Anpassung ethischer Leitlinien an neue Herausforderungen erforderlich.
\\
\\
Die vorliegenden Leitlinien stellen einen Ausgangspunkt für eine ethische Reflexion über das Quantencomputing dar. In Anbetracht der rasanten technologischen Fortschritte und der vielfältigen Anwendungsfelder ist es jedoch erforderlich, sie als dynamisch zu betrachten. Neue gesellschaftliche Herausforderungen und ethische Fragestellungen können zukünftig zu einer kontinuierlichen Überarbeitung und Präzisierung dieser Prinzipien führen. Darüber hinaus ist ihre Operationalisierung der Leitlinien von entscheidender Bedeutung: Um die Leitlinien in praktisch Anwenden zu können, sind konkrete Handlungsempfehlungen und Governance-Strukturen erforderlich, die in den diversen  Anwendungsbereichen des Quantencomputings eingesetzt werden können. Das Thema der Verantwortung und Governance wird im Rahmen unserer Arbeit im folgenden Abschnitt thematisiert. Kapitel 3 widmet sich den normativen Spannungsfelder mehrerer Anwendungsbereiche.


\section{Stakeholder und Verantwortung}
Die rasante Entwicklung von Quatencomputing verspricht transformative Fortschritte in zahlreichen Bereichen, wie Chemie, Finanzwesen, oder Logistik.\textbf{ Quelle: https://www.mckinsey.de/news/presse/quantum-technology-monitor-2024}
Doch dieses Innovationspotenzial ist nicht neutral. Die Art und Weise, wie diese Veränderungen die menschliche Lebenswelt prägen werden, hängt entscheidend von den Absichten und Eigenschaften der Instanzen ab, die den Einsatz von Quantencomputern steuern. 
\textbf{Quelle: https://publica-rest.fraunhofer.de/server/api/core/bitstreams/1c38e1c5-7250-44e0-bb6b-4bdae7052c74/content} 
Dieses Kapitel widmet sich daher der detaillierten Analyse der relevanten Stakeholder und der Verteilung von Verantwortung im Kontext des Quantencomputings, um die ethischen und gesellschaftlichen Herausforderungen dieser transformativen Technologie zu beleuchten und Ansätze für eine verantwortungsvolle Gestaltung zu entwickeln.


\subsection{Identifikation relevanter Stakeholder im Quantencomputing-Ökosystem}
Die Identifizierung und Analyse der verschiedenen Gruppen und Entitäten, die den Fortschritt des Quantencomputings maßgeblich beeinflussen oder von seinen Auswirkungen betroffen sind, ist von zentraler Bedeutung. Dies ermöglicht ein umfassendes Verständnis der Machtdynamiken, Interessenkonflikte und potenziellen gesellschaftlichen Konsequenzen.
\textbf{Quelle: \href{https://doi.org/10.48550/arXiv.2403.02921}{10.48550/arXiv.2403.02921}
Quelle: https://publica-rest.fraunhofer.de/server/api/core/bitstreams/1c38e1c5-7250-44e0-bb6b-4bdae7052c74/content }

\subsubsection{Individuen: Nutzer, Betroffene und die Frage der Privatsphäre und Autonomie}
Individuen bilden eine grundlegende Stakeholder-Gruppe, die von den Auswirkungen der Quantentechnologie betroffen ist. Quantencomputing kann das Leben und die Rechte von Einzelpersonen berühren. \textbf{\href{https://www.covingtondigitalhealth.com/2025/06/quantum-computing-and-its-impact-on-the-life-science-industry/}{https://www.covingtondigitalhealth.com/2025/06/quantum-computing-and-its-impact-on-the-life-science-industry/}} Individuen können von der Ressourcenallokation und der daraus resultierenden Ungleichheit betroffen sein, von Fragen der Rechenschaftspflicht und Transparenz bei komplexen Algorithmen sowie von der Transformation von Arbeitsplätzen.\href{https://www.quera.com/blog-posts/quantum-ethics}{
https://www.quera.com/blog-posts/quantum-ethics}} Die Fähigkeit von Quantencomputern, bestehende Verschlüsselungsmethoden zu umgehen, kann Auswirkungen auf persönliche und sensible Daten, Finanztransaktionen und die allgemeine Privatsphäre haben. \textbf{https://www.covingtondigitalhealth.com/2025/06/quantum-computing-and-its-impact-on-the-life-science-industry/} Der sogenannte "Store Now Decrypt Later"-Ansatz, bei dem verschlüsselte Informationen heute abgefangen werden, um sie in einigen Jahren mit leistungsfähigeren Quantencomputern zu entschlüsseln, ist besonders relevant für Daten mit langer Haltbarkeit, wie medizinische Aufzeichnungen.\textbf{\href{https://www.scientific-computing.com/article/ethics-quantum-computing}{https://www.scientific-computing.com/article/ethics-quantum-computing}}

Die Effizienz von Quantencomputern könnte viele derzeit von Menschen ausgeführte Aufgaben automatisieren. Ein weiteres Thema ist der ungleiche Zugang zu Quatencomputing-Ressourcen und Fachwissen, der bestehende Ungleichheiten sowohl innerhalb als auch zwischen Ländern schaffen könnte.\textbf{\href{https://www.quera.com/blog-posts/quantum-ethics}{https://www.quera.com/blog-posts/quantum-ethics}} Die derzeitige Nachfrage nach Quantenexperten und quanteninformierten Managern und politischen Entscheidungsträgern wird durch traditionelle Bildungspfade nicht gedeckt.\textbf{\href{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}}

\subsubsection{Unternehmen: Rolle in Entwicklung, Kommerzialisierung und ethischer Verantwortung}
Unternehmen sind die primären Motoren für die Kommerzialisierung und Anwendung des Quantencomputings. Sie positionieren sich strategisch, um durch Quantencomputing einen Wettbewerbsvorteil zu erzielen, Wachstum voranzutreiben und neue Einnahmequellen zu erschließen.\textbf{ \href{https://www.weforum.org/stories/2025/01/quantum-technology-business/}{https://www.weforum.org/stories/2025/01/quantum-technology-business/} }Führende Technologieunternehmen wie Google, Microsoft, IBM, Intel, IonQ, Rigetti und D-Wave Quantum investieren in Forschung und Entwicklung und konkurrieren um die Führungsrolle in diesem Sektor.\textbf{\href{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}}

Viele Unternehmen gehen kollaborative Partnerschaften mit der Wissenschaft und anderen Industrien ein, um Durchbrüche zu erzielen.\textbf{ Quelle: \href{https://www.weforum.org/stories/2025/01/quantum-technology-business/}{https://www.weforum.org/stories/2025/01/quantum-technology-business/} }Beispiele hierfür sind Kooperationen in der Pharmabranche (Roche/CQuatencomputing, Boehringer Ingelheim/Google, Moderna/IBM) und im Verkehrswesen (Volkswagen/Carris zur Verkehrsoptimierung). \textbf{\href{https://blog.linknovate.com/the-impact-of-quantum-computing-in-5-key-sectors/}{https://blog.linknovate.com/the-impact-of-quantum-computing-in-5-key-sectors/}} Frühe Anwender streben danach, Abläufe zu optimieren, das Umsatzwachstum zu verbessern und neue Einnahmequellen zu schaffen. \textbf{\href{https://www.weforum.org/stories/2025/01/quantum-technology-business/}{https://www.weforum.org/stories/2025/01/quantum-technology-business/}} Dies umfasst Anwendungen in Logistik, Finanzen (Portfoliooptimierung), Fertigung, Arzneimittelforschung und Materialwissenschaften. \textbf{https://wearecommunity.io/communities/36148Gdy5W/articles/6311}


\subsubsection{Forschung und Wissenschaft: Treiber der Innovation und ethische Reflexion}
Die Forschung und Wissenschaft, bestehend aus Universitäten, nationalen Laboratorien und einzelnen Forschern, bilden das intellektuelle Fundament des Quantencomputings. Sie treiben grundlegende Durchbrüche voran, entwickeln Algorithmen und spielen eine entscheidende Rolle bei der Erweiterung der Quantenarbeitskräfte und der Weiterentwicklung interdisziplinärer Quantenwissenschaftsprogramme.\textbf{\href{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}}

Die Forschung konzentriert sich auf die Überwindung technischer Hürden, die Entwicklung fehlertoleranter Quantencomputer, die Quantenfehlerkorrektur, die Quantensensorik und die Schaffung neuer Algorithmen. Quantencomputing kann Bereiche wie die Arzneimittelforschung, Materialwissenschaften revolutionieren und KI-Technologien verbessern. \textbf{\href{https://www.covingtondigitalhealth.com/2025/06/quantum-computing-and-its-impact-on-the-life-science-industry/}{https://www.covingtondigitalhealth.com/2025/06/quantum-computing-and-its-impact-on-the-life-science-industry/}} Forscher wenden Quantencomputinhg bereits auf biologische Daten an, um Vorteile im maschinellen Lernen zu erzielen. Angesichts des theoretischen Charakters der Quanteninformationswissenschaft sind umfangreiche Forschungs- und Testarbeiten erforderlich, bevor eine großtechnische Anwendung möglich ist. \textbf{\href{https://dornsife.usc.edu/quantum-information-science/}{https://dornsife.usc.edu/quantum-information-science/}} Regional betrachtet führt China in der Quantenkommunikationsforschung, während die USA in der Qualität der Quantencomputing-Forschung (meistzitierte Arbeiten) und der Quantensensorik führend sind.\textsuperscript{20}

\subsubsection{Staaten und Internationale Organisationen: Geopolitik, Regulierung und globale Zusammenarbeit}
Staaten und internationale Organisationen sind zentrale Akteure, die die Entwicklung des Quantencomputings durch strategische Planung, umfangreiche Finanzierung und die Gestaltung internationaler Beziehungen maßgeblich beeinflussen. Regierungen betrachten Quatencomputing als strategisches Gebot für die nationale Sicherheit und wirtschaftliche Wettbewerbsfähigkeit, was zu erheblichen öffentlichen Investitionen führt.\textbf{\href{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}}

China ist führend bei öffentlichen Investitionen (15 Milliarden US-Dollar) und Quanten-bezogenen Patenten und strebt einen "Whole-of-Society"-Ansatz an. Die EU liegt mit insgesamt 10 Milliarden US-Dollar an zweiter Stelle, hauptsächlich getrieben von Deutschlands Beiträgen, während die USA bei den öffentlichen Mitteln mit 5 Milliarden US-Dollar an dritter Stelle liegen, jedoch bei den privaten Investitionen führend sind.\textbf{\href{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}} Der US National Quantum Initiative Act (NQI Act) etabliert ein Bundesprogramm zur Beschleunigung von Forschung und Entwicklung, zur Koordinierung behördenübergreifender Anstrengungen, zur Unterstützung der Grundlagenforschung und zur Förderung des Beitrags des Privatsektors. \textbf{\href{https://science.house.gov/index.cfm?a=Files.serve&file_id=4CFFA00B-337B-441B-AA36-10BEE0BE2DA5}{https://science.house.gov/index.cfm?a=Files.serve\&file\_id=4CFFA00B-337B-441B-AA36-10BEE0BE2DA5}} Die Quantenstrategie der EU zielt darauf ab, Europa bis 2030 zu einem globalen Marktführer im Quantenbereich zu machen, ein widerstandsfähiges Ökosystem zu fördern, wissenschaftliche Durchbrüche in marktreife Anwendungen umzuwandeln. \textbf{\href{https://ec.europa.eu/commission/presscorner/detail/en/ip_25_1682}{https://ec.europa.eu/commission/presscorner/detail/en/ip\_25\_1682}}

Länder bauen eigene Kapazitäten auf und reduzieren die Abhängigkeit von anderen. Internationale Organisationen wie die OECD und die UN erforschen Prinzipien für die Entwicklung, fördern Multi-Stakeholder-Diskussionen und treiben die internationale Zusammenarbeit voran, um Fachkräftemangel und globale Ungleichheiten anzugehen. \textbf{\href{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}} Die UNESCO hat 2025 zum "Internationalen Jahr der Quantenwissenschaft und -technologie" erklärt, um Disparitäten zwischen dem Globalen Norden und Süden zu überbrücken.\textbf{\href{https://indico.un.org/event/1015312/}{https://indico.un.org/event/1015312/}} Geopolitische Konkurrenz (z.B. USA vs. China) kann zu Forschungsgeheimhaltung und protektionistischen Politiken führen, was die globale Entwicklung und Zusammenarbeit beeinflusst.\textbf{\href{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}}


\subsubsection{Zivilgesellschaft und NGOs: Anwaltschaft, Sensibilisierung und Partizipation}
Die Zivilgesellschaft und Nichtregierungsorganisationen (NGOs) spielen eine zunehmend wichtige Rolle in der Debatte um die gesellschaftlichen Auswirkungen aufkommender Technologien, einschließlich des Quantencomputings. Sie fungieren als kritische Fürsprecher für das öffentliche Interesse und den gerechten Zugang zur Quatencomputing-Entwicklung. NGOs sind zunehmend in politische Diskussionen über die gesellschaftlichen Auswirkungen aufkommender Technologien involviert und spielen eine entscheidende Rolle bei der Beeinflussung der aktuellen Politik und der langfristigen Ausrichtung.\textbf{\href{https://cstms.berkeley.edu/current-events/emerging-technologies-and-the-role-of-ngos-exploring-anticipatory-governance-in-practice/}{https://cstms.berkeley.edu/current-events/emerging-technologies-and-the-role-of-ngos-exploring-anticipatory-governance-in-practice/}}

Sie sind unerlässlich, um Stakeholder und die Zivilgesellschaft in die Steuerung der Entwicklung und der Auswirkungen der Quantentechnologie einzubeziehen.\textbf{\href{https://www.researchgate.net/publication/363102763_Own_the_Unknown_An_Anticipatory_Approach_to_Prepare_Society_for_the_Quantum_Age}{https://www.researchgate.net/publication/363102763\_Own\_the\_Unknown\_An\_Anticipatory\_Approach\_to\_Prepare\_Society\_for\_the\_Quantum\_Age}} NGOs, insbesondere diejenigen, die mit marginalisierten und schutzbedürftigen Gemeinschaften arbeiten, sind stark auf Verschlüsselung angewiesen und werden besonders häufig Opfer von Cyberangriffen; daher ist der Übergang zu quantensicherer Kryptographie für ihre digitale Sicherheit unerlässlich. Um NGOs beim Übergang in eine post-quanten Welt zu unterstützen, bedarf es eines kollektiven Vorstoßes für Bildung, Sensibilisierung und Wissensaustauschsysteme mit Experten. \textbf{\href{https://cyberpeaceinstitute.org/news/quantum-computing/}{https://cyberpeaceinstitute.org/news/quantum-computing/}} Das Engagement der Zivilgesellschaft ist eine Schlüsseldimension der antizipatorischen Governance, die aus den Erfahrungen mit Biotechnologie, Nanotechnologie und KI lernt. \textbf{\href{https://cstms.berkeley.edu/current-events/emerging-technologies-and-the-role-of-ngos-exploring-anticipatory-governance-in-practice/}{https://cstms.berkeley.edu/current-events/emerging-technologies-and-the-role-of-ngos-exploring-anticipatory-governance-in-practice/}} Sie setzen sich für verantwortungsvolle Innovation und die Gewährleistung ein, dass Quantentechnologien in einer werteorientierten Weise mit der Gesellschaft und dem Markt interagieren \textbf{\href{https://www.researchgate.net/publication/363102763_Own_the_Unknown_An_Anticipatory_Approach_to_Prepare_Society_for_the_Quantum_Age}{https://www.researchgate.net/publication/363102763\_Own\_the\_Unknown\_An\_Anticipatory\_Approach\_to\_Prepare\_Society\_for\_the\_Quantum\_Age}}


\subsection{Verantwortungsdimensionen}
Die Verteilung von Verantwortung im Quantencomputing ist komplex und vielschichtig, da sie politische, wirtschaftliche und ethische Aspekte umfasst. Die Fragen, wer Entscheidungen trifft, wer für die Folgen haftet und wie mit Machtasymmetrien umgegangen wird, sind von zentraler Bedeutung für eine ethische Entwicklung und Nutzung von Quantencomputing.

\subsubsection{Entscheidungsfindung und Haftung: Wer trifft Entscheidungen und wer haftet?}
Die Frage, wer welche Entscheidungen für wen trifft und wer für die Folgen von Quantencomputing haftet, ist von grundlegender Bedeutung für die Governance. Die Entwicklung, Bereitstellung und Nutzung von Quantencomputing-Systemen erfordert Entscheidungen auf verschiedenen Ebenen: von Forschern, die Algorithmen entwerfen, über Unternehmen, die Hardware und Software entwickeln, bis hin zu Regierungen, die den Einsatz regulieren. Die Komplexität und der "Black-Box"-Charakter von Quantenalgorithmen erschweren die Zuweisung von Rechenschaftspflicht erheblich.  \textbf{Quelle: \href{https://www.quera.com/blog-posts/quantum-ethics}{\

https://www.quera.com/blog-posts/quantum-ethics}}
Wenn ein Quantenalgorithmus einen Fehler macht, kann es schwierig sein, die Ursache oder den Mechanismus zu verstehen, der dazu geführt hat. \textbf{Quelle: 
\href{https://www.scientific-computing.com/article/ethics-quantum-computing}{https://www.scientific-computing.com/article/ethics-quantum-computing}} 
Dies kann zu sogenannten "Verantwortungslücken" führen, bei denen unklar bleibt, wer zur Rechenschaft gezogen werden kann.
Die derzeitige Entwicklungsphase des Quantencomputings bedeutet, dass die Haftungsrahmenwerke weitgehend unterentwickelt sind. Die fehlende Klarheit darüber, wer die Verantwortung trägt, könnte die Entwicklung und den Einsatz von Quantencomputing-Systemen verlangsamen. Es ist daher unerlässlich, proaktive rechtliche Rahmenwerke zu schaffen, die die Verantwortung für Design, Bereitstellung und Ergebnisse definieren, insbesondere wenn Quantencomputing von der Forschung in kommerzielle Anwendungen übergeht. Die Betonung der menschlichen Aufsicht über KI-Systeme in Chinas Normen unterstreicht die Notwendigkeit, dass Menschen die letztendlich verantwortlichen Akteure bleiben. Dies erfordert auch eine klare Definition der Verantwortlichkeiten der beteiligten Parteien und die Etablierung von Rechenschaftsmechanismen, die eine Untersuchung der Verantwortung nicht vermeiden. 
\textbf{Quelle: \href{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}}

\subsubsection{Machtasymmetrien & Repräsentationsfragen: Ungleichgewichte im Einfluss adressieren}
Die Entwicklung des Quantencomputings ist durch erhebliche Machtasymmetrien gekennzeichnet. Quatencomputing erfordert erhebliche Ressourcen, sowohl physischer als auch menschlicher Art, die nur wenigen Nationen zur Verfügung stehen. Dies könnte die globalen sozioökonomischen Unterschiede weiter vertiefen. 
\textbf{Quelle: \href{https://www.quera.com/blog-posts/quantum-ethics}{https://www.quera.com/blog-posts/quantum-ethics}}

Die hohen Kosten und das spezialisierte Fachwissen, die für Quatencomputing erforderlich sind, schaffen von Natur aus Machtungleichgewichte. Die aktuelle Investitionslandschaft, die durch die Dominanz privater US-amerikanischer Finanzierungen, Chinas staatlich gesteuerte Investitionen und Europas starke öffentliche, aber geringere private Investitionen gekennzeichnet ist, verschärft diese Asymmetrien. \textbf{Quelle: \href{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}} Zum Beispiel zieht die EU nur 5 \% der globalen privaten Finanzierung an, verglichen mit 50 \% in den USA. \textbf{Quelle: \href{https://www.eunews.it/en/2025/07/02/the-run-up-to-quantum-technologies-has-begun-eu-china-and-the-us-are-already-investing-in-military-uses/}{https://www.eunews.it/en/2025/07/02/the-run-up-to-quantum-technologies-has-begun-eu-china-and-the-us-are-already-investing-in-military-uses/}} Dies macht es unerlässlich, bewusste Anstrengungen zur Demokratisierung des Zugangs zu unternehmen und eine inklusive Beteiligung von unterrepräsentierten Regionen und Gemeinschaften (z.B. dem Globalen Süden) sicherzustellen.\cite{arrow_holistic_2023} Initiativen wie das "Internationale Jahr der Quantenwissenschaft und -technologie" der UNESCO im Jahr 2025 zielen darauf ab, diese Disparitäten zu verringern und die Forschungskapazitäten im Globalen Süden zu stärken. \textbf{Quelle: \href{https://indico.un.org/event/1015312/}{https://indico.un.org/event/1015312/}


\subsubsection{Wie ist Verantwortung verteilt – politisch, wirtschaftlich, ethisch? Wer kann, darf oder sollte ethische Rahmen setzen?}
Die Verteilung der Verantwortung für die ethische Gestaltung des Quantencomputing ist eine gemeinsame Aufgabe, die über verschiedene Sektoren hinweg getragen werden muss.

Auf \textbf{politischer Ebene} tragen Staaten und internationale Organisationen die Verantwortung für die Schaffung rechtlicher Rahmenbedingungen, die Festlegung von Standards und die Förderung internationaler Zusammenarbeit, um die Vorteile zu maximieren und Risiken zu mindern. \textbf{Quelle: \href{https://www.businessofgovernment.org/blog/quantum-technology-challenge-what-role-government}{https://www.businessofgovernment.org/blog/quantum-technology-challenge-what-role-government}} Dies umfasst auch die Regulierung von Dual-Use-Anwendungen und die Gewährleistung der nationalen Sicherheit. Sie müssen sicherstellen, dass die Entwicklung von Quantencomputing mit nationalen Sicherheitsinteressen und globalen Stabilitätszielen vereinbar ist. \href{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}{\textbf{Quelle: https://www.oecd.org/en/topics/sub-issues/quantum-technologies.htm}l} Das EU-KI-Gesetz, obwohl nicht direkt auf Quantencomputing zugeschnitten, bietet eine Analogie für Regulierung, die auf Quantencomputing übertragen werden könnte, um in Bereichen wie kritische Infrastrukturen oder Strafverfolgung hohe Risiken zu mindern.\textbf{ Quelle: \href{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}}

Auf \textbf{wirtschaftlicher Ebene} liegt die Verantwortung bei  Unternehmen und Investoren, die die Entwicklung und Kommerzialisierung von Quantencomputing vorantreiben. \textbf{Quelle: \href{https://www.weforum.org/stories/2025/01/quantum-technology-business/}{https://www.weforum.org/stories/2025/01/quantum-technology-business/}} Sie sind verantwortlich für die ethische Entwicklung ihrer Produkte, die Gewährleistung eines fairen Zugangs zu ihren Technologien und die Minderung negativer wirtschaftlicher Auswirkungen wie Arbeitsplatzverdrängung.  Ihre Verantwortung erstreckt sich auch auf die Investition in quantensichere Lösungen und die Anpassung ihrer Geschäftsmodelle.
\textbf{Quelle: \href{https://fastercapital.com/topics/the-ethics-of-quantum-computing-development.html}{https://fastercapital.com/topics/the-ethics-of-quantum-computing-development.html}
Quelle: \href{https://www.quera.com/blog-posts/quantum-ethics}{https://www.quera.com/blog-posts/quantum-ethics}}

Auf \textbf{ethischer Ebene} teilen sich alle Stakeholder die Verantwortung. Forscher tragen eine besondere Pflicht, die gesellschaftlichen Auswirkungen ihrer Arbeit zu bedenken und Ethik in den Designprozess zu integrieren. \textbf{Quelle: \href{https://pollution.sustainability-directory.com/term/quantum-technology-ethics/}{https://pollution.sustainability-directory.com/term/quantum-technology-ethics/}} Ethik-Boards und Multi-Stakeholder-Panels sind entscheidend für die Definition und Implementierung ethischer Richtlinien. Internationale Gremien wie das WEF, die OECD und die UNESCO arbeiten aktiv an der Entwicklung ethischer Rahmenbedingungen und Richtlinien, um globale Standards zu etablieren. Die Betonung liegt auf einer antizipatorischen Ethik, die darauf abzielt, ethische Überlegungen von den frühen Phasen der Forschung und Entwicklung an einzubetten. \textbf{Quelle: https://www.scientific-computing.com/article/quantum-computing-ethics https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html \href{https://www.unesco.org/en/articles/participate-unesco-survey-quantum-science-and-technology}{https://www.unesco.org/en/articles/participate-unesco-survey-quantum-science-and-technology}}

\subsubsection{Rahmenbedingungen: Akteure und Mechanismen zur Festlegung von Norme}
Die Festlegung von Rahmenbedingungen für das Quantencomputing ist eine gemeinsame Anstrengung verschiedener Akteure und Organisationen weltweit. Diese Bemühungen sind entscheidend, um die Entwicklung und den Einsatz von Quantencomputing in einer Weise zu steuern, die gesellschaftliche Werte widerspiegelt.

Globale Organisationen wie das World Economic Forum (WEF) und die National Academies of Sciences haben begonnen, Rahmenwerke für das Quantencomputing zu entwickeln. Quelle: Das WEF hat beispielsweise "Quantum Computing Governance Principles" veröffentlicht, die Inklusivität, Gerechtigkeit, Sicherheit, Umweltschutz, Transparenz und Rechenschaftspflicht betonen. Diese Prinzipien sollen die Gestaltung und Einführung von Quantentechnologien leiten. \textbf{Quelle: https://www.scientific-computing.com/article/quantum-computing-ethics} \textbf{Quelle: https://www.csiro.au/en/news/All/Articles/2022/January/quantum-computing-guidelines} Die OECD erforscht ebenfalls Prinzipien für die Entwicklung und Nutzung von Quantentechnologien und betont die Notwendigkeit eines multilateralen Konsenses. \textbf{Quelle: }\textbf{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html} Die UNESCO hat 2025 zum "Internationalen Jahr der Quantenwissenschaft und -technologie" erklärt, um globale Zusammenarbeit zu fördern. \textbf{Quelle: \href{https://www.unesco.org/en/articles/participate-unesco-survey-quantum-science-and-technology}{https://www.unesco.org/en/articles/participate-unesco-survey-quantum-science-and-technology}
}
Auf nationaler Ebene entwickeln Regierungen und Standardisierungsorganisationen eigene Ansätze. Die USA haben mit dem NIST AI Risk Management Framework (AI RMF) einen Leitfaden zur Verbesserung der Robustheit und Zuverlässigkeit von KI-Systemen geschaffen, der auf Rechenschaftspflicht und Transparenz abzielt. China hat "Ethical Norms for New Generation Artificial Intelligence" veröffentlicht, die grundlegende Anforderungen wie die Förderung des menschlichen Wohlergehens, Gerechtigkeit und die Sicherstellung der Kontrollierbarkeit betonen, wobei der Mensch die letztendliche Verantwortung trägt.\textbf{ \href{https://www.paloaltonetworks.com/cyberpedia/nist-ai-risk-management-framework}{Quelle:  https://www.paloaltonetworks.com/cyberpedia/nist-ai-risk-management-framework} \href{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}
}
Akademische Initiativen, wie das Quantum Ethics Project (QEP), tragen ebenfalls zur Gestaltung von Normen bei, indem sie Bildungsressourcen entwickeln und interdisziplinäre Diskussionen fördern. Sie betonen die Dringlichkeit, Gespräche über Quantenethik frühzeitig zu führen. \cite{arrow_holistic_2023}


\subsection{Governance \& Steuerung ethischer Herausforderungen im Quantencomputing}
Die effektive Governance und Steuerung des Quantencomputings erfordert die Implementierung robuster Mechanismen, die Transparenz, Rechenschaftspflicht und Inklusivität gewährleisten.


\subsubsection{Multi-Stakeholder-Panels: Förderung inklusiver Aufsicht}
Multi-Stakeholder-Panels sind entscheidende Instrumente zur Förderung einer inklusiven Aufsicht und zur Gestaltung der Entwicklung von Quantentechnologien. Diese Gremien bringen diverse Stakeholder zusammen – Regierungen, Industrie, Wissenschaft und Zivilgesellschaft –, um Prinzipien zu formulieren und einen breiteren Rahmen für den Einsatz von Quantencomputing zu schaffen.

Die Beteiligung verschiedener Stakeholder wird als entscheidend für eine legitime und effektive Governance anerkannt, die über rein technische Überlegungen hinausgeht. \textbf{\href{https://fashion.sustainability-directory.com/term/quantum-technology-governance/}{https://fashion.sustainability-directory.com/term/quantum-technology-governance/}} Die EU-Quantenstrategie sieht beispielsweise ein hochrangiges Beratungsgremium vor, das führende europäische Quantenwissenschaftler und Technologieexperten zusammenbringt, um unabhängige strategische Leitlinien zu liefern.\textbf{\href{https://ec.europa.eu/commission/presscorner/detail/en/ip_25_1682}{https://ec.europa.eu/commission/presscorner/detail/en/ip\_25\_1682}} Auch der US National Quantum Initiative Act (NQI Act) sieht die Einrichtung eines NQI Advisory Committee mit Vertretern aus Industrie, Universitäten und Bundeslaboren vor, um R\&D, Standards und Fragen zu beraten.\textbf{\href{https://www.klgates.com/The-National-Quantum-Initiative-Act-A-Bid-for-US-Quantum-Leadership-01-09-2019}{https://www.klgates.com/The-National-Quantum-Initiative-Act-A-Bid-for-US-Quantum-Leadership-01-09-2019}} Die World Economic Forum (WEF) Initiative zur Quantencomputing-Governance bringt ebenfalls eine globale Multi-Stakeholder-Gemeinschaft zusammen, um Prinzipien zu formulieren.\textbf{\href{https://www.weforum.org/projects/quantum-computing-ethics/}{https://www.weforum.org/projects/quantum-computing-ethics/}}

\subsubsection{\textbf{B. Internationale Regulierungsansätze: Lehren aus KI-Analogien und globaler Koordination}}

Angesichts der globalen Reichweite des Quantencomputings sind internationale Regulierungsansätze unerlässlich, um eine fragmentierte Landschaft von Regeln zu vermeiden und globale Herausforderungen koordiniert anzugehen. Obwohl das Quantencomputing noch in den Kinderschuhen steckt, wird oft auf die Regulierung von Künstlicher Intelligenz (KI) als Analogie verwiesen.

Einige Stakeholder haben sich an die Ethik der KI und des klassischen Computings gewandt, um Orientierung zu finden. Es wird jedoch argumentiert, dass ein unkritisches Übertragen bestehender Prinzipien aus der KI- und Computerethik unangemessen ist, da Quatencomputing grundlegende materielle Unterschiede aufweist, sich in einem früheren Entwicklungsstadium befindet und anders entwickelt wird.\textbf{\href{https://www.researchgate.net/publication/391817020_AI_and_quantum_computing_ethics-_same_but_different_Towards_a_new_sub-field_of_computing_ethics}{https://www.researchgate.net/publication/391817020\_AI\_and\_quantum\_computing\_ethics-\_same\_but\_different\_Towards\_a\_new\_sub-field\_of\_computing\_ethics}} Dennoch können Frameworks wie das EU-KI-Gesetz \textbf{\href{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}} oder das NIST AI Risk Management Framework (AI RMF) der USA \textbf{
\href{https://www.paloaltonetworks.com/cyberpedia/nist-ai-risk-management-framework}{https://www.paloaltonetworks.com/cyberpedia/nist-ai-risk-management-framework}
} wertvolle Lehren für Transparenz, Rechenschaftspflicht und menschliche Aufsicht bieten. Das EU-KI-Gesetz definiert beispielsweise vier Risikostufen (inakzeptabel, hoch, begrenzt, minimal) und legt strenge Pflichten für Hochrisiko-KI-Systeme fest, die auf Quatencomputing-Anwendungen übertragen werden könnten.\textbf{\href{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}{https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai}}

Die geopolitische Konkurrenz zwischen den USA, China und der EU kann fragmentierte Regulierungslandschaften schaffen, was die globale Zusammenarbeit behindern könnte. \textbf{\href{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}{https://dig.watch/updates/europes-quantum-ambitions-meet-us-private-power-and-chinas-state-drive}} China hat seine eigenen KI-Richtlinien, die sich auf menschliches Wohlergehen, Fairness und Kontrollierbarkeit konzentrieren. \textbf{\href{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}} Diese nationalen Strategien, obwohl wettbewerbsorientiert, zeigen ein globales Bewusstsein für die Notwendigkeit von Governance. Die Notwendigkeit internationaler Verträge und harmonisierter Standards ist von größter Bedeutung, um globale Sicherheit und gerechten Zugang zu gewährleisten. Multilaterale Foren wie die OECD spielen eine Rolle bei der Förderung inklusiver, Multi-Stakeholder-Diskussionen über Technologiepolitik. 
\textbf{\href{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}} Das Ziel ist es, adaptive und global koordinierte Regeln für Quantentechnologien zu entwickeln.\textbf{\href{https://www.uibk.ac.at/media/filer_public/0b/c1/0bc1bb22-f6c2-463c-98d1-216866ff9b50/flwp_2025-2.pdf}{https://www.uibk.ac.at/media/filer\_public/0b/c1/0bc1bb22-f6c2-463c-98d1-216866ff9b50/flwp\_2025-2.pdf}}

\subsubsection{\textbf{Offene Standards vs. Proprietäre Entwicklung: Implikationen für Zugang, Innovation und Gerechtigkeit}}

Die Wahl zwischen offenen Standards und proprietärer Entwicklung hat tiefgreifende Implikationen für den Zugang zu Quantentechnologien, die Geschwindigkeit der Innovation und die Verteilung ihrer Vorteile.

Proprietäre Entwicklung, die oft von großen Technologieunternehmen vorangetrieben wird, ermöglicht es diesen Unternehmen, Wettbewerbsvorteile durch den Schutz ihres geistigen Eigentums und die Kontrolle über ihre Technologie-Stacks zu erzielen. \textbf{\href{https://www.weforum.org/stories/2025/01/quantum-technology-business/}{https://www.weforum.org/stories/2025/01/quantum-technology-business/}} Dies kann zu schnelleren Innovationen innerhalb dieser geschlossenen Ökosysteme führen, da Ressourcen gebündelt und Entscheidungen zentralisiert werden können. Allerdings kann dieser Ansatz den Zugang zu Quatencomputing-Ressourcen und -Wissen begrenzen, was Ungleichheiten schaffen könnte.\textbf{\href{https://philarchive.org/archive/AAKEIO}{https://philarchive.org/archive/AAKEIO}} Wenn der Zugang zu Quantencomputing-Ressourcen auf diejenigen mit dem notwendigen Fachwissen und der Finanzierung beschränkt ist, kann dies Auswirkungen auf Einzelpersonen und die Gesellschaft insgesamt haben. \textbf{\href{https://fastercapital.com/topics/the-ethics-of-quantum-computing-development.html}{https://fastercapital.com/topics/the-ethics-of-quantum-computing-development.html}} Zudem kann die Verwendung von nicht verifizierten oder proprietären Algorithmen in der Quantencomputing-Forschung zu Bedenken hinsichtlich Transparenz und Rechenschaftspflicht führen.\textbf{\href{https://www.quera.com/blog-posts/quantum-ethics}{https://www.quera.com/blog-posts/quantum-ethics}}

Im Gegensatz dazu fördern offene Standards und Open-Source-Entwicklung Transparenz, unabhängige Überprüfung und die breitere Beteiligung an der Entwicklung von Quantentechnologien. \textbf{\href{https://ndupress.ndu.edu/Media/News/News-Article-View/Article/3447271/quantum-computing-a-new-competitive-factor-with-china/}{https://ndupress.ndu.edu/Media/News/News-Article-View/Article/3447271/quantum-computing-a-new-competitive-factor-with-china/}} Open-Source-Quantenforschungsinitiativen, Technologietransfer- und Austauschabkommen sowie Investitionen in die Quantenbildung in Entwicklungsländern können dazu beitragen, einen gerechteren Zugang zu gewährleisten. \textbf{\href{https://telefonicatech.com/en/blog/quantum-computing-governance-balancing-innovation-security-and-global-stability}{https://telefonicatech.com/en/blog/quantum-computing-governance-balancing-innovation-security-and-global-stability}} Eine offene Entwicklung kann auch die Zusammenarbeit über Grenzen hinweg fördern und sicherstellen, dass Quantentechnologien letztendlich der breiteren wissenschaftlichen Gemeinschaft und der Gesellschaft zugutekommen. \textbf{\href{https://www.uibk.ac.at/media/filer_public/0b/c1/0bc1bb22-f6c2-463c-98d1-216866ff9b50/flwp_2025-2.pdf}{https://www.uibk.ac.at/media/filer\_public/0b/c1/0bc1bb22-f6c2-463c-98d1-216866ff9b50/flwp\_2025-2.pdf}}


\subsubsection{\textbf{Steuerungs- und Kontrollmechanismen: Gebotene Schutzmaßnahmen}}

Um die Herausforderungen des Quantencomputings zu bewältigen, sind spezifische Steuerungs- und Kontrollmechanismen unerlässlich. Diese müssen proaktiv sein und in den gesamten Lebenszyklus der Quatencomputing-Systeme integriert werden.

Ein zentraler Mechanismus ist die \textbf{Transparenz und Rechenschaftspflicht}. \textbf{\href{https://www.quera.com/blog-posts/quantum-ethics}{https://www.quera.com/blog-posts/quantum-ethics}} Angesichts der Komplexität von Quantenalgorithmen ist es entscheidend sicherzustellen, dass ihre Funktionsweise und Entscheidungsprozesse nachvollziehbar sind. Dies ist besonders wichtig in sensiblen Bereichen wie dem Gesundheitswesen oder der Finanzwirtschaft, wo algorithmische Entscheidungen weitreichende Auswirkungen haben können. \textbf{\href{https://www.scientific-computing.com/article/ethics-quantum-computing}{https://www.scientific-computing.com/article/ethics-quantum-computing}} Die Forderung nach "Erklärbarkeit" von Algorithmen ist hierbei ein Schlüsselprinzip.

\textbf{Menschliche Aufsicht} ist ein weiterer kritischer Kontrollmechanismus. Unabhängig von der Autonomie von Quatencomputing-Systemen muss der Mensch die letztendliche Kontrolle und Verantwortung behalten. Dies beinhaltet das Recht, von KI-gestützten Diensten abzulehnen oder sich jederzeit aus Interaktionen zurückzuziehen. \textbf{\href{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}{https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/}}

Die systematische Integration in den Entwicklungsprozess umfasst die Identifizierung potenzieller Schwachstellen, insbesondere im Hinblick auf die Bedrohung aktueller Verschlüsselungsmethoden durch Quantencomputer.\textbf{\href{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}} Die Beschleunigung der Einführung von\textbf{ quantensicherer Kryptographie (PQuatencomputing) }ist eine dringende Maßnahme, um Daten und kritische Infrastrukturen zu schützen.

Das Prinzip der Integration von Prinzipien in das Design ist von entscheidender Bedeutung. Es bedeutet, Überlegungen von Anfang an in das Design und die Entwicklung von Quantentechnologien zu integrieren, anstatt sie nachträglich anzupassen. Dies beinhaltet die Berücksichtigung von Fairness, Datenschutz und Nachhaltigkeit bereits in der Konzeptionsphase. \cite{arrow_holistic_2023}
\textbf{\href{https://fastercapital.com/topics/the-ethics-of-quantum-computing-development.html}{https://fastercapital.com/topics/the-ethics-of-quantum-computing-development.html}}

\textbf{Bildung und Sensibilisierung} sind ebenfalls wichtige Steuerungsmechanismen.\textbf{\href{https://www.scientific-computing.com/article/ethics-quantum-computing}{https://www.scientific-computing.com/article/ethics-quantum-computing}} Die breite Öffentlichkeit, politische Entscheidungsträger und Stakeholder müssen über die potenziellen Auswirkungen von Quatencomputing informiert werden, um fundierte Diskussionen und Entscheidungen zu ermöglichen. Dies umfasst auch die Ausbildung einer qualifizierten Arbeitskraft.\textbf{\href{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}}

Schließlich ist \textbf{internationale Zusammenarbeit} unerlässlich, um globale Standards und Regulierungsansätze zu harmonisieren.\textbf{\href{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}}

\subsubsection{\textbf{Institutionalisierung fairer, partizipativer und globaler Prozesse}}

Die Institutionalisierung fairer, partizipativer und globaler Prozesse ist entscheidend, um sicherzustellen, dass die Entwicklung und Nutzung des Quantencomputings der gesamten Menschheit zugutekommt. Dies erfordert bewusste Anstrengungen zur Demokratisierung des Zugangs und zur Förderung der Inklusion.

Ein zentraler Aspekt ist die \textbf{Demokratisierung des Zugangs} zu Quantenressourcen und -wissen.Dies kann durch Open Access-Initiativen, Quantum-as-a-Service (QaaS)-Modelle und die Förderung der Bildung erreicht werden. \textbf{\href{https://www.covingtondigitalhealth.com/2024/01/quantum-computing-action-in-the-eu-and-potential-impacts/}{https://www.covingtondigitalhealth.com/2024/01/quantum-computing-action-in-the-eu-and-potential-impacts/}} Derzeit sind die Ressourcen für Quatencomputing auf wenige Nationen konzentriert. \textbf{\href{https://www.quera.com/blog-posts/quantum-ethics}{https://www.quera.com/blog-posts/quantum-ethics}} Daher sind gezielte Programme zur Bildung und zum Aufbau von Arbeitskräften unerlässlich, um eine breitere Bevölkerung auf die Quantenära vorzubereiten. \textbf{\href{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}{https://www.brookings.edu/articles/what-to-do-while-pursuing-the-promise-of-quantum-computing/}} Dies beinhaltet die Anpassung von Bildungsprogrammen und die Einführung von Berufsausbildungskursen. \textbf{\href{https://www.oecd.org/en/blogs/2025/02/a-policymakers-guide-to-quantum-technologies-in-2025.html}{https://www.oecd.org/en/blogs/2025/02/a-policymakers-guide-to-quantum-technologies-in-2025.html}}

\textbf{Technologietransfer und Wissensaustausch} sind entscheidend. UNESCO-Initiativen, wie das "Internationale Jahr der Quantenwissenschaft und -technologie 2025", zielen darauf ab, die Forschungskapazitäten zu stärken, die internationale Zusammenarbeit zu verbessern und ein integratives Wachstum in der Quantenwissenschaft und -technologie weltweit zu fördern.Diese Initiativen erkennen an, dass viele Länder, insbesondere im Globalen Süden, Herausforderungen beim Zugang zu Technologie, Forschungsfinanzierung und globalen Kooperationsmöglichkeiten gegenüberstehen. \textbf{\href{https://www.unesco.org/en/articles/participate-unesco-survey-quantum-science-and-technology}{https://www.unesco.org/en/articles/participate-unesco-survey-quantum-science-and-technology}}

\textbf{Internationale Zusammenarbeit} ist von größter Bedeutung, um fragmentierte Forschungs- und Regulierungsansätze zu vermeiden, die durch geopolitische Konkurrenz entstehen können.\textbf{\href{https://www.uibk.ac.at/media/filer_public/0b/c1/0bc1bb22-f6c2-463c-98d1-216866ff9b50/flwp_2025-2.pdf}{https://www.uibk.ac.at/media/filer\_public/0b/c1/0bc1bb22-f6c2-463c-98d1-216866ff9b50/flwp\_2025-2.pdf}} Die USA haben beispielsweise bilaterale Partnerschaften im Bereich der Quanteninformationswissenschaft (QIS) geschlossen, um die Zusammenarbeit zwischen Forschungseinrichtungen, Universitäten und der Industrie zu vertiefen.\textbf{\href{https://jpia.princeton.edu/news/us-must-deepen-quantum-partnerships-allies-compete-china}{https://jpia.princeton.edu/news/us-must-deepen-quantum-partnerships-allies-compete-china}} Es ist wichtig, diese Partnerschaften zu vertiefen und multilaterale Foren zu nutzen, um gemeinsame Werte und Standards zu etablieren. \cite{oecd}
\textbf{

\href{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}{https://www.oecd.org/en/topics/sub-issues/quantum-technologies.html}
}



\section{Normative Spannungsfelder  }
Die Verteilung von Verantwortung im Quantencomputing ist komplex und vielschichtig, da sie politische, wirtschaftliche und ethische Aspekte umfasst. Die Fragen, wer Entscheidungen trifft, wer für die Folgen haftet und wie mit Machtasymmetrien umgegangen wird, sind von zentraler Bedeutung für eine ethische Entwicklung und Nutzung von Quantencomputing.

\subsection{Zugang und Gerechtigkeit}


Die Vision eines breit zugänglichen Quantencomputing weckt die Erwartung, technologische Teilhabe grundlegend neu zu gestalten. Offene Cloud-Plattformen, frei verfügbare Software-Frameword sowie internationale Bildungskooperationen könnten Studierenden, Start-ups und Forschungseinrichtungen ohne eigene Kryo-Labore den direkten Zugriff auf reale Quantenprozessoren erlauben. In einer idealen Ausprägung senken solche Open-Access-Wege nicht nur Eintrittsbarrieren, sondern schaffen neue Lernökosysteme, in denen Programmier-AGs an Schulen mit Nobelpreislaboren vernetzt sind; ganze Regionen könnten so erstmals eigenständig Innovationspfade beschreiten.

Doch genau hier öffnet sich ein tiefes normatives Spannungsfeld. Virtueller Zugang allein garantiert keine strukturelle Gerechtigkeit, solange die physischen Schlüsselressourcen – supraleitende Chip-Fertigung, isotopenreine Silizium-28-Wafer oder energiefressende Dilution-Kühlsysteme – in den Händen weniger ökonomisch mächtiger Akteure verbleiben. Entsteht eine Quanten-Elite, kann sie Forschungsagenden, Standardisierungsprozesse und Lizenzmodelle diktieren. Für Staaten mit begrenzter Wirtschaftskraft droht eine neue technologische Abhängigkeit: Rechenzeit wird zwar „on demand“ angeboten, echte Souveränität über kritische Infrastruktur jedoch verwehrt.

Hinzu kommt das Phänomen eines potenziellen „Braindrain“. Hochqualifizierte Talente aus dem Globalen Süden erhalten Stipendien an hardwareführenden Universitäten, bleiben dort jedoch häufig dauerhaft, weil heimische Institutionen weder Infrastruktur noch Karrierepfade bieten. So vergrößert sich die Wissenskluft, während die Herkunftsländer weiterhin auf externen Quanten-Dienstleistungen angewiesen sind. Selbst umfangreiche Open-Source-Lehrpläne können diese Lücke nicht schließen, wenn vor Ort Laborkapazitäten und Wartungskompetenz fehlen.

Auch die ökonomische Gestaltung von Freemium-Tarifen für Cloud-Rechenzeit wirft Gerechtigkeitsfragen auf: Wer sich längere und komplexere Programme leisten will, muss zahlen – oft in Währungen und mit Zahlungsmitteln, die in Schwellen- und Entwicklungsländern schwer zugänglich sind. Dadurch entsteht eine digitale Schichtstruktur: kostenfreie Einstiegskurse für alle, fortgeschrittene Quantenressourcen für zahlungskräftige Kundschaft.

Ein weiterer Aspekt betrifft das geistige Eigentum. Selbst wenn Quantenalgorithmen quelloffen publiziert werden, sind sie häufig auf proprietäre Hardware zugeschnitten. Ohne plattformunabhängige Schnittstellen drohen Code-Lock-ins, die kollaborative Innovation einschränken. Regulatorische Ansätze können hier ansetzen, indem sie offene Hardware-Abstraktionsebenen und portierbare Compiler vorschreiben, sodass Wissenstransfer nicht an Unternehmensgrenzen scheitert.

Die normative Kernfrage verschiebt sich somit: Nicht ob Quantencomputing offen oder exklusiv sein wird, sondern welche Tiefen ebenen des Zugangs – von Bildungsinhalten über Testzugänge bis hin zu Fertigungskompetenz – erforderlich sind, um echte Chancengleichheit zu erreichen. Gelingt es, Hardware-Kapazitäten, Ausbildungsförderung und wirtschaftliche Teilhaberechte global neu zu verteilen, kann Quantencomputing Innovationslücken schließen und wissenschaftliche Diversität stärken. Misslingt dieser Balanceakt, droht die Technologie zum Verstärker bestehender Ungleichheiten zu werden – und damit genau jene digitale Hierarchie zu reproduzieren, die sie eigentlich überwinden sollte. \cite{seskirDemocratizationQuantumTechnologies2022}
\subsubsection{Demokratisierung – Open Access, QaaS und Bildung}
Quantencomputing soll im Idealfall denselben Klick-Zugang bieten wie herkömmliche Cloud-Dienste: Ein Laptop plus Internet genügt, um reale Qubits zu programmieren. Dieses Versprechen entfaltet jedoch mehrere Tiefenschichten, sobald die konkreten Bedingungen des Zugangs betrachtet werden.

\textbf{Vom offenen Wissen zur offenen Hardware}
Preprint-Server, Diamond-Open-Access-Journale und offene Frameworks wie Qiskit oder PennyLane schaffen einen frei verfügbaren Wissenspool für Algorithmen, Fehlermodelle und Benchmarks. Dieser Pool speist Lehrbücher, MOOCs und Hackathons und senkt so die kognitive Eintrittsschwelle .
Seit Mai 2016 stehen reale Prozessoren in der Cloud; IBM, D-Wave und Xanadu verknüpfen den Fernzugriff („Quantum-as-a-Service“, QaaS) mit Tutorials, Summerschool-Streams und Textbooks, um eine globale Nutzerbasis aufzubauen . Einsteigende erhalten kostenlose Kontingente, komplexere Jobs werden über gestaffelte Minutenpakete abgerechnet . So entsteht ein leicht zugängliches Front-End, während die teure Kryo- und Wafer-Infrastruktur im Hintergrund bleibt.\cite{seskirDemocratizationQuantumTechnologies2022}

\textbf{Bedingter Zugang – Front-End versus Back-End}
API-Login, Simulatoren und Freemium-Rechenzeit demokratisieren die Oberfläche. Die Schlüsselressourcen – Chip-Foundries, Verdünnungskryostaten, isotopenreine Wafer – bleiben jedoch räumlich und ökonomisch konzentriert; dadurch kann der digitale Graben eher wachsen als schrumpfen.
Für Quantencomputer lässt sich ein weltweiter Cloud-Dienst wirtschaftlich realisieren; für andere Quantentechnologien bricht dieses Modell ein. Schon bei Quanten-Key-Distribution würden selbst nach drastischer Kostensenkung Milliardenbeträge anfallen, um eine breite Hardware-Verteilung zu finanzieren.\cite{seskirDemocratizationQuantumTechnologies2022}

\textbf{Wer profitiert – und warum?}
Direkte Stakeholder wie Entwickler-Teams, Forschende und Start-ups nutzen QaaS, um ohne eigenes Labor Experimente zu fahren und Prototypen zu testen.
Indirekte Stakeholder – Bürgerinnen und Bürger, die Anwendungen später einsetzen oder politisch mitgestalten – bleiben oft auf die Rolle zu schulender Öffentlichkeit beschränkt, ohne echten Einfluss auf Standards oder Roadmaps.
Unternehmen gewinnen durch frühes Community-Building First-Mover-Vorteile: Ein großer User-Funnel sichert Markt- und Standardisierungsvorsprung.
Das Ergebnis: Fast jede Person mit Breitbandanschluss kann einen Account anlegen, doch nur eine kleine Expertengruppe kontrolliert die Hardwaretiefe und die Richtung der Normensetzung.\cite{seskirDemocratizationQuantumTechnologies2022}

\textbf{Bildung als Langzeit-Hebel}
Der globale Mangel an „Quantum-Aware Engineers“ treibt umfangreiche Bildungsinitiativen an. Mehr als 100 Mio. USD fließen allein bei IBM in Lehrbücher, Hackathons und Sommerschulen; drei Millionen Menschen wurden bereits erreicht . Grass-Roots-Netzwerke wie Q-World veranstalten Workshops in Dutzenden Ländern und übersetzen Materialien in zahlreiche Sprachen .
Virtuelle Lernpfade entfalten ihre gesellschaftliche Wirkung jedoch erst, wenn lokale Hochschulen, Labore und Karrierewege anschließen – sonst zieht qualifiziertes Personal dauerhaft in die etablierten Hardware-Zentren ab.\cite{seskirDemocratizationQuantumTechnologies2022}

\textbf{Chancengleichheit vs. neue Ungleichheit}
Freemium-Tarife schaffen eine Zwei-Klassen-Struktur: Demo-Zeit für alle, Tiefennutzung gegen Kreditkarte, was viele Schwellenländer ausschließt.
Kompetenzhürden bleiben hoch; ohne Ausbildung in linearer Algebra und Quantenphysik hilft der offene API-Zugang wenig.
Hardware-Monopole und Exportkontrollen können die technische Souveränität ärmerer Staaten einschränken und bestehende Machtasymmetrien verfestigen.
Offener Cloud-Zugang erweist sich damit als notwendige, aber keineswegs hinreichende Bedingung für echte Teilhabe. (\cite{seskirDemocratizationQuantumTechnologies2022})

\subsubsection{Gefahr einer „Quanten-Elite“}
Quantencomputing entwickelt sich in einem räumlich und institutionell hoch konzentrierten Ökosystem. Je tiefer man in die Liefer- und Entscheidungsketten blickt, desto stärker verdichtet sich die Macht in wenigen Clustern aus Hardware-Herstellern, Hyperscale Clouds und sicherheitspolitischen Akteuren. Damit entsteht das Risiko, dass eine Quanten-Elite den künftigen Zugang und die Spielregeln des Feldes definiert – mit spürbaren Folgen für Chancengleichheit. (\cite{seskirDemocratizationQuantumTechnologies2022})

\textbf{Hardware-Monopole und Back-End-Kontrolle}
Während offene Clouds den Eindruck vermitteln, jeder könne mit Qubits experimentieren, bleiben Kryoanlagen, supraleitende Foundries und isotopenreine Wafer in der Hand weniger Unternehmen und staatlich geförderter Spitzenlabore. Die Hardwareseite „demokratisiert“ also gerade nicht, sondern konzentriert Kapital, Know-how und Patente in ausgesuchten Regionen. Der Aufbau kommerzieller Quantenrechner birgt deshalb das Risiko, bestehende soziale Ungleichheiten eher zu vertiefen als zu verringern. (\cite{seskirDemocratizationQuantumTechnologies2022})

\textbf{Pfadabhängigkeit durch Early-User-Ökosysteme}
Cloud-Provider vergeben Gratiskontingente, richten Hackathons aus und veröffentlichen Tutorials, um eine frühzeitige Nutzerbasis aufzubauen. Wer die meisten „Early User“ an sich bindet, sichert sich First-Mover-Vorteile, weil Standards, Gate-Sets und API-Rhythmen später selten grundlegend geändert werden. Diese Pfadabhängigkeit verschiebt die Marktmacht nachhaltig zugunsten der ersten Plattformen. (\cite{seskirDemocratizationQuantumTechnologies2022})

\textbf{Geopolitik, Exportkontrollen und Souveränität}
Quantum wird zunehmend als sicherheitsrelevante Schlüsseltechnologie eingeordnet. Export-listen (etwa das WassenaarAbkommen) schränken die Weitergabe von Subsystemen und Talenten ein; bilaterale Militärpakte wie AUKUS bündeln Forschungsgelder exklusiv. Solche Maßnahmen zielen auf strategische Autonomie, entziehen aber gleichzeitig einer breiten Öffentlichkeit den direkten Einfluss auf Entwicklungspfade. (\cite{seskirDemocratizationQuantumTechnologies2022})

\textbf{Talente und Wissen als Selektionsfilter}
Offene MOOCs und Lehrbücher senken die kognitive Hürde nur bedingt. Ohne weiterführende Laborplätze, Stipendien oder Karrierepfade verlagern sich qualifizierte Fachkräfte in die Zentren der Hardwareproduktion. Auf diese Weise entsteht eine zweistufige Wissensordnung: globale Lernmodule an der Oberfläche, tiefes Systemverständnis in den Händen der Quanten-Elite (\cite{seskirDemocratizationQuantumTechnologies2022}).

\textbf{Folgen für Zugang und Gerechtigkeit} Wer bekommt Zugang – und zu welchen Bedingungen? Virtueller Erstzugang bleibt breit möglich, doch bezahlbare Premium-Rechenzeit, Hardware-Insights und Normensetzung hängen an Kapital, Visa und Netzwerken der Kernländer.
Chancengleichheit oder neue Ungleichheit?
Solange Back-End-Ressourcen, Standardisierung und Sicherheitspolitik in wenigen Händen liegen, verstärkt Quantencomputing bestehende Hierarchien. Erst wenn Fertigungs- und Governance-Kompetenzen globaler verteilt werden, kann die Technik tatsächliche Teilhabe schaffen (\cite{seskirDemocratizationQuantumTechnologies2022}).


\subsubsection{Globale Ressourcen-Verteilung}
\textbf{Verschiebung der Wertschöpfung in Nord-Cluster}
Die materielle Basis des Quantencomputings – isotopenreine Wafer, supraleitende Foundries und Dilutionskryostaten – konzentriert sich in wenigen Regionen mit hoher Kapital¬dichte. Front-End-Nutzende können zwar per Cloud Qubits ansteuern, doch die Back-End-Hoheit über Fabriken, Lieferketten und Patente bleibt bei einem engen Kreis von Unternehmen und staatlich geförderten Spitzenlaboren. Für Länder ohne eigene Halbleiterindustrie bedeutet das faktisch Miet-Computing statt Technik-Souveränität (\cite{seskirDemocratizationQuantumTechnologies2022}).

\textbf{Preisschwellen: von Freemium-Minuten zu Milliardeninvestitionen}
Cloud-Portale bieten kostenfreie Einstiegs¬kontingente; komplexe Jobs erfordern jedoch kosten-pflichtige Minutenpakete und internationale Zahlungsmittel. Bei Hardware zeigt sich das Gefälle noch deutlicher: Bereits stark vergünstigte QKD-Geräte liegen um 100 k € pro Einheit; selbst eine hundertfache Preisreduktion würde für eine flächendeckende EU-Ausrollung gut 200 Mrd. € erfordern. Der finanzielle Sockel für echte Massennutzung fehlt damit im Globalen Süden wie im öffentlichen Sektor vieler Industriestaaten (\cite{seskirDemocratizationQuantumTechnologies2022}).

\textbf{Regulatorischer Engpass und Geopolitik}
Exportkontrollen (Wassenaar-Liste, ITAR, EAR) drosseln die Verbreitung wichtiger Subsysteme und erschweren die Rekrutierung ausländischer Talente. Gleichzeitig verstärken militärisch motivierte Förderprogramme – etwa AUKUS oder nationale “Technologiesouveränitäts”-Agenden – die Blockbildung zwischen USA, China und EU. So entsteht ein politischer Filter, der den Zugang über finanzielle Aspekte hinaus auch durch Visa- und Lizenzregime begrenzt (\cite{seskirDemocratizationQuantumTechnologies2022}).

\textbf{Talente als knappste Ressource}
Hundertmillionen-Investitionen in Online-Kurse, Hackathons und Praktikumsprogramme sollen den globalen Fachkräfte¬mangel lindern; rund drei Millionen Lernende wurden bereits erreicht. Ohne lokale Labore und Karrierepfade mündet diese Ausbildung jedoch oft in dauerhafte Abwanderung in die Hardware-Zentren – eine Wissensmigration, die den Graben zwischen Nord und Süd weiter vertieft (\cite{seskirDemocratizationQuantumTechnologies2022}).

\textbf{Ansätze zur breiteren Verteilung}
Foundry-Sharing: Qualitativ geprüfte Spin-Qubit-Chips könnten an Hochschulen weltweit verteilt werden. Das Foundry-Modell senkt Einstiegskosten und verteilt Experimentiermöglichkeiten jenseits der Wohlstandscluster.
Gestaffelte Cloud-Tarife: Abrechnungsmodelle nach Kaufkraftklassen reduzieren das Pay-to-Compute-Gefälle.
Offene Standardgremien: Inklusive Governance vermeidet Pfadabhängigkeiten, die frühe API-Entscheidungen einfrieren und damit Marktbarrieren zementieren (\cite{seskirDemocratizationQuantumTechnologies2022}).

Wer bekommt Zugang – und zu welchen Bedingungen? Breitband und Freemium eröffnen oberflächlichen Zugang. Substantielle Nutzung (lange Rechenzeiten, Hardware-Einblick, Normensetzung) bleibt an Kapital, Exportgenehmigungen und etablierte Netzwerke gebunden.
Chancengleichheit oder Verschärfung von Ungleichheit? Solange Schlüsselressourcen, Talente und Regulierungsmacht in wenigen Clustern konzentriert sind, verstärkt Quantencomputing existierende Machtasymmetrien. Erst durch verteilte Foundries, differenzierte Cloud-Preise und inklusive Governance rückt echte Chancengleichheit in Reichweite (\cite{seskirDemocratizationQuantumTechnologies2022}).

\subsection{Sicherheit und Kontrolle}
Quantum Key Distribution (QKD) gilt als vielversprechendste Anwendung für eine radikal neue Sicherheitsarchitektur: Da bereits der Versuch des Abhörens unweigerlich den Quantenzustand verändert, lässt sich Manipulation physikalisch nachweisen. In der Praxis eröffnet das die Chance, die verwundbarste Stelle klassischer Kryptosysteme – den Schlüsselaustausch – in ein nachweisbar abhörsicheres Verfahren zu überführen. Wird QKD konsequent in Glasfaserringen großer Metropolen oder über satellitengestützte Links implementiert, könnten Regierungen, Betreiber kritischer Infrastrukturen und sensible Wirtschaftszweige einen Kommunikationskanal aufbauen, der selbst künftigen Quanten angreifern standhält. Das Vertrauen in digitale Prozesse würde dadurch erheblich gestärkt: Telemedizinnische Fernoperationen könnten Patientendaten ohne Restrisiko übertragen, Abstimmungssysteme würden ein Integritätsniveau erreichen, das Wahlmanipulation faktisch ausschließt, und industrielle Steuerungsnetze ließen sich so absichern, dass Ausfallangriffe auf Energienetze oder Wasserwerke scheitern. Darüber hinaus birgt QKD das Potenzial, vollständig neue Geschäftsmodelle zu generieren – etwa „trusted node as a service“, bei dem zertifizierte Knoten Dienstleistungen für Banken, Cloud-Provider oder Forschungskonsortien anbieten.

Demgegenüber stehen jedoch Risiken, die aus der Kluft zwischen theoretischer Perfektion und industrieller Wirklichkeit resultieren. Reale QKD-Geräte weisen zwangsläufig Imperfektionen auf: Mininale Abweichungen in Laserquellen oder Detektoren eröffnen Seitenkanäle, über die Angreifer verdeckt Informationen auslesen. Fehlkalibrierte Avalanche-Photodioden lassen sich mit hellen Lichtblitzen in einen deterministischen Modus zwingen („blinding attack“) und umgehen so die quantenphysikalische Alarmschranke; unzureichend abgeschirmte Modulatoren bieten Einfallstore für Trojan-Horse-Attacken, bei denen eingekoppeltes Licht verräterische Signaturen in den Rückstreuungen erzeugt; und bei endlichen Schlüssellängen führen statistische Fluktuationen dazu, dass die theoretisch garantierte Fehlerschranke unterschritten wird und dennoch Informationslecks entstehen. Solche Schwachstellen machen deutlich, dass QKD keineswegs per se unknackbar ist, sondern von sorgfältig geprüfter Hardware, vertrauenswürdigen Lieferketten und laufender Sicherheitszertifizierung abhängt.

Hinzu kommt die makroökonomische Dimension: Der Aufbau großskaliger QKD-Netze erfordert milliardenschwere Investitionen in Spezial faser, Satelliten-Repeater, Kryo-Optik und Quanten-Zufalls generatoren. Viele dieser Komponenten sind exportkontrolliert oder werden von wenigen Herstellern in geopolitisch sensiblen Regionen gefertigt. Wer die physische Hardware kontrolliert, gewinnt nicht nur technischen, sondern strategischen Einfluss – bis hin zur Gefahr, dass einzelne Staaten oder Konzerne die globale Sicherheits infrastruktur dominieren, Standards festlegen und politische Zugangsbedingungen diktieren. Im Extremfall entstünde ein neues Wettrüsten, bei dem Staaten versuchen, fremde QKD-Knoten zu infiltrieren oder Lieferketten gezielt zu stören, um ihre eigene Kommunikations überlegenheit zu sichern.

Das Spannungsfeld verschärft sich weiter durch die notwendige Kopplung von QKD an klassische Netze: Schlüsselmanagement, Authentisierung und Firmware-Updates bleiben nicht-quantische Angriffsflächen. Ohne robuste Schnittstellenstandards kann die Integration selbst hoch sicherer Quantenlinks ausgehebelt werden, wenn klassische Endpunkte kompromittiert sind. Daraus folgt, dass eine zukunftsfähige Sicherheitsstrategie technische Präzision (hochqualitative Komponenten, unabhängige Zertifizierung, Open-Hardware-Audits) mit politischer und wirtschaftlicher Governance (transparente Exportregeln, internationale Audit verfahren, faire Marktzugänge) verknüpfen muss. Erst wenn Liberalisierungspolitik offen gelegt, Lieferketten diversifiziert und Zertifizierungsstellen grenze über schreitend akkreditiert werden, kann QKD sein volles Schutzniveau entfalten, ohne neue Machtasymmetrien oder schwer kontrollierbare Angriffspunkte zu schaffen. 
\cite{sunReviewSecurityEvaluation2022} 

\subsubsection{Quantensichere Verschlüsselung (QKD, PQuatencomputing) als Chance}

Quantenschlüsselverteilung überträgt die Schlüsselinformation auf einzelne Photonen, deren Quantenzustand bereits bei jedem unbefugten Messversuch irreversibel verändert wird. Dadurch entsteht ein physikalisch verankerter Alarmmechanismus: Wird die Übertragungsstatistik gestört, verwerfen die legitimen Partner die betroffenen Bits und akzeptieren nur einen Schlüssel, der nachweislich unbeobachtet blieb. Die Praxis setzt zumeist auf schwach kohärente Laserpulse, deren Mehrphotonenanteil mittels (Decoy-State-Methode) sicher eingeschätzt wird, solange drei Grundannahmen eingehalten sind – konstante Intensität und bekannte Photonenverteilung, vollständige Phasenrandomisierung sowie Ununterscheidbarkeit der Decoy-Zustände bis auf die mittlere Intensität (\cite{sunReviewSecurityEvaluation2022} ).

Aus diesen Voraussetzungen erwächst eine reale Chance für digitale Souveränität: Selbst wenn der Quantenkanal vollständig unter Kontrolle eines Angreifers steht, lässt sich bei korrekter Parametrierung ein informationstheoretisch sicherer Schlüssel extrahieren . Die im Labor entwickelten Konzepte haben den Sprung in Langzeit-Netztests geschafft, wodurch sich großräumige Netze mit quantengesichertem Rückgrat aufbauen lassen (\cite{sunReviewSecurityEvaluation2022} ).

Damit diese Sicherheit verlässlich bleibt, beschreibt das Evaluationsschema messbare Abweichungen – etwa fehlerhafte Phasen, Seitenkanäle des Modulators oder Effizienzunterschiede der Detektoren – als Sicherheitsparameter. Jeder Parameter besitzt ein Prüfverfahren, bei dem ein Tester gezielt typische Angriffe nachstellt, Schwellen festlegt und das Ergebnis als bestanden oder nicht bestanden dokumentiert . Die Klassifizierung in „analysierte“ und „überwachte“ Parameter erlaubt praktikable Grenzwerte, ohne den Schlüssel unnötig zu schrumpfen . Durch diese Systematik entsteht eine Grundlage für unabhängige Zertifizierungen; internationale Vorhaben wie ISO/IEC 23837 knüpfen genau hier an und fördern vertrauenswürdige Produkte für staatliche und industrielle Hochsicherheitsanwendungen (\cite{sunReviewSecurityEvaluation2022}).

\textbf{Risiken für Freiheit, Privatsphäre und Demokratie:}

Die gleiche Technik eröffnet jedoch neue Angriffsflächen. Bereits minimale Abweichungen im Quelllaser – etwa lasergestützte Phasenmanipulation oder kontrollierte Intensitätsänderungen – geben einem Abhörer statistisch verwertbare Hinweise auf den Schlüssel . Im Encoder erlauben Seitenkanäle oder Trojan-Horse-Licht den Blick auf interne Modulatorzustände; die resultierende Unterscheidbarkeit der Signalzustände stellt das Zufallsprinzip infrage (\cite{sunReviewSecurityEvaluation2022} ).

Am Empfänger lassen sich zeitliche oder spektrale Effizienzunterschiede der Detektoren ausnutzen. Beispiele reichen von (Time-Shift-) und (Dead-Time-Attacken) bis hin zum Detector-Blinding, bei dem starke Lichtimpulse den Avalanche-Detektor in einen linearen Modus zwingen und gezielte Klicks provozieren . Selbst Hardwarebeschädigungen durch hochleistungsfähige Laser, die Attenuatoren oder Detektoren degradieren, sind experimentell bestätigt. Hinzu kommt das Phänomen rückwärts emittierter Backflash-Photonen, über das ein Angreifer detektorinterne Vorgänge ausliest (\cite{sunReviewSecurityEvaluation2022} ).

Diese Schwachstellen bedrohen nicht nur die Vertraulichkeit; sie wirken sich auch auf gesellschaftliche Werte aus. Ein Staat oder Konzern, der als einziger die Mittel besitzt, solche Seitenkanäle aufzuspüren oder auszunutzen, erlangt einen Machtvorsprung, der demokratische Gleichgewichte verschieben kann. Umgekehrt verlangt jede wirksame Verteidigung eine lückenlose Überwachung optischer und elektrischer Signale im Live-Betrieb – ein ausuferndes Monitoring, das seinerseits sensible Metadaten preisgeben und damit die Privatsphäre untergraben könnte (\cite{sunReviewSecurityEvaluation2022} ).

\textbf{Abwägung:}
QKD eröffnet einen Weg zu dauerhaft abhörsicheren Schlüsseln und stärkt damit die digitale Selbstbestimmung. Diese Chance realisiert sich jedoch nur, wenn alle Gerätekomponenten stetig geprüft, zertifiziert und im Betrieb überwacht werden. Andernfalls erzeugen die unvermeidlichen Imperfektionen einen potenziell größeren Angriffskorridor als in klassischen Systemen. Freiheit und Demokratie profitieren von einem offenen, transparenten Standardisierungsprozess, der Prüfverfahren, Schwellenwerte und Updates öffentlich nachvollziehbar hält – und damit verhindert, dass quantenbasierte Sicherheit neuen technischen Monopolen oder Überwachungsinfrastrukturen den Weg bereitet (\cite{sunReviewSecurityEvaluation2022} ).

\subsubsection{Überwachungspotenzial / Missbrauch durch Staaten oder Konzerne}
Angreifer mit erheblicher Ressourcenbasis – etwa staatliche Nachrichtendienste oder große Technologiekonzerne – können die im Review beschriebenen Quanten-Hacking-Methoden gezielt einsetzen, um unbemerkt in den Schlüsselaustausch einzugreifen. Schon am Sender lässt sich die Pulsintensität manipulieren: Wird in die Laserdiode ein starkes Licht injiziert, steigt die Ausgangsleistung um mehr als das Dreifache, ohne dass Alice oder Bob eine Abweichung registrieren . Dadurch unterschätzt das Protokoll den Mehrphotonen-Anteil; ein Abhörer erhält perfekte Kopien einzelner Bits, während die gemessene Fehlerrate scheinbar im grünen Bereich bleibt (\cite{sunReviewSecurityEvaluation2022} ).

Im Encoder genügt ein “Trojan-Horse”-Impuls, um den Modulator auszulesen. Ein Teil des eingestrahlten Lichts kehrt moduliert zurück, verrät die gewählte Phase und erzeugt null zusätzliche Bitfehler – perfekte Voraussetzungen für verdeckte Industriespionage oder staatliche Massenüberwachung . Selbst hochsichere Netze geraten so in Gefahr, weil die Attacke keine auffälligen Parameteränderungen im Datensatz hinterlässt (\cite{sunReviewSecurityEvaluation2022} ).

Auf Empfängerseite bieten die Single-Photon-Detektoren gleich mehrere Einfallstore. Durch Zeitverschiebung der ankommenden Pulse (Time-Shift) oder durch gezielte Totzeit-Ausnutzung entscheidet der Angreifer, welcher Detektor klickt, und steuert so das ausgelesene Bitmuster . Noch gravierender ist das Blinding-Szenario: Ein kontinuierliches Starklicht versetzt den Avalanche-Detektor dauerhaft in den linearen Betrieb; anschließend akzeptiert das System nur noch die von „Eve“ vorgegebene Fake-State-Intensität . Hochleistungslaser können die Effizienz einzelner Detektoren außerdem irreversibel absenken oder Bauteile beschädigen, was eine dauerhafte Hintertür schafft, ohne dass dies im Routinebetrieb sofort auffällt (\cite{sunReviewSecurityEvaluation2022} ).

Neben aktiven Manipulationen existieren leise Seitenkanäle: Nach jedem legitimen Klick strahlt ein Avalanche-Detektor einzelne Photonen zurück in die Glasfaser (Backflash). Deren Polarisation oder Laufzeit verrät, welcher Detektor gerade ausgelöst hat – ein passives Abhörfenster für beliebige Dritte entlang der Strecke . Ähnliche Informationen stecken in winzigen Timing-Unterschieden der Detektoren (\cite{sunReviewSecurityEvaluation2022} ).

Ironischerweise kann selbst die Gegenmaßnahme zum Überwachungsproblem werden: Um alle Sicherheitsparameter im laufenden Betrieb innerhalb der Schwellen zu halten, verlangt das Evaluationsschema die kontinuierliche Aufzeichnung optischer und elektrischer Signale . Wird diese Sensorik zentral durch staatliche Regulatoren oder Konzernbetreiber betrieben, entsteht ein umfangreicher Metadaten-Pool – einschließlich Nutzungsprofilen, Betriebszeiten und Fehlerraten – der für Verkehrsanalyse oder Verhaltensscoring missbraucht werden kann. Die geplanten Zertifizierungsmodelle bieten zwar herstellerunabhängige Vertrauensanker, bündeln aber auch Informationen über Schwellenwerte, Testverfahren und erkannte Schwachstellen an einer einzigen Stelle . Wenn Zugangsbeschränkungen oder Offenlegungspflichten fehlen, kippt die Balance zwischen kollektiver Sicherheit und individueller Privatsphäre (\cite{sunReviewSecurityEvaluation2022} ).






\subsection{Nachhaltigkeit und Umwelt}
Quantencomputing wird häufig als „Turbo“ für ökologische Problemlösungen beschrieben. Auf der Chancen Seite steht das Potenzial, Klima- und Energiemodelle um Größenordnungen schneller sowie präziser zu berechnen. Variationale Quantenalgorithmen ermöglichen eine bislang unerreichte Auflösung von Aerosol Prozessen, Wolkenbildung, Ozeanzirkulation und Kohlenstoff Kreisläufen – Parameter, die in heutigen Supercomputer Szenarien oft grob skaliert werden müssen. Dadurch lassen sich Emissionspfade robuster validieren, Extremwetterereignisse früher prognostizieren und Gegenmaßnahmen gezielter planen. Quantenoptimieren können darüber hinaus Stromnetze feiner takten, den Einsatz erneuerbarer Energiequellen in Echtzeit balancieren und Transportketten so reorganisieren, dass Leerfahrten minimiert und Routen treib Stoff effizient angepasst werden. Ebenso verheißungsvoll ist die Suche nach neuen Katalysatoren: Quantenchemische Simulationen können in Stunden statt Jahren Moleküle identifizieren, die Ammoniaksynthese, Zementproduktion oder Batterieressourcen energieärmer gestalten – jede eingesparte Kilowattstunde mindert den CO2-Ausstoß unmittelbar. (\cite{schwabeOpportunitiesChallengesQuantum2025a})

Demgegenüber stehen Risiken, die primär von der Hardwareseite ausgehen. Die meisten aktuellen Qubit-Plattformen erfordern Temperaturen nahe dem absoluten Nullpunkt; Dilution-Kryostaten benötigen Dauerleistungen von mehreren Dutzend Kilowatt. Zwar könnten Rechenzentren theoretisch vollständig mit erneuerbarem Strom betrieben werden, doch die Herstellung isotopenreiner Silizium-28-Wafer, hochreiner Halbleiter und Edelmetall Verdrahtungen erhöht heute schon den ökologischen Fußabdruck. Steigt die Nachfrage, verschärft sich der Konkurrenzdruck um seltene Isotope, Helium-3 oder Niobium – Rohstoffe, deren Gewinnung Energie- und wasserintensiv ist. Selbst Recyclingstrategien stoßen hier an Grenzen: Viele Quantenchips sind hochspezialisierte Einwegprodukte, deren Aufarbeitung technisch anspruchsvoll und wirtschaftlich unattraktiv bleibt. (\cite{rootQuantumTechnologiesContext2025})

Ein geografischer Aspekt verschärft die Lage weiter: Die Vorfertigung hochreiner Komponenten verlagert sich oft in Regionen mit günstiger Energiepolitik, aber laxeren Umweltstandards. So könnten CO2-intensive Herstellungsschritte exportiert werden, während saubere Rechenzentren in Industrieländern von dieser Wertschöpfungsschicht profitieren – eine Verlagerung der Emissionen statt ihrer Reduktion. Auch die Infrastruktur seitige Integration stellt Herausforderungen: Kühlwasser, Abwärme Management und Stromspitzen müssen städtisch eingeplant werden, sonst verdrängen Quantenrechenzentren kommunale Nachhaltigkeitsziele. (\cite{rootQuantumTechnologiesContext2025})

Ein tragfähiger Nachhaltigkeitsaspekt verlangt daher ein doppeltes Vorgehen. Erstens braucht es strikte Life-Cycle-Analysen, die nicht nur den Betrieb, sondern Rohstoffgewinnung, Transport und End-of-Life-Entsorgung erfassen. Nur wenn die eingesparte Energie beziehungsweise das vermiedene CO2 die Produktion und Instandhaltung deutlich übertreffen, ist der Einsatz ökologisch zu rechtfertigen. Zweitens müssen Industriestandards auf Modularität, Materialsubstitution und Kreislaufwirtschaft setzen: Alternativen zu Helium-3, modulare Qubit-Blöcke, die sich nachrüsten statt neu herstellen lassen, oder Abwärmerück gewinnungs systeme, die den Kältebedarf teilweise energetisch kompensieren. Ergänzend sollten politische Instrumente wie Carbon-Pricing, Umweltzertifikate und Exportkontrollen sicherstellen, dass ökologische Kosten sichtbar bleiben und nicht externalisiert werden. \cite{rootQuantumTechnologiesContext2025}

Erst wenn solche Leitplanken früh etabliert sind, hat Quantencomputing die Chance, sich von einem ressourcenintensiven Nischenexperiment zu einer Schlüsseltechnik der klimaneutralen Transformation zu entwickeln. Gelingt das nicht, droht eine neue Generation energiehungriger Rechenzentren, die das Klimaproblem nur verschiebt, statt es zu lösen.

\subsubsection{Energie‑ und Materialverbrauch von Quatencomputing}
Die Zahl der physischen Qubits in experimentellen und kommerziellen Prozessoren wächst exponentiell; bereits jetzt entstehen Prototypen mit mehreren Tausend Qubits, während Fahrpläne großer Plattformen bis 2050 fehlertolerante Systeme mit Millionen Qubits skizzieren (\cite{schwabeOpportunitiesChallengesQuantum2025a}) . Diese Skalierung verheißt immense Rechenleistung, erhöht aber zugleich den Energiebedarf der Gesamt-Infrastruktur, weil jedes weitere Qubit nicht nur Rechenzeit, sondern auch Kontroll-Elektronik, Datenleitung und Stabilisierung auf Kryotemperaturen erfordert. Selbst in hybriden Architekturen, die Quanten- und Hochleistungsrechnen koppeln, bleibt der Stromverbrauch des klassischen Koprozessors ein dominanter Posten, wenn die Quanten-Einheit noch klein ist (\cite{schwabeOpportunitiesChallengesQuantum2025a}).

Der Löwenanteil des laufenden Verbrauchs entfällt heute auf die Kühlung: Verdünnungskryostate halten supraleitende Qubits bei rund 10 mK und benötigen dazu zum Teil mehrere Kilowatt Dauerleistung. Eine nachhaltige Entwicklungsperspektive verlangt deshalb Forschung an effizienteren Kryogenik-Stufen, an Materialien mit höheren kritischen Temperaturen und an Software, die mit weniger aktiven Qubits auskommt (\cite{rootQuantumTechnologiesContext2025}).

Ob Quantencomputer langfristig einen Energie-Vorteil bieten, hängt stark von der Aufgabe ab. Simulationen zeigen ein Schwellenverhalten: Für manche Probleme sinkt die Gesamtenergie, weil der Quantenalgorithmus die Laufzeit drastisch verkürzt; für andere Szenarien bleibt der klassische Rechner trotz längerer Rechenzeit sparsamer. Nachhaltige Nutzung erfordert daher eine präzise Aufgaben- und Hardwarewahl sowie Vergleichsmetriken, die Kilowattstunden pro berechnetem Ergebnispunkt ausweisen (\cite{rootQuantumTechnologiesContext2025}).

Neben dem Betriebsstrom spielt die Materialbilanz eine zentrale Rolle. Quantenchips enthalten supraleitende Metalle, hochreines Silizium, Niob-Titan-Legierungen oder ionenfallenspezifische Edelmetalle. Deren Gewinnung ist mit hohem Energieeinsatz, Abraum und oft problematischen Lieferketten verbunden. Eine ganzheitliche Ökobilanz muss deshalb den Abbau kritischer Rohstoffe, die Halbleiterfertigung und die limitierte Recyclingfähigkeit einbeziehen (\cite{rootQuantumTechnologiesContext2025}).

Technologische Verantwortung bedeutet, ökologische Parameter bereits in der Architektur zu verankern: industrieweite Standards für CO2-Fußabdrücke, Zertifikate für ressourcenschonende Lieferketten, Energie-Metriken in den Gerätespezifikationen und Betriebsrichtlinien, die Lasten in Zeiten und Regionen mit niedrigem Emissionsfaktor verlagern. Ergänzend empfiehlt die Literatur Frugal-Computing-Prinzipien –also bewusste Beschränkung der Problemgrößen und adaptive Power-Caps–, um Rebound-Effekte zu vermeiden (\cite{rootQuantumTechnologiesContext2025}) . Parallel dazu können hybride Workflows mit klassischem HPC den Quantenanteil exakt dort einsetzen, wo er einen klaren Vorteil bringt, und so den Ressourceneinsatz insgesamt senken (\cite{schwabeOpportunitiesChallengesQuantum2025a}).

In Summe zeigt sich: Quantencomputing kann ökologische Herausforderungen adressieren, etwa durch beschleunigte Klimamodelle oder optimierte Material-Simulationen, doch ohne konsequente Beachtung seines eigenen Energie- und Materialverbrauchs würde es neue Lasten schaffen statt bestehende zu mindern. Eine technisch mitgedachte ökologische Verantwortung verbindet daher Effizienzforschung, Lebenszyklus-Analysen und Governance-Instrumente, die Nachhaltigkeit zur Kernmetrik jedes weiteren Qubit-Schritts machen.


\subsubsection{Effizienzsteigerung (Klimamodelle, Energiemanagement)}

Die Suche nach höherer Effizienz hat zwei Stoßrichtungen: erstens das Beschleunigen rechenintensiver Klimamodelle durch quantenbasierte Algorithmen, zweitens das Optimieren realer Energie- und Ressourcennetze mithilfe quantengestützter Entscheidungsverfahren. Beide Ansätze zielen darauf, Rechen- und Stromaufwand pro gewonnener Information drastisch zu senken – ohne die ökologischen Lasten einfach zu verlagern.

\textbf{Klimamodelle schneller und präziser rechnen}
Lineare Gleichungssysteme aus finiten-Differenzen- oder -Elemente-Diskretisierungen lassen sich in logarithmisch vielen Qubits kodieren; damit schrumpfen Speicher- und Zeitbedarf gegenüber klassischen Solver-Routinen von O(poly M) auf O(poly log M) Operationen (\cite{schwabeOpportunitiesChallengesQuantum2025a}) . Für strömungsdynamische Kernkomponenten demonstrieren hybride Verfahren, die Quanten-Amplitude-Schätzung in die Lösung diskretisierter Navier-Stokes-Gleichungen einbetten, potenzielle Exponentialspeed-ups – vorausgesetzt, Ein- und Ausgabeschritte werden effizient kodiert (\cite{schwabeOpportunitiesChallengesQuantum2025a}) . Quantum-Lattice-Boltzmann-Ansätze umgehen das explizite Lösen großer linearer Systeme ganz und versprechen damit weitere Einsparungen bei Zeit- und Energiebedarf, sobald geeignete Fehlerminderungsstrategien etabliert sind (\cite{schwabeOpportunitiesChallengesQuantum2025a}).

Über die Dynamik-Kerne hinaus lässt sich auch die Parametrisierung subskaliger Prozesse effizienter gestalten: Parameterized-Quantum-Circuit-Modelle (QNNs) können mit wenigen trainierbaren Variablen auskommen, was den Trainingsaufwand reduziert und gleichzeitig größere Ausdrucksstärke ermöglicht – ein Vorteil für Wolken- oder Turbulenzschemata, die klassisch enorme Datenmengen benötigen (\cite{schwabeOpportunitiesChallengesQuantum2025a}) . Ebenso kann die automatische Modell-Kalibrierung durch variantenreiche Quanten-Optimierer (bspw. QAOA auf emulierten Zielflächen) deutlich verkürzt werden, sodass Ensemble-Läufe seltener neu gestartet werden müssen (\cite{schwabeOpportunitiesChallengesQuantum2025a}) . Zusammengenommen verringern diese Beschleuniger den Rechen- und Kühlaufwand ganzer Klima-Workflows und eröffnen Spielräume für höhere Auflösung, ohne proportional höhere Emissionen zu erzeugen.

\textbf{Energiemanagement und Betriebsoptimierung}
Auch jenseits der Modellierung erschließt Quantencomputing Effizienzreserven. In Strom- und Wärmenetzen lässt sich die Lastfluss-Optimierung als Ising-Problem formulieren und auf Quantum-Annealern oder PQuatencomputing-basierten Optimierern lösen; frühe Studien weisen auf geringere Iterationszahlen und damit weniger Rechen- sowie Regelenergie im Netzbetrieb hin (\cite{rootQuantumTechnologiesContext2025}) . Nachhaltigkeitsleitlinien fordern jedoch, dass ein quantenbasierter Lösungsweg nur dann eingesetzt wird, wenn er nachweislich weniger Gesamtenergie pro Optimierungszyklus verbraucht als der beste klassische Algorithmus – ein Kriterium, das künftig in standardisierte „Energy-per-Solution“-Metriken einfließen soll (\cite{rootQuantumTechnologiesContext2025}) .
Zur weiteren Effizienzsteigerung rät die Fachliteratur, Quantenjobs zeit- und ortsabhängig in Rechenzentren mit niedriger CO2-Intensität auszuführen, Power-Caps für nicht-kritische Phasen zu setzen und Hardware- wie Software-Stacks gemeinsam zu co-designen, um vermeidbare Überspezifikationen auszuschließen (\cite{rootQuantumTechnologiesContext2025}) . Damit entsteht ein durchgängiges Energiemanagement, das von der Auswahl der Problemklasse über die Wahl des geeigneten Quantenprozessors bis zur Einbindung erneuerbarer Stromquellen reicht.

\textbf{Übergreifende Perspektive}
Effizienzsteigerung heißt nicht nur schneller rechnen, sondern ökologische Gesamtkosten minimieren. Quantenbasierte Klimamodelle können Entscheidungsgrundlagen für die Anpassung an den Klimawandel liefern, während quantengestützte Netz- und Ressourcen-optimierung unmittelbar Primärenergie einspart. Beide Erfolge materialisieren sich jedoch nur, wenn der eigene Energie- und Materialverbrauch lückenlos bilanziert, an belastbare Metriken gekoppelt und kontinuierlich verbessert wird – so wird Quantencomputing von einem potenziellen Zusatzproblem zu einem echten Teil der Lösung.

\subsubsection{Ökologischer Fußabdruck}
Der ökologische Fußabdruck von Quantencomputern umfasst den gesamten Lebenszyklus – von der Rohstoffförderung über Fertigung und Betrieb bis zur Entsorgung. Die Zahl der physischen Qubits steigt laut aktuellen Roadmaps exponentiell; mit jedem weiteren Kühlleiter und Steuerchip wächst der laufende Energiebedarf der Infrastruktur (\cite{schwabeOpportunitiesChallengesQuantum2025a}) . Lebenszyklusanalysen zeigen, dass wesentliche Emissionspfade bereits in vorgelagerten Prozessketten entstehen und ohne vollständige Bilanzierung unsichtbar bleiben (\cite{rootQuantumTechnologiesContext2025}).

Die Herstellung supraleitender oder Ionenfallen-basierter Prozessoren bindet kritische Rohstoffe wie Niob-Titan-Legierungen, Edelmetalle und hochreines Silizium. Abbau und Aufbereitung gehen mit hohem Ressourcenverbrauch, Abraum und oft fragilen Lieferketten einher. Nachhaltigkeits-leitlinien fordern daher Zertifizierungen für verantwortungsvolle Lieferketten, zirkuläres Design und längere Hardwarelebenszyklen, um Scope-3-Emissionen zu reduzieren (\cite{rootQuantumTechnologiesContext2025}).

Im Betrieb dominiert bislang der Strombedarf der Kryogenik: Verdünnungskryostate halten supraleitende Qubits bei 10 mk und verursachen beträchtliche Grundlastleistungen. Gleichzeitig wächst die elektrische Last des klassischen Ko-Prozessors, der Steuersequenzen berechnet und Fehler korrigiert. Eine enge HPC-Integration gilt als Schlüssel, um Rechenlasten optimal aufzuteilen und thermische wie elektrische Verluste zu senken (\cite{schwabeOpportunitiesChallengesQuantum2025a}) . Nachhaltigkeitsforschung empfiehlt Kennzahlen wie Kilowattstunden pro Lösungspunkt, die Einbeziehung regionaler Emissionsfaktoren sowie den Einsatz von Power-Caps – besonders wenn Jobs in Niedrig-CO2-Zeitfenstern oder Rechenzentren mit hohem Anteil erneuerbarer Energien laufen (\cite{rootQuantumTechnologiesContext2025}).

Ein Energie- oder Emissionsvorteil durch Quantenhardware ist aufgabenspezifisch. Studien beschreiben Schwellenwerte: Überschreitet die Hardware diese, sinken die Emissionen je Rechenlauf; bleibt sie darunter, steigt der Gesamtverbrauch. Deshalb werden standardisierte „Energy-per-Solution“-Metriken, adaptive Scheduler und frugale Nutzungsstrategien gefordert, um Rebound-Effekte zu vermeiden (\cite{rootQuantumTechnologiesContext2025}).

Governance-Instrumente verankern ökologische Kriterien bereits in Forschung und Entwicklung. Industriestandards für CO2-Fußabdrücke, Stakeholder-Dialoge über Frugal-Computing-Ansätze sowie politische Anreize für grünen Strom sollen sicherstellen, dass Quantentechnologien tatsächlich mehr zur Lösung ökologischer Probleme beitragen, als neue Belastungen zu schaffen (\cite{rootQuantumTechnologiesContext2025}) .


\subsection{Autonomie und Verantwortlichkeit}

3.4.1 \textbf{”Black-Box“-Entscheidungen durch Quantenalgorithmen}
Mit der zunehmenden Verbreitung von Quantenalgorithmen entstehen neue ethische Fragen – insbesondere dann, wenn Entscheidungen nicht mehr vollständig nachvollziehbar sind. Quantenalgorithmen arbeiten häufig probabilistisch: Ergebnisse basieren auf Wahrscheinlichkeiten und sind nicht immer eindeutig erklärbar. Dies führt zu sogenannten „Black-Box“-Entscheidungen, bei denen weder Nutzer noch Entwickler genau verstehen, wie eine Entscheidung zustande kam.
Gerade in sensiblen Bereichen wie Justiz oder Medizin kann diese Intransparenz problematisch werden. Wer trägt Verantwortung, wenn ein System eine folgenschwere Entscheidung trifft, deren Grundlage niemand versteht? Laut Stahl (2021) ist fehlende Nachvollziehbarkeit ein zentrales ethisches Risiko, das das Vertrauen in technische Systeme untergräbt.\cite{stahl_artificial_2021}
Zudem zeigen Gil-Fuster et al. (2024), dass gängige Methoden der erklärbaren KI (wie SHAP) nur begrenzt auf Quantenmodelle übertragbar sind – unter anderem wegen der komplexen Zustandsräume und der Natur der Quantenmessung. Die technischen Grenzen der Erklärbarkeit verschärfen somit das ethische Dilemma.\cite{gil-fuster_opportunities_2024}
Zukünftig wird entscheidend sein, wie sich Transparenz und Kontrolle trotz wachsender Systemkomplexität sichern lassen. Gelingt dies nicht, droht ein Vertrauensverlust in quantenbasierte Systeme – selbst dann, wenn sie technisch überlegen wären.

 

 3.4.2 \textbf{Verantwortungslücken bei Systemversagen}
 Je autonomer ein technisches System agiert, desto schwieriger wird es, klare Verantwortlichkeiten zu definieren. Im Kontext des Quantencomputings verschärft sich dieses Problem durch die inhärente Komplexität der Technologie: Fehlerhafte Entscheidungen oder unvorhergesehene Systemreaktionen lassen sich oft nicht auf eine konkrete Handlung oder einen spezifischen Code-Fehler zurückführen. So entstehen sogenannte Verantwortungslücken, in denen niemand eindeutig haftbar gemacht werden kann.
Besonders brisant wird dies bei sicherheitskritischen Anwendungen wie etwa in der Verkehrssteuerung, der Energieverteilung oder der staatlichen Verwaltung. Hier können Schäden nicht nur ökonomischer, sondern auch gesellschaftlicher oder menschlicher Natur sein. Ethisch wie rechtlich stellt sich daher die Frage, wie man Verantwortung in solchen Systemen rekonstruieren oder neu definieren kann. Eine Möglichkeit besteht darin, das Prinzip der „kontrollierbaren Autonomie“ stärker zu verankern: Systeme dürfen nur dann autonom agieren, wenn sie von außen überprüfbar, nachvollziehbar und im Ernstfall abschaltbar sind\cite{floridi_ai4peopleethical_2018}.
 
3.4.3 \textbf{Intransparente Systeme }
Nicht nur die Algorithmen, sondern auch die physikalischen und infrastrukturellen Grundlagen des Quantencomputings sind oft schwer nachvollziehbar. Kommerzielle Anbieter wie IBM entwickeln proprietäre Systeme, bei denen die interne Funktionsweise nicht offengelegt wird. Diese Intransparenz erschwert nicht nur das wissenschaftliche Verständnis, sondern auch die gesellschaftliche Kontrolle (IBM, 2023)\cite{noauthor_defining_nodate}.
Gleichzeitig entsteht eine Machtasymmetrie zwischen Tech-Konzernen und der Öffentlichkeit: Entscheidungen über Infrastruktur oder Datensicherheit liegen in der Hand weniger Unternehmen – ohne ausreichende demokratische Kontrolle (Scientific Computing World, 2022)\cite{noauthor_ethics_nodate}.
Ein möglicher Ausgleich wäre die Einführung von \textbf{Transparenzstandards}: etwa verpflichtende Offenlegung ethischer Bewertungen und die Förderung von \textbf{Open-Source-Initiativen}. IBM hat etwa interne Prinzipien gegen militärische Nutzung eingeführt (IBM, 2023)\cite{noauthor_defining_nodate}. Solche Maßnahmen können helfen, das Vertrauen der Gesellschaft zu stärken – eine zentrale Voraussetzung für Akzeptanz und nachhaltige Kontrolle neuer Technologien (Quera, 2023)\cite{noauthor_quantum_nodate}.
 
\subsection{Dual‑Use und militärisches Potenzial}
\textbf{1.3.5.1 \textbf{Friedliche vs. militärische Nutzung (z. B. Quanten‑Cyberwaffen)} }
Quantentechnologien bieten transformative Möglichkeiten für Wissenschaft und Gesellschaft, jedoch auch erhebliche Risiken im militärischen Kontext. Besonders deutlich wird dies bei quantenbasierten Cyberwaffen: So ermöglicht etwa der Shor-Algorithmus, aktuelle asymmetrische Verschlüsselungssysteme zu brechen – mit potenziell drastischen Folgen für sicherheitskritische Infrastrukturen und geheime Kommunikationskanäle\cite{krelina_quantum_2021}. Gleichzeitig erlaubt die Quantenkommunikation durch sogenannte Quanten‑Key‑Distribution (QKD) eine vollkommen abhörsichere Übertragung sensibler Daten, was insbesondere für militärische Einsatzszenarien attraktiv ist\cite{neumann_quantum_2020}.

Diese gegensätzlichen Möglichkeiten unterstreichen die doppelte Verwendbarkeit (Dual-Use) quantentechnologischer Entwicklungen. Während zivile Anwendungen auf medizinische Diagnostik oder Verkehrsoptimierung abzielen, könnten dieselben Technologien in militärischen Kontexten zur Datenüberwachung oder Sabotage von gegnerischer Infrastruktur führen. Die Gefahr besteht darin, dass Staaten Technologien unter dem Deckmantel ziviler Forschung entwickeln, jedoch für offensive Zwecke einsetzen. Ohne transparente internationale Regulierungsmechanismen könnten solche Entwicklungen langfristig zu einer Destabilisierung der globalen Sicherheitsarchitektur führen.

\textbf{ 1.3.5.2 Wettlauf und Kontrollverlust bei globaler Technologiekonkurrenz} 
Die Quantentechnologie ist längst zu einem zentralen Bestandteil geopolitischer Konkurrenz geworden. Länder wie die USA, China und Russland investieren Milliarden in Forschung und Entwicklung, mit dem Ziel, eine technologische Vorherrschaft zu erreichen – insbesondere im Bereich Kommunikation, Kryptografie und militärische Anwendungen (Lawfare, 2024)\cite{howell_restrict_2023}. Diese Dynamik erinnert zunehmend an ein neues Wettrüsten, das nicht nur sicherheitspolitische, sondern auch wirtschaftliche Spannungen verschärft.

Die Furcht vor einem sogenannten „Q-Day“ – dem Tag, an dem Quantencomputer herkömmliche Verschlüsselungen brechen können – hat bereits zu präventiven Maßnahmen geführt. Die USA etwa haben Exportkontrollen verschärft, um zu verhindern, dass kritische Technologien in „feindliche“ Hände gelangen. Solche Maßnahmen, etwa im Rahmen des Wassenaar-Abkommens oder durch US-amerikanische OFAC-Vorgaben, spiegeln die zunehmende Unsicherheit wider, wie mit der grenzüberschreitenden Verfügbarkeit von Quantentechnologie umzugehen ist (Deloitte Insights, 2025) \cite{buchholz_quantum_nodate}.

Der Trend zu nationaler Abschottung und Technologiekontrolle birgt jedoch Risiken: Je stärker sich Staaten voneinander isolieren, desto größer wird die Gefahr eines unkontrollierten Fortschritts ohne ethische oder sicherheitspolitische Rückkopplung. Ein globaler Dialog über gemeinsame Regeln und transparente Forschung ist daher essenziell, um langfristig einen kontrollierten und friedlichen Einsatz von Quantentechnologie sicherzustellen.

 \textbf{1.3.5.3 Erkennung und Begrenzung missbräuchlicher Anwendungen }
 Um dem potenziellen Missbrauch von Quantentechnologien vorzubeugen, sind gezielte Kontrollmechanismen und internationale Zusammenarbeit unabdingbar. Aufgrund des Dual-Use-Charakters besteht eine ethische Verpflichtung, frühzeitig Kriterien zu definieren, die zwischen legitimer Forschung und gefährlicher Militarisierung unterscheiden (Carnegie Council, 2020)\cite{malekos_smith_preparing_2023}. Hierzu gehören verbindliche Exportkontrollen, die Etablierung von Ethikgremien sowie die Verpflichtung zur Offenlegung von Sicherheitsrisiken bei öffentlich finanzierter Forschung.

Ein zukunftsweisender Ansatz wäre die Implementierung von „Ethics by Design“ – also die Integration ethischer und sicherheitspolitischer Kriterien bereits in der Entwicklungsphase neuer Technologien. Darüber hinaus könnte eine globale Datenbank für Quantenprojekte helfen, Transparenz zu fördern und gefährliche Projekte frühzeitig zu identifizieren.

Langfristig stellt sich die Frage, ob sich ein internationaler Governance-Rahmen etablieren lässt, ähnlich wie bei der zivilen Nutzung der Atomkraft. Sollte dies nicht gelingen, droht ein Szenario, in dem nationale Interessen, verdeckte Forschungsprojekte und unklare Haftungsfragen die Oberhand gewinnen. Die Zeit für vorsorgliche Regulierung ist daher jetzt – bevor sich unkontrollierbare Dynamiken verselbstständigen.

 


 

 
 

 

\section{Fazit und Ausblick}
\subsection{Fazit}


Quantencomputing erscheint hier als technischer Doppeldecker–oben der Flug über neue Horizonte in Klima‑, Material‑ und Sicherheitsforschung, unten das lauernde Risiko einer vertieften Kluft zwischen Zentren der Macht und dem Rest der Welt. Gerade weil dieser Kontrast so scharf ausfällt, reicht es nicht, die gängigen KI‑Leitplanken zu kopieren; die Analyse plädiert für ein eigenständiges Feld der„Quantum Ethics“, das die besonderen Hardware‑, Dual‑Use‑ und Materialfragen ausdrücklich adressiert.

Doch Ethik allein genügt nicht, wenn der Zugang zur Kryo‑ und Chip‑Infrastruktur in wenigen Clustern konzentriert bleibt. Ohne gezielte Öffnung droht eine Quanten‑Elite, die Standards und Preise diktiert. Neben fairen Lizenz‑ und Fördermodellen verlangt der Bericht deshalb einen doppelten Sicherheitsansatz: quantenbasierte Schlüsselverteilung (QKD) plus zertifizierte Hardware und diversifizierte Lieferketten, um Seitenkanäle und geopolitische Abhängigkeiten gleichermaßen einzudämmen.

Das ökologische Argument sitzt wie ein Stachel im Fortschritt: milli‑Kelvin‑Kühlung, seltene Metalle und hoher Strombedarf machen harte Energie‑und‑Materialmetriken –etwa „EnergyperSolution“ – zur Voraussetzung jeder Skalierung. All diese Elemente laufen schließlich in den Ruf nach robuster, multilateraler Governance zusammen: Exportkontrollen, Ethics‑by‑Design‑Vorgaben und vertragliche Abstimmungen sollen den Wettlauf bremsen, ohne Innovation zu ersticken.

Bleibt dieser Fünf‑Punkte‑Kurs Theorie, formt sich rasch eine technische Aristokratie; wird er umgesetzt, verwandelt sich das quantische Versprechen in einen kollektiven Fortschritt, der Nutzen und Verantwortung in ein tragfähiges Gleichgewicht bringt.



\subsection{Ausblick}


Der technologische Sprint hin zu praxistauglichen Quantenprozessoren wird in den nächsten fünf bis zehn Jahren einen Kipppunkt erreichen: Erste utility‑scale‑Rechner dürften komplexe Klimamodelle oder Wirkstoffsimulationen in Stunden statt Monaten durchlaufen lassen. Damit schiebt sich die ethische Verantwortung nach vorn –zu einer antizipatorischen Ethik, die Risiken bereits in der Laborphase adressiert, statt sie retrospektiv zu reparieren.

Parallel verdichtet sich der geopolitische Wettbewerb. Ohne koordinierten Regulierungsrahmen drohen Exportkontrollen, Patentkriege und Blockbildungen eine fragmentierte Quantenlandschaft zu zementieren. Die Literatur sieht deshalb einen akuten Bedarf an globaler Koordination, um eine neue technische Spaltung zu vermeiden. Virtueller Cloud‑Zugang allein genügt nicht; solange Kryo‑Fabs, isotopenreine Wafer und Superkühlung in wenigen Clustern verbleiben, wächst die Gefahr einer Quanten‑Elite, die Standards und Preise diktiert.

Ökologisch entscheidet sich der Wert der Technologie an messbaren Kennziffern statt an Visionen. Standardisierte Energy‑per‑Solution ‑Metriken, adaptive Scheduler und frugale Nutzungsstrategien sollen sicherstellen, dass jeder zusätzliche Qubit‑Schritt mehr CO2 einspart, als er verbraucht.

Um Transparenz und Dual‑Use‑Kontrolle zu stärken, schlagen die Autor\*innen eine weltweite Datenbank für Quantenprojekte sowie verbindliche Ethics‑by‑Design‑ Prozesse vor; frühzeitige Offenlegung würde gefährliche Anwendungspfade identifizieren, bevor sie skaliert werden. Entscheidend bleibt jedoch, ob sich –ähnlich dem Atomrecht – ein multilateraler Governance‑Rahmen etablieren lässt; gelingt das nicht, könnte nationale Abschottung die Technologie in unkontrollierbare Dynamiken stoßen.

Kurzum: Die kommende Dekade entscheidet, ob Quantencomputing zum inklusiven Werkzeug globaler Problemlösung wird – oder zum Katalysator neuer Macht‑ und Emissionsgefälle. Die Weichen dafür müssen heute gestellt werden.











