%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Anwendung in der Chemie \& Materialforschung}
\label{trends} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\chapterauthor{Hüma Yilmaz, Sabine Weigand}

\abstract{some abstract}

\section{Relevanz \& Problemstellung}
\section{Top 3 Anwendungsfelder (Praxis \& Theorie)}
\subsection{Simulation von Molekülen und Reaktionen}

Die Simulation von Molekülen und chemischen Reaktionen geht auf Arbeiten von Richard Feynmann und Yuri Manin in den 1980er Jahren zurück \cite{cao_quantum_2019}. Sie erkannten, dass klassische Computer fundamentale Grenzen bei der Simulation quantenmechanischer Systeme haben, da der Rechenaufwand exponentiell mit der Teilchenzahl wächst. Sie übergingen dies, indem sie Quantensysteme mit anderen Quantensystemen emulieren.  Dies eröffnet neue Möglichkeiten, chemische Systeme auf atomarer und molekularer Ebene zu verstehen, zu modellieren und zu steuern.
\subsubsection{Grundlegende Bedeutung und Herausforderungen}
Die Eigenschaften von Molekülen, ihre Bindungen, Reaktionsfähigkeit und Dynamik lassen sich nur durch die Lösung der Schrödinger-Gleichung für Vielteilchensysteme vollständig beschreiben. Klassische Computer stoßen hierbei an ihre Grenzen, da der Rechenaufwand exponentiell mit der Systemgröße wächst. Quantencomputer nutzen im Gegensatz zu klassischen Computern die Prinzipien der Quantenmechanik, um diese Komplexität effizient nachzubilden.

\subsubsection{Arten der Quantensimulation:}
\textbf{Digitale Quantensimulation} \\
Digitale Quantensimulation folgt dem Vorbild moderner klassischer Computer, indem die Berechnung in eine Reihe diskreter \glqq Gatter\grqq-Operationen wird, die gezielt auf quantenmechanischen Systemen wirken. Ein besonderer Vorteil dieses Ansatzes ist die Möglichkeit zur Fehlerkorrektur bei unvollkommenen Implementierungen der Gatter, was potenziell fehlertolerante Operationen für umfangreiche Quantenberechnungen ermöglicht. Allerdings arbeiten die meisten aktuell verfügbaren Geräte ohne solche Fehlerkorrekturen und werden als \glqq noisy intermediate-scale quantum (NISQ) computing\grqq{} bezeichnet.
 Das Rauschen begrenzt dabei die Anzahl der ausführbaren Operationen.(Practical quantum advantage... Daley et al). Digitale Quantensimulatoren sind universell einsetzbar, da sie prinzipiell jeden gewünschten Hamilton-Operator realisieren können und somit ein breites Spektrum von Modellen untersuchen, ohne dass diese direkt im Labor umgesetzt werden müssen. Zwar können Algorithmen zur Präparation komplexer Niederenergiestaaten exponentiell in der Zeit skalieren, doch lässt sich die Entwicklung eines präzisen Ausgangszustands mit beliebig hoher Genauigkeit mit polynomiellem Aufwand durchführen. Für einen praktischen Quantenvorteil sind jedoch noch erhebliche Fortschritte in der Hardware-Entwicklung erforderlich, da die benötigten Gatter-Operationen und Qubits die Fähigkeiten aktueller NISQ-Geräte weit übersteigen.\\
\textbf{Analoge Quantensimulation}\\ 
Analoge Quantensimulation nutzt spezialisierte Quantensysteme, die gezielt bestimmte Klassen von Modellen abbilden, ähnlich wie ein Windkanal die Aerodynamik simuliert. Hierbei wird der gewünschte Hamilton-Operator mit gut kalibrierten Parametern umgesetzt. Ein großer Vorteil dieser Methode besteht darin, dass sie bereits heute auf große Systemgrößen skalieren kann und somit ein natürlicher Bereich für die Suche nach einem praktischen Quantenvorteil im Vergleich zu klassischen Simulationen ist. Allerdings sind analoge Quantensimulatoren anfällig für Kalibrierungsfehler, Dekohärenz und Rauschen. Dennoch lassen sich diese Fehlerquellen prinzipiell bestimmen und in der Praxis begrenzen, sodass die Simulationen eine quantitative Zuverlässigkeit auf dem Niveau der Kalibrierungsfehler erreichen. Analoge Quantensimulatoren haben bereits wertvolle Einblicke in wissenschaftliche Fragestellungen geliefert und operieren in Bereichen, die für klassische Algorithmen unzugänglich sind. Die Einschränkung liegt darin, dass nur solche Modelle simuliert werden können, deren Hamilton-Operatoren direkt im Analogsystem realisiert werden können.\\

\textbf{Hybride digital-analoge Quantensimulation} \\
Hybride digital-analoge Quantensimulation kombiniert die Fähigkeiten von analoger und digitaler Quantensimulation. Diese Methode gilt als vielversprechend für die nahe Zukunft, da sie die Vorteile beider Ansätze vereint. Die nativen analogen Operationen können zur Erzeugung hochverschränkter Zustände genutzt werden, während digitale Kontrolle und Variationsansätze zum Einsatz kommen. In diesen programmierbaren Hybridsimulatoren ist die Systemdynamik nicht mehr auf einfache Evolutionen unter einem nativen Hamilton-Operator beschränkt, da die zeitabhängige Kontrolle die Gestaltung verschiedener Modelle ermöglicht, etwa durch Floquet-Engineering. Hybrid-Algorithmen sind besonders für aktuelle NISQ-Geräte praktisch, da sie die Stärken von Quanten- und klassischen Computern nutzen und die Rechenaufgaben entsprechend aufteilen \cite{cao_quantum_2019}.

\subsection{Materialforschung für Batterien\& Quantenmaterialien}
cite Computational Design of Battery Materials

\subsubsection*{Simulation komplexer Quantensysteme und Materialien}

Batteriematerialien zeichnen sich durch eine ausgeprägte quantenmechanische Komplexität aus, die sich in der Vielzahl elektronischer Wechselwirkungen, Ionenbewegungen und Grenzflächeneffekte manifestiert. Klassische Simulationsmethoden stoßen hier an ihre Grenzen, da sie Wellenfunktionen nicht explizit darstellen können und der Hilbertraum mit der Anzahl der beteiligten Teilchen exponentiell wächst. Quantencomputer hingegen nutzen die natürliche Verschränkung von Qubits, um solche Systeme präzise abzubilden. Dadurch werden Elektronenkorrelationen, Ladungstransferprozesse und die elektronische Struktur von Materialien wie Lithium-Metalloxiden oder Festkörperelektrolyten realistisch modellierbar -- Eigenschaften, die für die Entwicklung leistungsfähiger Batterien von zentraler Bedeutung sind.

\subsubsection*{Vorhersage und Design von Materialeigenschaften}

Quantensimulationen ermöglichen es, Schlüsseleigenschaften von Batteriematerialien präzise vorherzusagen und gezielt zu optimieren. Dazu zählen die Ermittlung der Energiespeicherkapazität durch Berechnung der elektronischen Struktur von Elektrodenmaterialien, die Bestimmung von Spannungsprofilen anhand der Energieänderung beim Ein- und Ausbau von Ionen sowie die Berechnung von Diffusionskoeffizienten zur Analyse der Ionenmobilität in kristallinen oder amorphen Materialien. Auch Redoxpotentiale, die für die Stabilität und Lebensdauer von Batterien entscheidend sind, lassen sich durch quantenmechanische Modellierung von Elektronentransferprozessen an Grenzflächen wie der Fest-Elektrolyt-Phase (SEI) bestimmen. Diese Erkenntnisse sind essenziell für die Entwicklung sowohl von Lithium-Ionen- als auch von Post-Lithium-Batterien, etwa auf Basis von Natrium oder Magnesium.

\subsubsection*{Analyse von Zeitentwicklungen und Dynamiken}

Die Simulation der Zeitentwicklung unter einem Hamilton-Operator stellt klassische Computer vor erhebliche Herausforderungen, insbesondere bei Prozessen wie der Bildung der Fest-Elektrolyt-Grenzphase (SEI), der Ionenmobilität in Festkörperelektrolyten oder dem Bindungsbruch in Elektrodenmaterialien. Quantencomputer sind in der Lage, diese zeitabhängigen Prozesse effizient zu modellieren und so wertvolle Einblicke in die Mechanismen der Batteriealterung, der chemischen Stabilität und der Degradation zu liefern. Atomistische Molekulardynamik (AIMD) auf Quantencomputern könnte Rechenzeiten drastisch reduzieren und realistischere Zeitskalen abdecken, was für die Entwicklung robuster und langlebiger Batterien von großer Bedeutung ist.

\subsubsection*{Entdeckung und Optimierung neuer Batteriematerialien}

Quantencomputing eröffnet neue Möglichkeiten zur systematischen Erkundung und Optimierung neuartiger Materialklassen. So können Post-Lithium-Systeme wie Natrium-, Magnesium- oder Aluminium-Batterien simuliert werden, die aufgrund größerer Ionenradien andere Einlagerungsmechanismen erfordern. Auch Festkörperelektrolyte aus Sulfiden, Oxiden oder Polymeren mit hoher Ionenleitfähigkeit lassen sich auf ihre Stabilität und Kompatibilität mit Elektroden prüfen. Darüber hinaus werden Quantenmaterialien wie topologische Isolatoren oder Supraleiter untersucht, die ungewöhnliche elektronische Eigenschaften aufweisen und für spezielle Anwendungen, etwa als Quantensensoren, interessant sind. Analoge Quantensimulatoren liefern bereits heute Einblicke in Modellklassen, die klassischen Algorithmen verschlossen bleiben, etwa die Langzeitdynamik von Ionen in Defektstrukturen.

\subsubsection*{Optimierung von Fertigungsprozessen}

Die Kombination aus Quantencomputing und maschinellem Lernen revolutioniert die Batterieproduktion. Durch hybride Algorithmen lassen sich Fertigungsparameter wie Temperatur, Druck und Dotierungskonzentrationen bei der Synthese von Elektrodenmaterialien präzise einstellen. Quantenunterstützte Bildanalysen ermöglichen die Detektion von Mikrorissen in Elektrodenbeschichtungen oder Unregelmäßigkeiten in Elektrolytschichten. Zusätzlich werden durch quantenbasierte Stoffdatenbanken nachhaltige und recyclingfähige Materialien sowie energieeffiziente Herstellungsverfahren beschleunigt. Projekte wie \textit{BASIQ} (DLR) und Kooperationen zwischen Industrie und Start-ups, beispielsweise Volkswagen und Xanadu, demonstrieren bereits den erfolgreichen Transfer dieser Technologien in die Praxis.

\subsection{Wirkstoffforschung}

\section{Top Technologien \& Algorithmen}
In der Entwicklung quantenbasierter Verfahren zur Lösung komplexer Probleme haben sich in den letzten Jahren verschiedene vielversprechende Ansätze etabliert. Insbesondere hybride Algorithmen, die Quanten- und klassische Rechenmethoden kombinieren, spielen dabei eine zentrale Rolle. Sie nutzen das Potenzial von Quantencomputern für spezielle Aufgaben, wie die präzise Bestimmung von Energiezuständen in Molekülen oder die effiziente Lösung kombinatorischer Optimierungsprobleme, und ergänzen diese durch bewährte klassische Optimierungsverfahren.
\vspace{1em}
Dieses Kapitel stellt drei zentrale Vertreter solcher Technologien vor: Variational Quantum Algorithms (VQAs), die Quantum Phase Estimation (QPE) sowie den Quantum Approximate Optimization Algorithm (QAOA). Dabei werden sowohl deren Funktionsweise als auch ihre Anwendungsmöglichkeiten in der Chemie und Materialforschung und Grenzen näher beleuchtet.



\subsubsection*{Variational Quantum Algorithms}

Variational Quantum Algorithms (VQAs) sind eine Methode, um mit Quantencomputern die Energiezustände eines physikalischen Systems, beispielsweise eines Moleküls, zu berechnen. Es handelt sich hierbei um ein hybrides Verfahren, bei dem sowohl Quantencomputer als auch herkömmliche Computer genutzt werden, um ein klassisches Optimierungsverfahren auszuführen. \citealp[6]{weidmanQuantumComputingChemistry2024a}

Es gibt dabei zwei Haupttypen von VQAs, zwischen denen zu differenzieren ist: Variational Quantum Simulation (VQS) und Variational Quantum Optimization (VQO). VQS simulieren dynamische Prozesse, indem sie die zeitliche Entwicklung in einem abstrakten Raum möglichst akkurat nachbilden. VQO dagegen streben danach, Zielzustände zu finden, indem sie eine sogenannte Kostenfunktion minimieren. \citealp[23]{mottaEmergingQuantumComputing2022}

Konkret finden VQO Bedeutung in der Chemie unter anderem als Variational Quantum Eigensolver (VQE). Dieser VQE-Algorithmus wurde zur Bestimmung der Grundzustandsenergie elektronischer Moleküle entwickelt. Dafür wird zunächst eine angenommene Form der Elektronenverteilung, der sogenannten Wellenfunktion, für ein Molekül gewählt. Der Quantencomputer berechnet den Erwartungswert der Energie eines Systems für bestimmte Parameter (E(θ)) \citealp[23]{mottaEmergingQuantumComputing2022}, woraufhin der klassische Computer die Parameter dieser Wellenfunktion so anpasst, dass die berechnete Energie immer kleiner wird. Dieser Ablauf wird so lange wiederholt, bis die geringste mögliche Energie ermittelt wird und somit die Grundzustandsenergie des elektronischen Moleküls geschätzt werden kann. \citealp[6]{weidmanQuantumComputingChemistry2024a}
Diese statistische Schätzung ergibt sich als Durchschnittswert mit einer gewissen Streuung. Deshalb müssen die Optimierungsverfahren, die die Parameter schrittweise verbessern, darauf achten, dass diese Messwerte nicht exakt, sondern mit Unsicherheit behaftet sind. \citealp[23]{mottaEmergingQuantumComputing2022}

Dieser VQE-Algorithmus wurde auch auf andere Bereiche ausgeweitet, etwa Lösungsverfahren für lineare Gleichungssysteme, Matrixzerlegung und numerische lineare Algebra. \citealp[6]{weidmanQuantumComputingChemistry2024a}

Bei dem VQE-Verfahren sind die Messfehler statistisch klar abschätzbar, allerdings bleibt dennoch eine gewisse Unsicherheit in der geschätzten elektronischen Energie bestehen. Um diese Fehler zu verringern, müsste der verwendete Ansatz (die Wellenfunktion) verbessert werden. Dafür wären allerdings deutlich mehr Quantenoperationen nötig, als heutige Quantencomputer ohne Fehlerkorrektur ausführen können. Aus diesem Grund sollte alternativ die Quantum Phase Estimation (QPE) betrachtet werden. \citealp[7]{vonburgQuantumComputingEnhanced2021}



\subsubsection*{Quantum Phase Estimation}

Quantum Phase Estimation (QPE) ist ein grundlegender Quantum-Algorithmus, mit dem sich die Eigenenergie eines Quantensystems präzise bestimmen lässt. In der Quantenchemie wurde bereits 2005 mit dem QPE die Energie eines Moleküls in einer Simulation ermittelt. \citealp[5]{weidmanQuantumComputingChemistry2024a} Der Algorithmus kann den möglichst niedrigen Energiezustand direkt identifizieren, wenn ein geeigneter Startzustand verwendet wird. Je öfter die zentrale Quantenoperation wiederholt wird, desto genauer wird das Ergebnis.

Die Durchführung der Methode sieht die Vorbereitung von zwei Qubit-Registern vor, eines für das Molekül (Systemregister) und eines für die Messung (Hilfsregister). Das Hilfsregister wird mithilfe sogenannter Hadamard-Operationen in eine Überlagerung gebracht, sodass es viele mögliche Phasen gleichzeitig abbilden kann. Diese Vorbereitung ermöglicht es, die Energieinformation als Phase im späteren Verlauf präzise auszulesen. \citealp[25]{weidmanQuantumComputingChemistry2024a}, \citealp[29]{mottaEmergingQuantumComputing2022} 

Das Quantensystem wird so weiterentwickelt, dass die Energie als Phase im Hilfsregister gespeichert wird. Dazu sorgt jedes Hilfs-Qubit dafür, dass das System für eine bestimmte Zeit weiterläuft. Wenn das System eine bestimmte Energie hat, sammelt das Hilfsregister bei jeder Zeitentwicklung einen passenden Phasenwert. Um diese im Hilfsregister gespeicherten Phaseninformationen in eine konkrete Energieangabe zu überführen, wird eine inverse Quanten-Fourier-Transformation angewendet. Dieser Schritt sorgt dafür, dass sich die zuvor aufgebauten Phasen zu einem klaren Interferenzmuster überlagern. Dadurch entsteht ein charakteristisches Signal, aus dem die Energie des Systems als Bitfolge abgelesen werden kann. \citealp[7]{vonburgQuantumComputingEnhanced2021}, \citealp[25]{mottaEmergingQuantumComputing2022} Im letzten Schritt wird das Hilfsregister gemessen. Wenn der Systemzustand zu Beginn bereits ein reiner Eigenzustand des Hamiltonoperators ist, liefert die Messung exakt die zugehörige Energie – ohne statistische Schwankungen. Dies ist die sogenannte Zero-Variance-Eigenschaft. Wenn der Startzustand jedoch eine Mischung verschiedener Eigenzustände war, projiziert die Messung das System zufällig auf einen dieser Zustände. Die Wahrscheinlichkeit, mit der ein bestimmter Energiewert dabei herauskommt, ist davon abhängig, wie stark der Anfangszustand mit dem jeweiligen Eigenzustand übereinstimmt. Wird dieser Vorgang mehrfach wiederholt oder der Startzustand gezielt gewählt, so kann die gewünschte Energie zuverlässig ermittelt werden. \citealp[7]{vonburgQuantumComputingEnhanced2021},\citealp[25]{mottaEmergingQuantumComputing2022}

Ein Nachteil ist allerdings, dass QPE eine deutlich komplexere und längere Quantenberechnung erfordert. Deshalb ist sie auf zukünftige, fehlerkorrigierte Quantencomputer angewiesen. Doch im Gegensatz zu Methoden wie VQE, bei denen immer eine gewisse Unsicherheit bleibt, liefert QPE exakte und kontrollierbare Ergebnisse, sobald die Hardware leistungsfähig genug ist. \citealp[7]{vonburgQuantumComputingEnhanced2021},\citealp[25]{mottaEmergingQuantumComputing2022}



\subsubsection*{Quantum Approximate Optimization Algorithm}

Neben diesen Verfahren zur präzisen oder variativen Bestimmung von Energiezuständen gibt es weitere hybride Algorithmen, die speziell für Optimierungsaufgaben konzipiert wurden. Ein besonders vielversprechender Vertreter ist der Quantum Approximate Optimization Algorithm (QAOA), der sich für kombinatorische Probleme eignet und zunehmend auch in der Chemie und Materialforschung Anwendung findet.

QAOA gehört zur Familie dieser hybriden Variationsverfahren und wurde speziell für kombinatorische Optimierungsprobleme entwickelt, wodurch die optimalen Lösungen schneller ermittelt werden, als es durch brute force möglich wäre. \citealp[3]{guoHarnessingQuantumPower2024} Das Problem wird durch einen sogenannten Kosten-Hamiltonian beschrieben. Dies ist ein Operator, der jeder möglichen Lösung einen Energiezustand zuweist, wobei die Lösung dem niedrigsten Energiezustand entspricht. \citealp[24]{mottaEmergingQuantumComputing2022}, \citealp[8]{guoHarnessingQuantumPower2024} Ergänzt wird dieser durch einen sogenannten Mixer-Hamiltonian, der das System kontrolliert aus dem Startzustand in verschiedene Konfigurationen lenkt. Der Algorithmus führt anschließend abwechselnd beide Operatoren für bestimmte Zeitspannen aus, wobei die Parameter dieser Zeitspannen durch klassische Optimierung angepasst werden. Diese basieren auf wiederholten Messungen und der Bewertung der Ergebnisqualität. \citealp[8]{guoHarnessingQuantumPower2024}
Diese Kombination von quantenmechanischer Zustandserzeugung und klassischer Rückkopplung ermöglicht es, gute Lösungen effizient zu finden, ohne den gesamten Lösungsraum durchsuchen zu müssen. Mit steigender Anzahl der Schichten (p-Level) im Raum wird die Lösungsqualität besser, allerdings steigt hiermit auch die Anforderung an die Hardware.

In der Materialwissenschaft wurde QAOA bereits in Simulationen zur Gestaltung von Metamaterialien eingesetzt. Dabei wurde ein Quantenannealing-Modell genutzt, um transparente Kühlbeschichtungen mit optimierter Energieeffizienz zu entwerfen. \citealp[18]{guoHarnessingQuantumPower2024} Währenddessen zeigen sich in der Chemie Anwendungsmöglichkeiten in der molekularen Wirkstoffsuche, wie durch Optimierung molekularer Strukturen im Hinblick auf ihre Bindungsstärke. Guo et al. (2024) demonstrierten dies mit einem quantengestützten Verfahren, das verschiedene zweiatomige Moleküle auf ihre Bindungsstärke an eine Protein-Tasche untersuchte. Die aktuell erhältliche Hardware ist allerdings begrenzt und schränkt die Anwendungsmöglichkeiten ein. Herausforderungen wie Rauschunempfindlichkeit und das Barren-Plateau-Problem müssen jedoch noch überwunden werden. \citealp[12]{weidmanQuantumComputingChemistry2024a}


\section{Top 3 Zukunftsprojekte \& Forschungsinitiativen}

In diesem Kapitel werden drei herausragende Initiativen vorgestellt, die maßgeblich zur Weiterentwicklung des Quantencomputings beitragen. Sie stehen exemplarisch für unterschiedliche institutionelle und strategische Ansätze, mit denen Wissenschaft, Industrie und Politik den Weg zur praktischen Nutzung der Quanteninformationstechnologie gestalten. 

\subsubsection*{PASQuanS2 – EU-Flaggschiff-Initiative}

PASQuanS2 – EU-Flaggschiff-Initiative für Quantum Simulation
Das europäische „Quantum Flagship“ ist eine der drei großen Forschungsinitiativen der EU und verfolgt das Ziel, Europa als zentrale Kraft in der zweiten Quantenrevolution zu etablieren und eine wettbewerbsfähige Quantenindustrie aufzubauen. \citealp[1]{} \cite{}(Riedel et al., 2019), S. 1) Mit einem Budget von rund einer Milliarde Euro über zehn Jahre konzentriert sich das Programm auf vier zentrale Technologiefelder: Quantenkommunikation, -computing, -simulation sowie -sensorik und -metrologie. \cite{}(Riedel et al., 2017), S. 3)
Ein koordiniertes europäisches Vorgehen wurde notwendig, da es an standardisierten Schnittstellen, klaren Verwertungsstrategien und langfristigen industriellen Partnerschaften mangelte. \cite{}((Riedel et al., 2019), S. 2; \citealp[2]{rasanenPathEuropeanQuantum2021} Einen entscheidenden politischen Impuls lieferte das 2016 veröffentlichte „Quantum Manifesto“, das die Quantenforschung als strategische Schlüsseltechnologie zur Sicherung digitaler Souveränität definierte. \citealp[1]{vandeventerEuropeanStandardsQuantum2022} Um die Interoperabilität der Systeme und deren Skalierbarkeit zu gewährleisten, gilt die Entwicklung gemeinsamer technischer Standards als entscheidender Schritt. Auch wenn laut \citealp[2]{vandeventerEuropeanStandardsQuantum2022} bislang noch keine übergreifende Standardisierungsstrategie existiert, arbeiten Initiativen wie das Quantum Industry Consortium (QuIC) sowie mehrere Arbeitsgruppen des Quantum Flagship aktiv daran, regulatorische und technische Rahmenbedingungen europaweit zu harmonisieren. Auf dieser Grundlage entstand eine strategische Forschungsagenda, die technologische Meilensteine definiert und sowohl Grundlagenforschung als auch industriellen Transfer miteinander verzahnt. \cite{}((Riedel et al., 2019), S. 3) Im Folgenden werden zentrale technische Umsetzungsansätze und konkrete Forschungsprojekte innerhalb des Quantum Flagship näher beleuchtet.

Ein herausragendes Beispiel dafür ist das Projekt PASQuanS2 (Programmable Atomic Large-Scale Quantum Simulation), das im Rahmen des Flagships finanziert wird. Ziel von PASQuanS2 ist die Entwicklung hochskalierbarer, programmierbarer Quantensimulatoren auf Basis ultrakalter Atome und Ionen, um realitätsnahe Vielteilchenprobleme effizient berechnen zu können. \citealp[6]{rasanenPathEuropeanQuantum2021} Die aktuelle Projektphase PASQuanS2.1 (2023–2026) fokussiert auf Plattformen mit mindestens 2.000 steuerbaren Quantenteilchen und legt damit die Grundlage für spätere Systeme mit bis zu 10.000 Teilchen. 

PASQuanS2 stützt sich technologisch auf drei sich ergänzende Plattformstrategien, die jeweils unterschiedliche physikalische Prinzipien nutzen. Die erste basiert auf Neutralatom-Arrays in optischen Gittern, bei denen ultrakalte Atome mithilfe von Lasern in regelmäßigen Strukturen angeordnet werden. Die zweite Plattform nutzt sogenannte optische Pinzetten (Tweezers-Systeme), in denen einzelne Atome präzise fixiert und über kontrollierte Rydberg-Wechselwirkungen miteinander gekoppelt werden. Die dritte Strategie setzt auf Ionenfallenarchitekturen, bei denen geladene Teilchen in elektromagnetischen Feldern eingeschlossen und durch kollektive Schwingungen miteinander verbunden werden. Jede dieser Technologien bringt unterschiedliche Stärken hinsichtlich Skalierbarkeit, Stabilität und Kopplungspräzision mit sich, verfolgt jedoch das gemeinsame Ziel, quantenmechanische Vielteilchendynamiken experimentell zugänglich zu machen. \cite{}(PASQuanS2 - Transforming the Landscape for Programmable Quantum Simulation in Europe | PASQuanS 2, o. J.) Ein zentraler technischer Fokus liegt auf der Erhöhung der Zustandsfidelity auf unter \(0{,}1\,\%\), der Verbesserung von Lese- und Schreibprozessen sowie der Entwicklung verlässlicher Messprotokolle wie Randomized Benchmarking oder Hamiltonian Learning zur Validierung der Simulationsergebnisse. Gleichzeitig wird an modularen Softwareschnittstellen gearbeitet, um eine einfache Integration der Hardware in bestehende IT-Infrastrukturen und Cloud-Umgebungen zu ermöglichen. Dadurch sollen industrielle und wissenschaftliche Anwendergruppen ohne tiefergehende Hardwarekenntnisse Zugang zu den Quantensimulatoren erhalten – ein entscheidender Schritt in Richtung praktischer Nutzbarkeit. \citealp[6]{rasanenPathEuropeanQuantum2021}

Das Forschungsprojekt PASQuanS2 verdeutlicht exemplarisch, wie das europäische Quantum Flagship wissenschaftlichen Fortschritt gezielt mit technologischer Skalierbarkeit und wirtschaftlicher Verwertbarkeit zu verknüpfen versucht. Es verkörpert den strategischen Anspruch Europas, sich durch ein föderiertes, konsensorientiertes Innovationsmodell gegenüber international stark subventionierten Programmen in den USA und China langfristig wettbewerbsfähig aufzustellen. \citealp[7]{vogiatzoglouEUsQuestDigital2025}
 

\subsubsection*{IBM Quantum & Qiskit Nature} 

IBM (International Business Machines Corporation) zählt zu den traditionsreichsten Technologieunternehmen weltweit und hat die Entwicklung der Informatik über Jahrzehnte maßgeblich mitgestaltet, beispielsweise durch den Mainframe, relationale Datenbanken oder IBM Watson \cite{aruteQuantumSupremacyUsing2019a}. Nun positioniert sich IBM als treibende Kraft im Quantencomputing. Mit der Initiative IBM Quantum verfolgt das Unternehmen das Ziel, skalierbare Quantenhardware zu entwickeln, über die Cloud zugänglich zu machen und durch das Open-Source-Framework Qiskit auch für Forschung und Industrie nutzbar zu gestalten. \cite{} (Akash & Jamema, 2025 \cite{miceliQuantumComputationVisualization2018} 

IBM Quantum bezeichnet IBMs industrielle Quanten-Computing-Initiative, die sowohl modernste Quantenhardware als auch Cloud-Zugriff für Forschung und Industrie bereitstellt. Bereits 2016 stellte IBM den ersten frei zugänglichen Quantencomputer über die IBM Quantum Experience in der Cloud zur Verfügung. Dies bildete die Grundlage für eine breit angelegte Forschungsinfrastruktur. Dies zeigt sich zu Beginn des Jahres 2024, wobei mehr als drei Billionen Quantenoperationen über IBMs Quanten-Cloud durchgeführt und rund 2.800 wissenschaftliche Arbeiten unter Verwendung von IBM-Quantenprozessoren veröffentlicht wurden. IBM hat sich damit als führender Akteur etabliert und adressiert ein breites Anwendungsspektrum von quantenchemischen Simulationen und Optimierungsproblemen bis hin zu Kryptographie und Machine Learning. Gleichzeitig treibt IBM die Hardware-Entwicklung voran. Dies zeigt sich 2023 mit der Vorstellung von „Condor“, ein supraleitender Quantenchip mit 1.121 Qubits, nachdem bereits 2021/22 die Prozessoren „Eagle“ (127 Qubits) und „Osprey“ (433 Qubits) technische Meilensteine gesetzt hatten. Diese enge Verzahnung von Forschungs-Initiative und zugänglicher Infrastruktur macht IBM Quantum zu einer zentralen Plattform für Quantencomputing-Fortschritte. \citealp[2]{abughanemIBMQuantumComputers2025}

Ein essenzieller Bestandteil von IBM Quantum ist das Open-Source-Software-Framework Qiskit, das 2017 von IBM veröffentlicht wurde. Qiskit stellt ein umfangreiches Software Development Kit (SDK) für Quantencomputing bereit und hat sich zum meistgenutzten Werkzeug dieser Art entwickelt. Es bietet eine benutzerfreundliche API zur Konstruktion von Quanten-Schaltkreisen und automatisiert die nötigen Schritte, um diese auf echter Quantenhardware auszuführen. Konkret definiert das Qiskit-„Patterns“-Modell einen strukturierten Workflow in vier Schritten. Zuallererst wird das ursprüngliche Problem in einen Quanten-Schaltkreis übersetzt. Anschließend wird diese Schaltung mittels Transpilation optimiert und an die Ziel-Hardware angepasst. Folgend wird der Schaltkreis auf einem ausgewählten Quantenprozessor (oder Simulator) ausgeführt und abschließend werden die Resultate ausgelesen und klassisch nachverarbeitet. \citealp[7]{abughanemIBMQuantumComputers2025} Dieses Cloud-basierte Toolchain-Konzept ermöglicht es Nutzern, Quantenprogramme nahtlos vom Laptop bis auf die supraleitenden IBM-Quanten-Prozessoren zu bringen. Über Qiskit können Berechnungen entweder auf hochleistungsfähigen Simulatoren oder direkt auf den realen IBM-Quantencomputern in der Cloud durchgeführt werden. IBM stellt dafür mehrere Quantenprozessoren mit über IBM Quantum Experience frei zur Verfügung, zugänglich über ein webbasiertes Interface, welches insbesondere den Einstieg für eine breite Nutzerschaft erleichtert. \citealp[5]{wangAdvantagesTwoQuantum2025} Die robuste Hardware in Verbindung mit einer nahtlos integrierten Software-Umgebung hat wesentlich dazu beigetragen, dass bereits Millionen von Experimenten mithilfe von Qiskit auf IBM-Systemen durchgeführt wurden. Damit zählt IBM zu den zentralen Akteuren in der praktischen Umsetzung und Skalierung von Quantencomputing-Anwendungen weltweit.

Der hohe Praxisbezug von IBM Quantum zeigt sich insbesondere in der gezielten Anwendung wissenschaftlicher Algorithmen auf reale Problemstellungen. Ein zentrales Anwendungsfeld ist dabei die Quantenchemie, in der klassische Rechner schnell an ihre Grenzen stoßen. Hier kommen hybride Quantenalgorithmen wie der Variational Quantum Eigensolver zum Einsatz, der bereits im vorherigen Unterkapitel ausführlich behandelt wurde. \citealp[5]{miceliQuantumComputationVisualization2018} IBM’s Software-“Stack” unterstützt solche hybriden Ansätze mit spezialisierten Bibliotheken, wie Qiskit Nature (früher als Qiskit Chemistry bekannt). Dies ist ein Open-Source-Framework, das quantenmechanische Probleme aus den Naturwissenschaften, insbesondere in der Chemie, im Computer abbildet. Qiskit Nature stellt Funktionen bereit, um elektronische Strukturen von Molekülen in Form der Zweiten Quantisierung zu formulieren und an Quantenalgorithmen zu koppeln. So können klassische Quantenchemie-Programme für Vorabrechnungen integriert werden. Beispielsweise lässt sich ein Hartree-Fock-Grundzustand mit etablierten Programmpaketen berechnen und an Qiskit übergeben. Darauf aufbauend kommen quantum-classical Algorithmen wie VQE zum Einsatz, um die Korrektur zur Hartree-Fock-Lösung zu bestimmen. Typischerweise wird dabei ein Unitary Coupled Cluster-Ansatz mit Einfach- und Doppelanregungen (UCCSD) verwendet, der vom Hartree-Fock-Zustand ausgeht. \citealp[2]{avramidisGroundStateProperty2024}

Die Leistungsfähigkeit und Herausforderungen dieses Ansatzes werden in aktuellen Forschungsarbeiten deutlich. So wurden 2024 erstmals die Grundzustands-Eigenschaften mehrerer Lithiumhydrid-Komplexe (LiH, LiH2, LiH3 samt ihren Ionen) vollständig mit Qiskit und dem VQE-Algorithmus simuliert. Dabei diente ein UCCSD-Ansatz als quantenchemische Wavefunction, und es wurden neben den Grundzustandsenergien auch Elektronenaffinitäten, Ionisierungsenergien und Dipolmomente der Moleküle berechnet. Die Ergebnisse zeigen, dass VQE in der Lage ist, die elektronischen Grundzustandsgrößen dieser einfachen Moleküle mit hoher Genauigkeit zu reproduzieren. \citealp[4]{avramidisGroundStateProperty2024}

Zusammenfassend bildet IBM Quantum mit seinem ganzheitlichen Ansatz, von der leistungsfähigen Hardware über das umfangreiche Qiskit-Softwarepaket bis hin zu spezialisierten Modulen wie Qiskit Nature, eine zukunftsweisende Forschungsplattform. Als eines der “Top Zukunftsprojekte” treibt IBM Quantum damit sowohl die Weiterentwicklung der Quantencomputing-Technologie als auch deren Anwendung in wissenschaftlichen und industriellen Domänen maßgeblich voran.

\subsubsection*{Google Quantum AI & Sycamore} 

Googles Quantenforschungseinheit, Quantum AI, ist eine ambitionierte privatwirtschaftliche Initiative im Bereich der Quanteninformationsverarbeitung.  Eingebettet in die Alphabet-Struktur verfolgt Google einen Full-Stack-Ansatz. Dabei werden alle Ebenen der Quantencomputer-Architektur intern entwickelt. Dies umfasst die Hardware, darunter supraleitende Qubits und Quantenchips. Auch die Steuer- und Ausleseelektronik wird intern entwickelt. Hinzu kommen Softwarekomponenten wie Compiler, Fehlerkorrekturverfahren und das Open-Source-Framework Cirq, mit dem sich Quantenalgorithmen programmieren lassen. Im Mittelpunkt steht der 2019 vorgestellte Sycamore-Prozessor, ein supraleitender Quantenchip mit 53 funktionsfähigen Transmon-Qubits, der auf einem zweidimensionalen Gitter basiert. \cite{aruteQuantumSupremacyUsing2019a}

Der zentrale Durchbruch gelingt Google mit der Demonstration der Quantenüberlegenheit (quantum supremacy). Es ist definiert als die Fähigkeit eines Quantencomputers, eine spezifische Rechenaufgabe schneller zu lösen als jeder bekannte klassische Computer. Konkret ließ Google seinen Sycamore-Prozessor eine Zufalls-Sampling-Aufgabe ausführen, bei der bitstring-Ausgaben aus tiefen, pseudozufälligen Quanten-Schaltkreisen generiert wurden. Diese Aufgabe wurde in 200 Sekunden bewältigt. Im Vergleich dazu hätte ein klassischer Supercomputer wie Summit für dieselbe Aufgabe laut Google rund 10.000 Jahre benötigt. \cite{aruteQuantumSupremacyUsing2019a}
Dieses Ergebnis wurde über die sogenannte Cross-Entropy Benchmarking (XEB)-Fidelity mit einem Wert von ca. 0.002 validiert. \cite{maksudul_shadat_akash_quantum_2025} Dies weist darauf hin, dass die gemessenen Ausgaben signifikant von gleichverteilten Zufallswerten abweichen und somit mit den theoretisch erwarteten Quantenverteilungen übereinstimmen.

IBM, als direkter Wettbewerber im Bereich supraleitender Qubits, widersprach dieser Interpretation und argumentierte, dass sich dieselbe Sampling-Aufgabe durch Optimierungen klassischer Algorithmen und Einsatz von Massenspeicher innerhalb von 2,5 Tagen auf klassischen Maschinen lösen ließe. \cite{pednault_quantum_2019} Diese Diskussion verursachte nicht nur technische, sondern auch begriffliche Spannungen, wobei IBM den Begriff „Quantenüberlegenheit“ als irreführend kritisiert und stattdessen quantum advantage als neutraleren Ausdruck vorschlug (IBM Quantum Blog, 2019). Ungeachtet der Debatte bleibt Sycamore ein technologischer Meilenstein. Die auf dem Chip eingesetzten sogenannten Sycamore-Gates sind eine spezielle Form von fSim-Gates. Das ist eine Kombination aus rotationssymmetrischen iSWAP-Operationen und kontrollierten Phasenverschiebungen, die hohe Verschränkung bei gleichzeitig reduzierter Fehlerrate ermöglichen. \cite{abughanemPhotonicQuantumComputers2024} Die durchschnittliche Die Fehlerquote beträgt bei simultaner Ausführung \(0{,}93\,\%\) für Zwei-Qubit-Gates und \(3{,}8\,\%\) bei der Auslesung. \cite{arute_quantum_2019}

Im Anschluss an das Supremacy-Experiment rückten Forschungsfragen zur Fehlertoleranz, Skalierbarkeit und plattformübergreifenden Vergleichbarkeit in den Vordergrund. Einen bemerkenswerten Beitrag dazu leistet die Studie von AbuGhanem & Eleuch (2024), die eine vollständige Quantenprozess- und Zustandstomographie des Sycamore-Gates auf IBMs Quantencomputern durchführten. \cite{abughanemPhotonicQuantumComputers2024} Die Autoren untersuchten dabei nicht nur Simulationen unter idealen und verrauschten Bedingungen, sondern führten die Messungen auch auf echter IBM-Quantenhardware durch. Die Ergebnisse zeigten auf IBMs Plattform zeigte eine Prozessfidelity von \(81{,}02\,\%\). Dieser Wert liegt nahe an dem, was unter realen Bedingungen auf aktueller Hardware technisch erreichbar ist, und zeigt, dass Sycamore-Gates auch auf einer konkurrierenden Architektur wie der von IBM mit hoher Qualität implementiert werden können. \cite{abughanem_full_2025}
Solche herstellerübergreifenden Analysen sind wegweisend: Sie ermöglichen nicht nur Benchmarks für Gate-Qualitäten unter realistischen Bedingungen, sondern fördern auch eine Standardisierung der Metriken zur Bewertung von Quantencomputern. \cite{abughanemPhotonicQuantumComputers2024}

Die Kombination aus hoher Gate-Qualität, tiefer Architekturkenntnis und öffentlich zugänglicher Software macht Google Quantum AI zu einem der Treiber des Fortschritts auf dem Weg zur praktischen Nutzung von Quantencomputern. Langfristig fokussiert sich Google auf die Umsetzung eines fehlertoleranten, skalierbaren Quantencomputers mit Anwendungen in Chemie, Optimierung und KI \cite{}. (Butt et al., 2024).
Damit steht Google Quantum AI mit Sycamore exemplarisch für den Übergang von wissenschaftlichen Demonstrationen hin zu strategisch nutzbaren Quantentechnologien. Dies hat erheblichen Implikationen für Forschung, Industrie und sicherheitskritische IT-Systeme.


\section{Bewertung anhand der Kriterien}

\subsection{Vergleichende Bewertung der Anwendungsfelder}

Quantencomputer eröffnen neue Perspektiven für zentrale Fragestellungen in der Chemie und Materialwissenschaft. Die nachfolgende vergleichende Bewertung konzentriert sich auf drei Schlüsselanwendungsfelder: (1) die \textit{Simulation von Molekülen}, (2) die \textit{Materialforschung am Beispiel Batterien} und (3) die \textit{Modellierung von Elektronenkorrelationen und Defekten in Festkörpern}.

Diese Gegenüberstellung erfolgt anhand von fünf übergeordneten Bewertungskriterien: Technologischer Reifegrad, Marktrelevanz, Gesellschaftlicher Nutzen, Forschungspotenzial sowie Risiken und ethische Implikationen. Diese wurden im Kapitel 4.6 Bewertung anhand der Kriterien bereits näher erläutert.

Ziel ist es, die unterschiedlichen Entwicklungsstände, Potenziale und Herausforderungen dieser Anwendungsbereiche systematisch einzuordnen – in direkter Anlehnung an die Methodik des vorangegangenen Kapitels zum Finanzwesen. Die folgende Tabelle fasst die Bewertung der drei Felder übersichtlich zusammen.

\begin{table}[h]
\centering
\begin{tabular}{|p{0.25\linewidth}|p{0.23\linewidth}|p{0.23\linewidth}|p{0.23\linewidth}|}
\hline
\textbf{Kriterium} & \textbf{Simulation von Molekülen} & \textbf{Materialforschung (Batterien)} & \textbf{Festkörper: Korrelation \& Defekte} \\
\hline
\textbf{Techno\-logischer Reife\-grad} & mittel & niedrig & niedrig \\
\hline
\textbf{Markt\-relevanz} & hoch & sehr hoch & hoch \\
\hline
\textbf{Gesell\-schaft\-licher Nutzen} & mittel & hoch & hoch \\
\hline
\textbf{Forschungs\-potenzial} & sehr hoch & sehr hoch & sehr hoch \\
\hline
\textbf{Risiken und Ethik} & gering & mittel & mittel--hoch \\
\hline
\end{tabular}
\caption{Bewertung der drei Hauptanwendungsfelder von Quantencomputing in der Chemie und Materialwissenschaft}
\label{tab:qc-chemie-bewertung}
\end{table}



\subsubsection{Technologischer Reifegrad}

Die \textit{Simulation von Molekülen} weist derzeit den höchsten technologischen Reifegrad unter den betrachteten Feldern auf. Erste experimentelle Umsetzungen des VQE wurden bereits auf realer Quantenhardware wie den IBM Q-Quantenprozessoren durchgeführt. \cite{kandala_hardware-efficient_2017} Auch die Implementierung von Quantum Generative Adversarial Networks zur Wahrscheinlichkeitsverteilung von Molekülzuständen wurde auf Simulatoren und kleinen Quantenprozessoren gezeigt. \cite{zoufal_quantum_2019} Die theoretischen Grundlagen sind gut etabliert und wurden umfassend dargestellt, unter anderem von McArdle et al. \cite{mcardle_quantum_2020}. Dennoch bestehen Einschränkungen bei der Skalierung auf größere Systeme, insbesondere aufgrund der notwendigen Fehlerkorrektur und Qubitanzahl. Insgesamt ergibt sich daher ein \textbf{mittlerer Reifegrad}.

\vspace{0.5em}

Die \textit{Materialforschung im Bereich Batterien} befindet sich in einem früheren Stadium. Während Konzepte zur quantenmechanischen Berechnung von Zellspannungen und Ionenmobilität vorliegen \cite{urban_computational_2016}, existieren bisher keine Demonstrationen dieser Verfahren auf realer Quantenhardware. Auch Demir et al. beschreiben die potenzielle Anwendung, betonen aber, dass die derzeitige Forschung überwiegend konzeptionell ist. \cite{demirApplicationQuantumComputing2024} Damit ist das Feld technologisch \textbf{niedrig entwickelt}.

\vspace{0.5em}

Auch die \textit{Modellierung elektronischer Korrelationen und Defekte} weist derzeit einen \textbf{niedrigen Reifegrad} auf. Zwar wurde in Pilotstudien der Einsatz von Quantencomputern zur Simulation von Farbzentren in Festkörpern wie Diamant oder Siliziumkarbid untersucht \cite{cao_ab_2023}, jedoch bleibt die Modellierung stark korrelierter Systeme auf praktische Anwendungen mit wenigen Qubits beschränkt. Daley et al. zeigen zwar experimentelle Fortschritte im Bereich analoger Quantensimulationen, doch digitale Anwendungen in realistischen Materialszenarien sind bislang limitiert. \cite{daley_practical_2022}


\subsubsection{Marktrelevanz}

Die \textit{Simulation von Molekülen} besitzt ebenfalls eine \textbf{hohe wirtschaftliche Bedeutung}, vor allem in der chemischen und pharmazeutischen Industrie. Eine präzisere Modellierung von Reaktionsmechanismen kann Entwicklungszeiten senken und gezieltere Designs neuer Wirkstoffe oder Katalysatoren ermöglichen. \cite{mcardle_quantum_2020} Auch Anwendungen in der organischen Halbleiterentwicklung oder der Polymerchemie sind relevant, jedoch weniger breit in der Industrie verankert als die Batterieforschung.

\vspace{0.5em}

Die \textit{Materialforschung im Kontext von Batterien} besitzt die \textbf{höchste Marktrelevanz} der drei Felder. Fortschritte in der Simulation von Batteriekomponenten haben unmittelbare ökonomische Auswirkungen auf Elektromobilität, erneuerbare Energiesysteme und stationäre Energiespeicher. Der globale Markt für Lithium-Ionen- und Nachfolgetechnologien wächst rasant, und Simulationsmethoden gelten als strategisches Werkzeug zur Materialentwicklung. \cite{demirApplicationQuantumComputing2024} Urban et al. betonen die Rolle rechnergestützter Vorhersagen zur Optimierung der Energie- und Leistungsdichte künftiger Batteriesysteme. \cite{urban_computational_2016}

\vspace{0.5em}

Die \textit{Festkörper-Defektmodellierung} weist eine eher \textbf{spezialisierte Marktrelevanz} auf. Sie betrifft insbesondere die Halbleiterindustrie, optoelektronische Bauteile und Quanteninformationssysteme. \cite{bassett_quantum_2019} Zwar sind Defekte entscheidend für die Leistungsfähigkeit moderner Bauelemente, doch beschränkt sich der direkte Markteinfluss auf Nischenanwendungen mit hohem technologischem Anspruch. Damit ist die wirtschaftliche Hebelwirkung insgesamt geringer als bei den beiden anderen Feldern.



\subsubsection{Gesellschaftlicher Nutzen}

Die \textit{Molekülsimulation} bringt einen \textbf{mittleren gesellschaftlichen Nutzen}. Eine präzisere Vorhersage von Reaktionen kann zur Entwicklung umweltfreundlicherer chemischer Verfahren und neuer Medikamente beitragen. Der Beitrag ist real, aber schwerer quantifizierbar und eher indirekt in seiner gesellschaftlichen Wirkung. \cite{mcardle_quantum_2020}

\vspace{0.5em}

Die \textit{Batterieforschung} hat den \textbf{größten gesellschaftlichen Nutzen}. Fortschritte bei der Simulation ermöglichen die Entwicklung sicherer, langlebiger und nachhaltiger Speichertechnologien. Dies ist essenziell für die Elektrifizierung des Verkehrs, die Integration erneuerbarer Energien und damit für das Erreichen internationaler Klimaziele. \cite{demirApplicationQuantumComputing2024} Der Nutzen ist somit direkt, breit wirksam und politisch hoch priorisiert.

\vspace{0.5em}

Auch die \textit{Modellierung von Defekten und Korrelationen in Festkörpern} leistet einen \textbf{hohen gesellschaftlichen Beitrag}. Anwendungen reichen von verbesserter Effizienz in Solarzellen bis hin zu stabileren und sichereren Mikroprozessoren. Zudem betrifft das Feld Quantenkommunikation und Quantenkryptographie, die langfristig zur Datensicherheit und digitalen Souveränität beitragen können. \cite{cao_ab_2023,bassett_quantum_2019}


\subsubsection{Forschungspotenzial}

Alle drei Anwendungsfelder weisen ein \textbf{sehr hohes Forschungspotenzial} auf.

\vspace{0.5em}

Die \textit{Molekülsimulation} ist eines der historisch ersten und methodisch vielfältigsten Einsatzfelder des Quantencomputings. McArdle et al. zeigen eine breite Forschungslandschaft rund um VQE, QPE und hybride Verfahren. \cite{mcardle_quantum_2020} Auch die Anwendung generativer Netze (qGANs) zur Modellierung komplexer Zustände wird aktiv untersucht. \cite{zoufal_quantum_2019}

\vspace{0.5em}

Die \textit{Batteriematerialforschung} weist ebenso ein \textbf{enormes Innovationspotenzial} auf. Die Integration von quantenchemischer Simulation, Materials Design und datenbankgetriebener Suche ist methodisch komplex und hochgradig interdisziplinär. \cite{urban_computational_2016} Die Forschung steht noch am Anfang, bietet jedoch viele offene Fragestellungen zu Struktur–Eigenschafts-Beziehungen, Ionenmigration und Grenzflächendynamik und weist dadurch ein hohes Forschungspotenzial auf.

\vspace{0.5em}

Auch das \textit{Feld der korrelierten Festkörper und Defekte} ist methodisch extrem fordernd. Daley et al. sprechen explizit von einem ``practical quantum advantage'' \cite{daley_practical_2022} in der Simulation solcher Systeme. Die Verbindung von Spinphysik, Vielteilchentheorie und \textit{ab-initio}-Materialsimulation ist komplex und birgt langfristig neue Konzepte für Materialdesign und Quantentechnologien. \cite{bassett_quantum_2019}


\subsubsection{Risiken und ethische Implikationen}

Die \textit{Molekülsimulation} birgt vergleichsweise \textbf{geringe Risiken}. Fehlerhafte Simulationen können zu ineffizienten Designs oder längeren Entwicklungszeiten führen, betreffen jedoch keine kritischen Infrastrukturen. Zudem sind die meisten Simulationen mit klassischen Methoden überprüfbar. \cite{mcardle_quantum_2020}

\vspace{0.5em}

Die \textit{Batterieforschung} ist mit \textbf{mittleren Risiken} verbunden. Falsch modellierte Stabilitäts- oder Reaktivitätseigenschaften könnten zu sicherheitskritischen Fehlfunktionen führen (z.\,B.\ thermischer Durchgang, Elektrolytdegradation). Auch Fragen zur Ressourcenverteilung und Rohstoffabhängigkeit können ethisch relevant sein. \cite{demirApplicationQuantumComputing2024}

\vspace{0.5em}

Die \textit{Festkörper-Defektmodellierung} ist am ehesten mit \textbf{mittelhohen Risiken} behaftet. Defekte in Halbleitern oder Quantenmaterialien wirken sich direkt auf die Funktion von Speicherchips, Sensoren und Quantenkommunikation aus. Fehlerhafte Simulationen in sicherheitskritischen Systemen könnten zu Fehlfunktionen führen, zudem ist die Nachvollziehbarkeit quantenmechanischer Multizustandsmodelle begrenzt. \cite{orus_quantum_2019,freysoldt_first-principles_2014}


\section{Teilfazit}


\printbibliography
