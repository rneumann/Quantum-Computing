%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Fehlerkorrektur}
\label{error_correction} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\chapterauthor{Niklas Bodfeld, Tim Boschert, Manuel Meixner, Ann-Kathrin Wenzel}

\abstract{some abstract}

\section{Fehlertypen in Quantenrechnern}\label{chap:QEC1}

\subsection{Die Herausforderung von Fehlern in Quantencomputern}
Quantencomputer gelten als vielversprechende Technologie der Zukunft, da sie bestimmte Probleme deutlich effizienter lösen können als klassische Rechner. Ihre praktische Realisierung steht jedoch vor einer grundlegenden Hürde. Die Fehler. Diese treten in Quantensystemen häufiger auf als in klassischen Rechnern. Zudem sind sie viel schwerer zu kontrollieren.

Während klassische Systeme mit extrem niedrigen Fehlerraten arbeiten (unter 10⁻¹⁷), liegen diese bei heutigen Quantenprozessoren um Größenordnungen zwischen 10⁻³ und 10⁻¹. Der Grund dafür liegt unteranderem in der hohen Empfindlichkeit von Qubits gegenüber Umwelteinflüssen wie elektromagnetischen Feldern, Temperaturfluktuationen oder Strahlungen. Hinzu kommen Fehler durch ungenaue Hardware, Crosstalk zwischen benachbarten Qubits und unzuverlässige Mess- oder Initialisierungsprozessen.

Die Auswirkungen dieser Fehler sind tiefgreifend. Schon ein einzelner Fehler kann sich durch Verschränkung auf das gesamte System ausbreiten. Besonders kritisch sind Phasenfehler, da sie die Interferenzmuster zerstören können, auf denen viele Quantenalgorithmen beruhen.

Deshalb ist die Entwicklung effektiver Fehlerkorrekturmethoden zentral für die Zukunft des Quantencomputings. Nur wenn solche Mechanismen greifen, kann der theoretische Nutzen in der Praxis ausgeschöpft werden.


\subsection{Physikalische Fehlerursachen in Quantenrechnern}
Quantencomputer arbeiten nicht mit Bits sondern mit Qubits, die sich gleichzeitig in mehreren Zuständen befinden können. Dies ist ein Phänomen, das man als Superposition bezeichnet. Diese Superpositionen und die damit verbundenen quantenmechanischen Effekte machen Quantencomputer so deutlich leistungsstärker als normale Rechner. Gleichzeitig machen sie die Systeme aber auch äußerst empfindlich gegenüber äußeren Einflüssen.

Der bedeutendste physikalische Fehlermechanismus ist dabei die sogenannte Dekohärenz. Diese wird so beschreiben, dass in dem ein Qubit durch den Kontakt mit seiner Umgebung „gestört“ wird. Dabei verliert das Qubit seine Fähigkeit, in einem Überlagerungszustand zu bleiben, und verhält sich mehr wie ein klassisches Bit. Dies hat zur Folge, dass Informationen, welche eigentlich im Zustand des Qubits gespeichert waren, verloren gehen. Diese Art von Fehler entsteht nicht durch eine bestimmte Fehlbedienung, sondern ist ein ganz natürlicher Effekt, sobald ein Quantensystem mit seiner Umgebung in Kontakt tritt (Nielsen & Chuang, 2010, Kap. 8.3).

Man unterscheidet dabei zwei Hauptarten von Dekohärenz. Zum einen die Phasendekohärenz und die Amplitudendekohärenz. Bei der Phasendekohärenz bleibt das Qubit grundsätzlich im selben Zustand. Zum Beispiel 
∣0⟩ oder ∣1⟩,
aber die sogenannte Phase zwischen den Zuständen verändert sich. Das hat  weitreichende Folgen. Die Phase ist entscheidend dafür, ob Quanteninterferenzen funktionieren, also ob die verschiedenen Zustände eines Qubits sinnvoll zusammenwirken können. Ist die Phase gestört, kann der Quantencomputer falsche oder gar keine Ergebnisse mehr liefern.

Amplitudendekohärenz dagegen beschreibt eine Veränderung des Energiezustands des Qubits. Das tritt häufig dann auf, wenn das Qubit aus einem angeregten Zustand wieder in seinen Grundzustand „zurückfällt“. Dieser Vorgang entspricht einem Energieverlust. Dies passiert zum Beispiel durch spontane Emission von Strahlung oder durch Wärmeabgabe an die Umgebung. In vielen physischen Systemen, etwa bei supraleitenden Qubits oder bei Ionenfallen, ist dieser Prozess besonders häufig zu beobachten (Rieffel \& Polak, 2011, Kap. 10.4.4).

Doch Dekohärenz ist nicht die einzige Fehlerquelle in Quantencomputern. Weitere Ursachen liegen in verschiedenen Rauschprozessen, also zufälligen oder unkontrollierten Störungen aus der Umgebung. Dazu zählen zum Beispiel thermisches Rauschen, das durch Temperaturunterschiede entstehen kann, oder elektromagnetische Fluktuationen, etwa von Stromleitungen, elektronischen Geräten oder sogar Mobilfunkstrahlung. Auch mechanische Vibrationen. Diese können unteranderem durch Pumpen oder Bewegungen im Kühlapparat entstehen. Diese können sich auf die empfindlichen Qubits übertragen.

Ein besonderes Problem ist die unzureichende Isolation der Qubits. Idealerweise sollte ein Qubit vollständig von seiner Umgebung abgeschottet sein. In der Realität ist das kaum möglich. Selbst im Vakuum oder bei extrem tiefen Temperaturen gibt es noch mikroskopisch kleine Wechselwirkungen mit anderen Teilchen, Materialien oder Defekten. Manche Materialien enthalten zum Beispiel sogenannte Zwei-Niveau-Systeme, die mit den Qubits interagieren können und als zusätzliche Störquellen fungieren. Auch kosmische Strahlung oder Magnetfelder aus der Umgebung können Störungen verursachen (Tutschku et al., 2023).

Aus all diesen Gründen ist es extrem wichtig, die Quantenhardware so gut wie möglich gegen äußere Einflüsse zu schützen. Das geschieht zum Beispiel durch ultratiefe Temperaturen im Millikelvin-Bereich, durch spezielle Abschirmungen gegen elektromagnetische Wellen, durch mechanische Entkopplung der Geräte und durch den Einsatz besonders reiner Materialien ohne Defekte. Trotzdem lässt sich nie ganz vermeiden, dass äußere Einflüsse auf das System wirken. Daher ist es essenziell, zusätzlich auf softwareseitige Verfahren zur Fehlerkorrektur zu setzen – etwa durch den Einsatz logischer Qubits oder redundanter Kodierungen.

Quantenfehler sind also ein Zusammenspiel aus unvermeidbaren physikalischen Prozessen, technologischen Grenzen und Umwelteinflüssen. Sie zu verstehen und zu kontrollieren ist eine der größten Herausforderungen beim Bau und Betrieb eines funktionierenden Quantencomputers.

\subsection{Klassifizierung von Quantenfehlern}
Die Klassifikation von Quantenfehlern stellt eine essenzielle Grundlage für die Entwicklung robuster Fehlerkorrekturverfahren im Quantencomputing dar. Aufgrund der Superposition können Fehler nicht nur den Zustand selbst, sondern auch die relative Phase, Amplitude oder die Kohärenzeigenschaften eines Qubits beeinflussen. Die gebräuchlichste Einteilung unterscheidet zwischen diskreten, kontinuierlichen, zufälligen und systematischen Fehlern sowie zwischen Einzelqubit- und Mehrqubitfehlern.

Die Diskrete Fehlerarten sind die Pauli-Fehler. Die einfachste und zugleich mathematisch fundamentale Klasse von Quantenfehlern basiert auf den sogenannten Pauli-Fehlern, benannt nach den drei Pauli-Matrizen 
X, Y und Z. Diese Operatoren stellen Basisoperationen im zweidimensionalen Qubit-Zustandsraum dar und bilden eine vollständige Fehlerbasis für beliebige Ein-Qubit-Störungen

Der Bit-Flip-Fehler (X-Fehler): Ein Bit-Flip entspricht einer Umkehrung des Qubitzustands von ∣0⟩ nach ∣1⟩ oder umgekehrt. Dieser Fehler kann zum Beispiel durch thermische Anregung entstehen, etwa wenn ein Qubit spontan aus dem Grundzustand in einen angeregten Zustand übergeht. In Formeln entspricht dies der Anwendung des X-Operators: 
X∣0⟩=∣1⟩ (Nielsen & Chuang, 2010, S. 449).

Der Phase-Flip-Fehler (Z-Fehler): Beim Phase-Flip verändert sich die relative Phase eines Qubits. So wird etwa der Zustand 
∣+⟩=(∣0⟩+∣1⟩)/ 
2
​	
  durch einen Z-Operator in  ∣−⟩=(∣0⟩−∣1⟩)/ 2
​	
 überführt. Dieser Fehler beeinträchtigt Interferenzeffekte und ist besonders kritisch für Algorithmen, die auf kohärenter Phasenevolution basieren (ebd., S.449).
 
Kombinierte Bit- und Phase-Fehler (Y-Fehler): Der Y-Operator kombiniert Bit- und Phase-Flip gleichzeitig: Y=iXZ. Solche Fehler treten häufig bei komplexeren physikalischen Störungen auf, wie zum Beispiel durch Crosstalk zwischen Qubits oder bei nichtlokalen Umwelteinflüssen (Rieffel & Polak, 2011, S.375).

Diese drei Operatoren bilden eine vollständige Fehlerbasis für Ein-Qubit-Störungen. Jeder beliebige Fehler auf einem Qubit kann mathematisch als Linearkombination von 
I, X, Y und Z dargestellt werden. Diese Eigenschaft ist von zentraler Bedeutung für den Aufbau von Quantenfehlerkorrekturcodes wie dem Shor-Code oder dem Steane-Code, welche gezielt auf Pauli-Fehler reagieren (Nielsen & Chuang, 2010, S.449–451).


Neben diskreten Fehlern gibt es auch kontinuierliche Fehlerprozesse, die sich nicht durch einzelne Pauli-Operatoren darstellen lassen, sondern über sogenannte Quantenkanäle modelliert werden. Die zwei wichtigsten Beispiele sind Amplitudendämpfung und die Phasendämpfung.

Amplitudendämpfung: Diese beschreibt den Verlust von Energie durch das Qubit, etwa infolge spontaner Emission oder thermischer Relaxation. Der Zustand ∣1⟩ relaxiert dabei mit einer gewissen Wahrscheinlichkeit 
γ in den Grundzustand ∣0⟩. Die Kraus-Darstellung des Amplitudendämpfungskanals lautet:
 BILD EINFÜGEN 
(Nielsen & Chuang, 2010, S.376–377)

Phasendämpfung (Dephasing): Hierbei geht keine Energie verloren, aber Kohärenz. Die Off-Diagonal-Elemente der Dichtematrix, also die, die Superpositionen erzeugen, werden gedämpft. Dies geschieht durch Umwelteinflüsse wie Fluktuationen elektromagnetischer Felder. In der Praxis ist Phasendämpfung oft die dominante Dekohärenzquelle (Rieffel & Polak, 2011, S.374–375).
Die Modellierung dieser kontinuierlichen Prozesse ist entscheidend für die realistische Simulation von Quantensystemen und wird durch Kraus-Operatoren und Lindblad-Gleichungen mathematisch beschrieben.

Quantenfehler lassen sich nicht nur nach ihrer physikalischen Form, sondern auch nach ihrer Entstehungsart klassifizieren. Hierzu zählen zufällige Fehler und systematische Fehler.

Zufällige Fehler (stochastisch): Diese Fehler entstehen unvorhersehbar durch thermisches Rauschen, Photonen-Einfälle, kosmische Strahlung oder spontane Kopplungen an die Umgebung. Sie sind oft kurzzeitig, unkorreliert und lassen sich statistisch modellieren, z. B. über Randomized Benchmarking (Tutschku et al., 2023, S. 50).
Systematische Fehler: Diese beruhen auf wiederholbaren, deterministischen Einflüssen, wie falscher Kalibrierung von Pulssequenzen, ungenauen Gatterzeiten oder falsch modellierter Kopplung. Solche Fehler summieren sich über Zeit auf und können ganze Rechenprozesse verfälschen. Sie sind schwerer zu korrigieren, da sie nicht durch Mittelwertbildung „herausrauschen“ (Google Quantum AI, 2024, S. 3–4).
Die Unterscheidung ist für die Architekturplanung essenziell, da systematische Fehler oft durch verbesserte Hardware, kontrollierte Steuerungen oder modellgestützte Korrektur vermieden werden können.

Ein wachsendes Problem in größeren Quantenprozessoren ist das Auftreten von korrelierten Fehlern. Solche Fehler beeinflussen mehrere Qubits gleichzeitig und können durch gemeinsame Störquellen oder durch ungewollte Kopplungen entstehen. Anders als unabhängige Fehler breiten sie sich nicht lokal aus, sondern erzeugen komplexe Fehlerbilder, die von klassischen Fehlerkorrekturmodellen oft nicht erfasst werden.


\subsection{Hardwarebedingte Fehler und Systematische Grenzen}
Neben den oben beschriebenen Fehlermechanismen spielen auch hardwarebedingte Fehler eine zentrale Rolle in der Fehlertheorie des Quantencomputings. Diese entstehen durch Unzulänglichkeiten in der technischen Umsetzung der Steuerung, der Auslese und der physikalischen Qubit-Architektur. In realen Quantenprozessoren, wie supraleitenden Schaltkreisen, Ionenfallen oder spinbasierten Systemen, sind diese Fehlerquellen ein wesentlicher limitierender Faktor für die Skalierbarkeit und Zuverlässigkeit der Systeme.

Ein wesentliches Problem sind fehlerhafte Gatteroperationen. Idealerweise sollen Quantenlogikgatter wie das CNOT- oder Hadamard-Gate eine definierte unitäre Transformation auf den Zustand der Qubits ausführen. In der Praxis weichen die implementierten Operationen von diesem Ideal ab. Ursachen dafür sind unter anderem folgende Fehler:

Timing-Fehler: Ungenauigkeiten in der Pulslänge oder -frequenz führen zu nicht vollständig abgeschlossenen Rotationen.
Crosstalk: Eine ungewollte Kopplung zwischen benachbarten Qubits oder Steuerleitungen kann zu Störungen führen, die nicht auf das Zielqubit beschränkt bleiben.
Nichtlinearitäten in der Pulselektronik, die insbesondere bei stark skalierten Systemen auftreten.
Diese Gatterfehler akkumulieren sich über tiefe Schaltkreise hinweg und führen dazu, dass die logische Fehlerrate mit zunehmender Schaltungstiefe exponentiell ansteigt (Nielsen & Chuang, 2010, S.390–391; Tutschku et al., 2023, S.51). Selbst bei modernen Quantenprozessoren liegt die mittlere Gatterfidelität, also die Wahrscheinlichkeit, mit der ein Gatter korrekt ausgeführt wird, typischerweise nur bei 99–99,9% (Google Quantum AI, 2024, S.3).

Ein weiterer kritischer Punkt sind Messfehler. Die Messung eines Qubits erfolgt meist über eine Verstärkung und Auswertung eines quantenmechanischen Signals (z.B. dispersive Kopplung an einen Resonator bei supraleitenden Qubits oder Fluoreszenz in Ionenfallen). Dabei kann es zu Fehlern kommen durch:

Rauschen in der Verstärkerschaltung
Überlappung von Signalverteilungen für 
∣0⟩ und ∣1⟩
Verzögerungen oder Sättigungseffekte in der Ausleseelektronik
Diese Fehler können zu falscher Interpretation des Qubit-Zustands führen und beeinträchtigen damit die Zuverlässigkeit von Algorithmen und insbesondere von Fehlerkorrekturcodes, die sich auf korrektes Auslesen von Syndromen verlassen (Nielsen & Chuang, 2010, S.414; Tutschku et al., 2023, S.50).

Die Initialisierung von Qubits ist ein notwendiger erster Schritt jeder Quantenberechnung. Ziel ist es, alle Qubits zuverlässig in den Grundzustand ∣0⟩ zu setzen. In der Praxis gelingt dies nicht immer mit perfekter Genauigkeit. Ursachen sind unter anderem:

Thermische Anregung bei unzureichender Kühlung
Unvollständiges Relaxieren aus dem angeregten Zustand
Restkopplungen an Resonatoren oder Nachbarqubits
Fehlerhafte Initialisierungen wirken sich direkt auf die korrekte Ausführung der ersten Gatteroperationen aus und können sich durch den Schaltkreis fortpflanzen (Rieffel & Polak, 2011, S.378; Google Quantum AI, 2024, S.3).

In skalierbaren Architekturen, vor allem in zweidimensionalen Gittern supraleitender Qubits, sind nicht alle Qubits direkt miteinander verbunden. Diese eingeschränkte Konnektivität führt dazu, dass logische Operationen, die Qubits an weit entfernten Positionen betreffen, durch zusätzliche SWAP-Gatter oder Relaisoperationen realisiert werden müssen. Diese zusätzlichen Schritte erhöhen nicht nur die Rechenzeit, sondern fügen dem System zusätzliche Fehlerquellen hinzu. Besonders kritisch ist dies in tiefen Schaltkreisen oder bei komplexen Algorithmen wie QAOA oder QFT (Tutschku et al., 2023, S.49).

Alle oben genannten Fehlerarten werden zusätzlich durch Kalibrierfehler verstärkt. Diese entstehen, wenn die physikalischen Parameter des Systems, wie Frequenzen, Kopplungsstärken, Pulsamplituden oder Dämpfungsraten nicht exakt bekannt oder nicht stabil sind. Selbst kleine Abweichungen in der Kalibrierung können über viele Rechenzyklen hinweg zu signifikanten Abweichungen im Endergebnis führen.

Studien zeigen, dass Kalibrierfehler insbesondere bei nicht regelmäßig durchgeführten Rekalibrierungszyklen zu systematischer Fehlentwicklung der Qubit-Zustände führen (Google Quantum AI, 2024, S.4–5).

\subsection{Quantifizierung und Modellierung von Quantenfehlern}
Die präzise Modellierung und Quantifizierung von Fehlern in Quantencomputern ist essenziell, um deren Auswirkungen auf die Informationsverarbeitung zu verstehen und wirksame Fehlerkorrekturstrategien zu entwickeln. Aufgrund der Besonderheiten quantenmechanischer Systeme, insbesondere Superposition, Nichtklassikalität und Unitarität, ist eine adäquate Beschreibung von Fehlerprozessen nur durch formalisierte mathematische Modelle möglich, die über klassische Störmodelle weit hinausgehen. In der Theorie offener Quantensysteme werden solche Fehler durch Quantenkanäle beschrieben, deren Wirkung über sogenannte Krausoperatoren modelliert wird.

Fehlerkanäle und Krausoperatoren

Ein Quantenkanal beschreibt die Wirkung eines Umgebungseinflusses auf ein Quantensystem, das sich dadurch in einen neuen Zustand überführt. Formal wird ein Quantenkanal (E) durch eine completely positive, trace-preserving (CPTP) Abbildung auf dem Raum der Dichtematrizen definiert. Die Wirkung eines solchen Kanals auf einen Zustand ρ wird durch die Kraus-Zerlegung beschrieben:

FORMEL BILD EINFÜGEN
Hierbei sind  E k E  k die Krausoperatoren, die jeweils eine mögliche Fehlerwirkung modellieren. Diese Darstellung ist besonders mächtig, da sie sowohl diskrete als auch kontinuierliche Fehlermechanismen integriert. Je nach Art des Rauschprozesses ergeben sich unterschiedliche konkrete Kanalformen:
Amplitudendämpfungskanal: Modelliert Energieverluste wie spontane Emission. Dabei relaxiert das Qubit mit Wahrscheinlichkeit 
γ vom angeregten Zustand ∣1⟩ in den Grundzustand ∣0⟩. Die zugehörigen Krausoperatoren lauten:

(BILD EINFÜGEN DER FORMEL)
(Nielsen & Chuang, 2010, S.377)
Depolarisationskanal: Beschreibt symmetrisches Rauschen, bei dem mit Wahrscheinlichkeit p ein zufälliger Pauli-Fehler (X, Y, Z) auftritt:
(BILD VOPN FORMEL EINFÜGEN)
 (XρX+YρY+ZρZ)
(ebd., S.376)
Diese Modelle bilden die Grundlage für theoretische Fehleranalysen und sind integraler Bestandteil moderner Simulationen von Quantenalgorithmen.

Fehlerraten

Neben der qualitativen Modellierung von Fehlerprozessen ist deren quantitative Beschreibung durch Fehlerraten entscheidend. Die Fehlerrate gibt an, mit welcher Wahrscheinlichkeit eine Operation – z.B. ein Quanten-Gatter oder eine Messung, fehlerhaft ausgeführt wird. Sie stellt eine wichtige Benchmarkgröße dar, insbesondere im Kontext der Skalierbarkeit und Fehlerkorrektur.

Typische Fehlerraten in heutigen NISQ-Systemen (Noisy Intermediate-Scale Quantum):

Ein-Qubit-Gatterfehler: ca. 10−4 bis 10−3
 
Zwei-Qubit-Gatterfehler: ca. 
10−3 bis 10−2
 
Messfehler: typischerweise zwischen  0.5% und 1% je nach Architektur (Tutschku et al., 2023, S. 51–52)
Eine häufig verwendete Metrik ist die average gate fidelity, welche angibt, wie nahe die tatsächlich implementierte Gatteroperation im Mittel an der idealen, unitären Operation liegt.

Fehlervermessung: Methoden zur Fehlerquantifizierung

Um Fehlermodelle nicht nur theoretisch anzusetzen, sondern empirisch zu validieren und zu quantifizieren, kommen in der experimentellen Quanteninformatik verschiedene Messverfahren zum Einsatz:

Quantum Process Tomography (QPT): Ermöglicht die vollständige Rekonstruktion des Quantenkanals durch systematische Zustandsvorbereitungen und Messungen. Aufgrund des exponentiellen Aufwands bei wachsender Qubit-Anzahl ist QPT auf wenige Qubits beschränkt (Nielsen & Chuang, 2010, S.388).
Randomized Benchmarking (RB): Verwendet zufällig gewählte Gatterfolgen aus einer Gruppe (typischerweise Clifford-Gruppenoperationen), um statistisch robuste Aussagen über mittlere Fehlerraten zu treffen. Der Vorteil von RB liegt in seiner Unempfindlichkeit gegenüber SPAM-Fehlern (State Preparation and Measurement) (Tutschku et al., 2023, S.52).
Cross-Entropy Benchmarking (XEB): Ein Verfahren, das besonders bei großen supraleitenden Qubit-Systemen wie Sycamore oder Bristlecone zum Einsatz kommt. Es vergleicht die gemessene Ausgabeverteilung eines komplexen Quantenkreises mit der theoretisch erwarteten Verteilung aus klassischer Simulation. Die Übereinstimmung (Cross-Entropy) gibt Rückschluss auf die Gesamteffizienz und Fehlerhäufigkeit des Systems (Google Quantum AI, 2024, S.3–4)

\subsection{Auswirkungen von Fehlern auf Quantenalgorithmen und Systemarchitektur}
Fehler in Quantencomputern wirken sich nicht isoliert auf einzelne Operationen aus, sondern haben umfassende Konsequenzen für die Stabilität, Zuverlässigkeit und Effizienz ganzer Quantenalgorithmen. Aufgrund der spezifischen Eigenschaften von Quanteninformation, vor allem der Verwendung von Superposition und Interferenz, kann bereits ein einziger Fehler in einem Quantenregister dazu führen, dass das Rechenergebnis vollständig verfälscht oder unbrauchbar wird. Das No-Cloning-Theorem schließt zusätzlich aus, dass fehleranfällige Quanteninformationen einfach dupliziert oder redundant gespeichert werden können (Nielsen & Chuang, 2010, S.466). Damit sind Fehlerkorrektur und Systemtoleranz nicht nur Ergänzungen, sondern fundamentale Voraussetzungen für zuverlässige Quantenverarbeitung.

Rechenabbrüche und Fehlerakkumulation

In praktisch ausgeführten Algorithmen, wie bei Grover’s Algorithmus, Shor’s Faktorisierungsverfahren oder variationalen Methoden wie VQE (Variational Quantum Eigensolver), kann ein einzelner Fehler eine Verkettung von Folgefehlern auslösen. Dies führt dazu, dass die Zustandsamplituden falsche Interferenzen erzeugen oder sich logisch fehlerhafte Zustände im System ausbreiten. Besonders problematisch ist dabei die Akkumulation von Fehlern über viele Rechenzyklen hinweg. Bei jedem Gatteraufruf, jeder Qubit-Wechselwirkung und jeder Messung steigt die Wahrscheinlichkeit, dass sich ein Fehler einschleicht (Tutschku et al., 2023, S.53).

Rechenabbrüche treten dann auf, wenn die Fehlerzahl oder ihre Art eine erfolgreiche Fortsetzung oder Interpretation des Algorithmus unmöglich macht. Ursachen hierfür sind unter anderem:

Timing-Fehler oder falsche Pulslängen bei Zwei-Qubit-Gattern, wodurch die beabsichtigte Kopplung nicht korrekt implementiert wird.
Überschreiten der Kohärenzzeiten T1 (Relaxation) und T2 (Dephasierung), was in tief verschachtelten Algorithmen fast unausweichlich ist, wenn keine Fehlerkorrektur erfolgt.
Leckagefehler, bei denen ein Qubit aus dem definierten Zustandsraum (z.B. ∣0⟩, ∣1⟩) in einen höheren Energiezustand übergeht, oder Crosstalk, bei dem Qubit A durch das Ansteuern von Qubit B unbeabsichtigt beeinflusst wird (Tutschku et al., 2023, S.53; Google Quantum AI, 2024, S.3).

Fehlerbudget und Fehlertoleranz
Zur planvollen Kontrolle dieser Risiken wird in der Architekturentwicklung ein sogenanntes Fehlerbudget verwendet. Dieses definiert, wie viele und welche Arten von Fehlern ein System in einer gegebenen Operationstiefe oder Zeitdauer tolerieren kann, ohne dass die logische Konsistenz der Berechnung verloren geht. Entscheidend dafür ist die Fehlerschwelle (fault-tolerance threshold), also die maximale physikalische Fehlerrate, bei der eine effektive Fehlerkorrektur noch möglich ist.

Liegt die effektive Fehlerrate pro Gatteroperation oder Qubit unterhalb dieser Schwelle, so kann durch wiederholte Korrekturzyklen ein logisch fehlerfreier Zustand aufrechterhalten werden. Wird sie überschritten, kann auch ein noch so ausgeklügelter Fehlerkorrekturcode das System nicht mehr retten (Nielsen & Chuang, 2010, S.452; Preskill, 1998, S.226).

Beispiele für Fehlerschwellen:

Surface Codes: Schwelle bei ca. 1% pro Gatteroperation, gelten als besonders robust, da sie nur lokale Interaktionen benötigen (Google Quantum AI, 2024, S.2–4).
Steane- und Bacon-Shor-Codes: Schwelle bei etwa 10 −3, dafür mit höherem logischem Overhead (Rieffel & Polak, 2011, S.393–395).
Um unterhalb dieser Schwelle zu operieren, ist ein signifikanter Hardware-Overhead erforderlich. Für ein einziges logisch geschütztes Qubit werden, je nach verwendetem Code und Fehlertoleranz, zwischen 50 und über 1.000 physikalische Qubits benötigt (Tutschku et al., 2023, S.53). Diese Relation bestimmt maßgeblich die Skalierungsgrenzen aktueller Quantenprozessoren.

Architekturstrategien für Fehlertoleranz

Die Gestaltung der Quantenhardware und -architektur muss daher darauf ausgerichtet sein, mit einer begrenzten Anzahl physikalischer Qubits möglichst effektive Fehlerresistenz zu gewährleisten. Dazu gehören:

Fehlertolerante Layouts, z.B. zweidimensionale Qubit-Gitter mit lokaler Nachbarschaftsstruktur (wie im Surface Code), die einfache Syndrome-Messung und Korrektur ermöglichen.
On-Chip-Feedbacksysteme, die Fehler während der Berechnung in Echtzeit detektieren und sofort korrigierend eingreifen können – etwa durch Mikrocontroller oder FPGA-Logik in der Steuerungselektronik.
Adaptives Routing, das es erlaubt, defekte oder fehlerhafte Qubits dynamisch zu umgehen, ohne die gesamte Berechnung zu unterbrechen (Google Quantum AI, 2024, S.3–5).
Zukünftige Entwicklungen konzentrieren sich zudem auf intelligente Dekodierer, die mithilfe maschinellen Lernens in der Lage sind, nicht nur klassische, sondern auch korrelierte und systematische Fehler zu erkennen und zu kompensieren.


\section{Grundprinzipien in Quantenfehlerkorrektur}\label{chap:QEC2}
\subsection{Fehlererkennung ohne Zustandsmessung}
Eines der zentralen Konzepte der Quanten-Fehlerkorrektur ist, dass man Fehler erkennen kann, ohne die kodierte Quanteninformation zu messen und somit zu zerstören. In klassischen Systemen könnten wir etwa redundante Bitmuster verwenden, um Fehler aufgrund direkter Messung zu erkennen und zu korrigieren. In einem quantenmechanischen System ist es jedoch nicht so einfach möglich, eine Messung durchzuführen, da ein solcher Schritt den Superpositionszustand des Qubits kollabieren ließe und damit die gespeicherte Information unter Umständen irreversibel zerstören könnte. \cite{nielsen_michael_a_and_isaac_l_chuang_quantum_2010}

Um das zu umgehen, führt man im Bereich der Quanten-Fehlerkorrektur sogenannte Syndrommessungen durch. Dabei handelt es sich um Messungen, bei denen nicht der Zustand des Qubits direkt gemessen wird, sondern nur Information über das Eintreten von Fehlern. Man misst dabei den sogenannten Stabilisator-Operator, der zu einem speziellen Quanten-Fehlerkorrekturcode gehört. Diese Operatoren definieren eine Menge erlaubter Zustände – sogenannte +1-Eigenzustände der Stabilizer-Gruppe. Tritt ein Fehler auf, so verändert sich die Eigenwertstruktur des Stabilisators. Ein Wechsel zwischen \(+1\) und \(-1\) ist zum Beispiel ein  Zeichen dafür, dass ein bestimmter Fehler passiert. \cite[Seite 444-446]{nielsen_michael_a_and_isaac_l_chuang_quantum_2010}

Die Syndrommessung erfolgt in der Praxis meist so, dass man Hilfsqubits (Ancillae) einführt, auf denen man den Stabilisator wirken lässt und so Informationen darüber erhält, was der Stabilizer auf den gekodeten Zustand angewendet hätte, ohne hierbei den gekodeten Zustand zu verändern. So lassen sich ernsthaft Fehler zum Beispiel durch die sogenannten kontrollierten Operatoren auch zu dem Hilfs-Qubit "weiterleiten" und damit dort auslesen. Dabei bleibt die gekodete Quanteninformation erhalten. Man nutzt dies dazu, das sogenannte Fehlersyndrom zu bestimmen um dann gezielt unitäre Korrektur-Operatoren durchzuführen (siehe Kapitel \ref{chap:QEC3}.). \cite[Seite 444-446]{nielsen_michael_a_and_isaac_l_chuang_quantum_2010}

Ein Schlüsselelement des fehlertoleranten Quantencomputings ist die Möglichkeit, einen Quantenzustand messen zu können, ohne dass er in seinen Ursprungszustand zu zwei Drittelwahrscheinlichkeit kollabiert. Dank dieser Fähigkeit können Codes konstruiert werden, welche die Integrität von Quanteninformation auch dann noch lange Zeit sicherstellen, wenn wiederkehrende Fehler laufend erkannt und ausgebessert werden.

\subsection{Redundanz durch Kodierung}
Quanten-Fehlerkorrektur beruht auf zentraler Idee, Quantenzustände vor Fehlern zu schützen, indem Redundanz eingefügt wird. Das bedeutet, ein einzelnes logisches Qubit wird nicht durch ein einziges physikalisches Qubit dargestellt, sondern auf mehrere physikalische Qubits verteilt. Genau durch diese zusätzliche Kodierungskapazität können Fehler eintreten, ohne dass dabei die logische Quanteninformation verloren geht.

Im Unterschied zur klassischen Fehlerkorrektur, bei der Redundanz verwendet wird (z.B. dreifache Paritätsbits), muss in der Quantenwelt solch ein Ansatz jedoch mit Bedacht gewählt werden. Denn das No-Cloning-Theorem verhindert, dass man auch einfach unbekannte Quantenzustände duplizieren kann. \cite{nielsen_michael_a_and_isaac_l_chuang_quantum_2010} Stattdessen wird die Kodierung mithilfe einer kohärenten Verschränkung mehrerer Qubits erreicht, sodass die Quanteninformation nicht in einem einzelnen Qubit, sondern in einem höherdimensionalen Unterraum des Gesamtsystems gespeichert wird.

Ein bekanntes Beispiel ist der Shor-Code, bei dem ein logisches Qubit auf neun physikalische Qubits abgebildet wird. Dieser Code schützt sowohl vor Bit-Flip- als auch vor Phase-Flip-Fehlern, indem er erst eine dreifache Kopie erstellt, um Bit-Flips zu entdecken, dann jede dieser Kopien nochmals mit einer Phase-Flip-Korrektur kodiert. \cite{shor_scheme_1995}
Ebenso gut ist der Steane-Code, der sieben Qubits zum Darstellen eines logischen Qubits verwendet und den klassischen (7, 4, 3)-Hamming-Code verwendet. \cite{Steane}

Mit Hilfe einer solchen Kodierung kann Quanteninformation in einem sogenannten Code-Raum abgelegt werden, der durch eine Gruppe von Stabilisator-Operatoren definiert ist. Tritt ein Fehler auf, so wird der Zustand in einen orthogonalen Unterraum verschoben, der außerhalb des Code-Raums liegt. Dieses Umschichten lässt sich über die Syndrommessung identifizieren – und zwar ohne, dass der ursprüngliche Zustand vermessen wird. Wesentlich ist, dass die Redundanz dabei hilft, Fehler zu lokalisieren und Korrekturoperationen durchzuführen, ohne die Quantenkohärenz zu stören.

Die Redundanz durch Kodierung ist somit die Voraussetzung für alle Quanten-Fehlerkorrekturverfahren: Denn nur indem man Information auf viele Qubits verteilt, kann man dem System Fehler \(\)verzeihen, ohne dass es aus funktionalem Zerfallen lässt.

\subsection{Fehlerzustände müssen unterscheidbar sein}
Ein entscheidendes Konzept in der Quantenfehlerkorrektur ist, dass Fehler auf einfache Weise voneinander unterscheidbar sein müssen. Nur wenn Fehler Zustände auf unterschiedliche Weise beschädigen, hat man eine Chance, festzustellen welcher der Fehler aufgetreten ist, mit dem Ziel diesen gezielt zu korrigieren ohne die Quanteninformation zu verlieren.

In der Sprache der Quanteninformatik bedeutet dies, dass die durch Fehler erstellten Zustände orthogonal zueinander sein müssen, beziehungsweise so aufgebaut, dass man durch Messung eines fehlerhaften Zustands auf den Code-Raum schließen kann. Andernfalls besteht die Gefahr, dass Fehler, die zu gleichem Messausgang führen, nicht unterschieden werden können und die Messung zu einer falschen Korrektur und dem Verlust von Information führt. \cite[Seite 449–451]{nielsen_michael_a_and_isaac_l_chuang_quantum_2010}

Dieses Konzept wird durch das sogenannte Fehlerkorrekturkriterium mathematisch präzisiert. Für eine Menge von Fehlern \(\left\{E_{i}\right\}\) , die von einem Quantencode korrigiert werden sollen, gilt:
\begin{equation}
    \left\langle\psi_{a}\right| E_{i}^{\dagger} E_{j}\left|\psi_{b}\right\rangle=C_{i j} \delta_{a b}
\end{equation}
für alle Zustände \(
    \left|\psi_{a}\right\rangle,\left|\psi_{b}\right\rangle
\)  im Code-Raum. Hierbei hängt die Konstante \(
    C_{i j}
\) vom Fehler,  nicht jedoch vom kodierten Zustand ab. Dieses Kriterium ist notwendig um sicherzustellen, dass die Wirkung eines Fehlers unabhängig von der gespeicherten Information eindeutig bestimmbar ist.

Aber aufgepasst: Wenn zwei unterschiedliche Fehler denselben Effekt auf den Code haben – also nicht unterscheidbar sind –, kann keine gezielte Korrektur erfolgen. Die Fähigkeit zur Unterscheidung solcher Fehlerzustände ist also die Voraussetzung für \textit{irgendeine} syndrombasierte Fehlerkorrektur. Nur dadurch können beispielsweise Look-Up-Tabellen oder automatische Fehlerkorrekturmechanismen sicher die \textit{einzig mögliche} Korrektur wählen.

Das Prinzip hat übrigens eine Parallele zur Konstruktion von Stabilizer-Codes, bei denen unterschiedliche Fehler unterschiedliche Syndrommuster hinterlassen. Durch geschickte Wahl der Stabilisatorgruppe kann man erreichen, dass in einem Stabilizer-Code alle Fehler innerhalb der Toleranzgrenze eindeutige Syndrome haben. \cite{gottesmann Stabilizer Codes}

\subsection{Reversibilität}
Ein entscheidendes Prinzip der Quanten-Fehlerkorrektur ist es, dass sämtliche eingesetzten Operationen im Fehlerkorrekturprozess umkehrbar sein müssen. In der Quantenmechanik entsprechen sämtliche möglichen zeitlichen Entwicklungen von isolierten Systemen unitären Transformationen – also Operationen, die völlig umkehrbar sind und keine Information vernichten. Diese Forderung gilt es auch für die Fehlerkorrektur durchzusetzen, da eine unwiderrufliche Fehlerkorrektur notwendigerweise den kodierten Quantenzustand zerstören und die Kohärenz der Quanteninformation verlieren würde. \cite[Seite 450-451]{nielsen_michael_a_and_isaac_l_chuang_quantum_2010}

Anders ausgedrückt: Wenn bei einer Kette von Ereignissen ein Fehler durch das Syndrom erkannt wurde, erfolgt ihre Korrektur durch eine bestimmte, unitäre Operation, welche uns den Fehlerzustand wieder ins Urteil des Codes zurückführt. Ist dieser Zustand erreicht, ist unser Fehler nicht nur korrigiert, sondern auch so, dass wir ihn\textbf{ kohärent rekonstruieren} können – und dies ist für weiteres Quantenrechnen und für fehlerresistente Quantenrechner notwendig.

Dieses Erfordernis der Rückkehr zur Ausgangslage besitzt auch eine sehr klare und bequeme theoretische Beschreibung: Das sogenannte Nicht-Trivialitätskriterium für Fehlerkorrektur sagt uns, dass für jede nichttriviale Fehlerwirkung \(E_i\) eine zugehörige Korrekturoperation \(R_i\) existiert, so dass 
\begin{equation}
    R_{i} E_{i}|\psi\rangle=|\psi\rangle \quad \text { für alle }|\psi\rangle \in \mathcal{C}
\end{equation}
Die Fehlererkennung und -korrektur insgesamt bildet somit einen Prozess, bei dem der Quanteninformationsträger lediglich entlang eines Umwegs durch Code-Hilfsraum entlanggeführt wird und die Logikqubit-Effektivität erhalten bleibt, um in der Lage zu sein, auch weitere Quantengatter auf diese anzuwenden.

Darum unterscheidet sich die Quanten-Fehlerkorrektur deutlich z.B. von klassischen Korrekturverfahren, welche nicht unbedingt reversibel sein müssen. In der Quanteninformatik wiederum ist Reversibilität jedoch ein \textbf{fundamentales physikalisches Prinzip}, das sowohl theoretisch als auch bei der Praxis der Implementierung beachtet werden muss.

\subsection{Fehlerlokalisierung}
Ein weiteres wichtiges Konzept, das hinter der Quanten-Fehlerkorrektur steckt, ist das der Fehlerlokalisierung. Damit ist gemeint, dass ein aufgetretener Fehler am besten so gefunden und behandelt werden kann, dass er sowohl räumlich als auch logisch eng eingegrenzt ist, und dass seine Folgen sich nicht unkontrolliert über das gesamte System ausbreiten. Nur wenn ein Fehler örtlich identifiziert und korrigiert werden kann, ist eine skalierbare und robuste Fehlerkorrektur möglich. \cite[Seite 451-452]{nielsen_michael_a_and_isaac_l_chuang_quantum_2010}

Fehler können in einem Quantencomputer verschiedene Ursprünge haben – z. B. Dekohärenz, Wechselwirkung mit der Umwelt oder Fehler bei Gates. Nach dem Prinzip der Fehlerlokalisierung sollte jeder dieser Fehler anfänglich lediglich eine begrenzte Anzahl physikalischer Qubits beeinflussen. Diese Voraussetzung soll es ermöglichen, mittels sogenannter Syndrommessungen zu erkennen, wo ein Fehler aufgetreten ist, ohne dabei den globalen Quantenzustand zu zerstören.

Dieses Konzept ist eng mit der Verwendung sogenannter lokaler Fehlerkorrektur-Codes wie z. B. topologischen Codes wie dem Surface Code verbunden. Bei diesen Codes sind die Qubits in einer regelmäßigen, zweidimensionalen Gitteranordnung angeordnet, bei der jeder Stabilizer-Operator nur Qubits betrachtet, die benachbart sind. Ein auftretender Fehler produziert ein lokales Syndrom, das sich aus den Nachbarn des fehlerhaften Qubits ergibt. Dadurch kann der Fehler nicht nur gefunden, sondern auch eingegrenzt und so gezielt korrigiert werden. \cite{Fowler et al}

Insbesondere für große, skalierbare Quantenrechner ist die Fähigkeit zur Fehlerlokalisierung essentiell: Denn ohne diese müssten sämtliche Qubits miteinander verschränkt und abgefragt werden, was einen exponentiellen Kontroll- und Ressourcenaufwand bedeuten würde. Bei lokalisierbaren Fehlern hingegen können lineare oder sogar konstante Skalierungsstrategien verfolgt werden – wodurch fehlertolerante Quantencomputer in greifbare Nähe rücken.

Zusammengefasst: Fehlerlokalisierung sorgt dafür, dass weder einzelne Fehler den gesamten Code-Raum beeinträchtigen, noch langfristige Korrekturprozesse destabilisieren. Sie ist damit eine notwendige Voraussetzung für praktische, modulare und skalierbare Quanten-Architekturen.

\subsection{Fehlerschwelle (Threshhold Theorem)}
Ein fundamentaler Mechanismus, um fehlertolerante Quantencomputer auch technisch umsetzen zu können, ist das Fehlerschwellenprinzip. Dieses besagt, dass sich Quanten-Fehlerkorrektur auf lange Sicht nur dann lohnt, wenn bei den einzelnen Qubits und Operationen die Wahrscheinlichkeit eines physikalischen Fehlers unter einem bestimmten Schwellenwert liegt, der als Fehlerschwelle bezeichnet wird. Wird die Schwelle unterschritten, so können beliebig exakte Rechnungen dadurch durchgeführt werden, dass man die Fehlerkorrektur immer wieder einsetzt – selbst wenn fortwährend physikalische Fehler passieren. \cite[Seite 454-456]{nielsen_michael_a_and_isaac_l_chuang_quantum_2010}

Dieses Prinzip wurde im Rahmen des Threshold Theorems auch rigoros bewiesen \cite{Aharonov und Ben-Or}. Das Theorem besagt, dass bei einer hinreichend kleinen Fehlerrate jedes Quanten-Computing mit beliebiger Genauigkeit durchgeführt werden kann – vorausgesetzt, es gibt genügend Redundanz und Korrekturschritte. Oder anders ausgedrückt: wenn Fehler selten genug und die Möglichkeit zur Fehlerkorrektur groß genug sind, dann können Fehler „überlebt“ werden.

Der genaue Wert der Fehlerschwelle hängt vom gewählten Code, der Architektur und der Art der Fehler ab. Er beträgt typischerweise  Wert zwischen \(
    10^{-4}
\)  und \(
    10^{-2}
\) und zählt derzeit Surface Codes zu den robustesten Verfahren. Deren experimentelle Fehlerschwelle liegt bei etwa \(1 \%\) für 
ein Gatemodell. \cite{Fowler et al.}

Von zentraler Bedeutung ist das Fehlerschwellenprinzip für die Entwicklung von Quantenhardware und -architektur, da es ermöglicht Fehler unterhalb einer beherrschbaren Schwelle und nicht als absolutes Hindernis aufzufassen. Es ist damit im Grunde eine theoretische Grundlage für das fehlertolerante Quantencomputing: Man kann Rechner auch mit unzuverlässigen Bauteilen aufbauen, solange die Wahrscheinlichkeit von Fehlern unterhalb der Fehlerschwelle liegt.

\section{Praktische Realisierung der Fehlerkorrektur}\label{chap:QEC3}

Dieses Kapitel konzentriert sich auf die praktische Umsetzung der Fehlerkorrektur (Quantum Error Correction, QEC).\\
Zuerst betrachten wir einige grundlegende Code-Beispiele, die auf den in Kapitel 5.2 erläuterten Prinzipien aufbauen (Abschnitt 5.3.1). Anschließend wird der Fehlerkorrekturzyklus detailliert am Beispiel der vielversprechenden Oberflächencodes dargestellt (Abschnitt 5.3.2). Abschließend werden aktuelle Schwierigkeiten auf dem Weg zu fehlertoleranten Quantencomputern diskutiert und ein Ausblick auf zukünftige Entwicklungen gegeben (Abschnitt 5.3.3). Der Fokus liegt dabei auf Aspekten der praktischen Umsetzung sowie den damit verbundenen Problemen und Lösungen.

\subsection{Anwendung von QEC-Prinzipien: Erste Code-Beispiele}

Wie in Abschnitt 5.1 ausführlich dargestellt wurde, sind Qubits anfällig für diverse Fehler, und wie in Abschnitt 5.2 erläutert, erfordern die grundlegenden Prinzipien der Quantenmechanik (wie das No-Cloning-Theorem und das Messproblem) spezielle Strategien zur Fehlerkorrektur. Anstatt diese Prinzipien hier erneut zu vertiefen, konzentrieren wir uns darauf, wie sie in ersten, einfachen Fehlerkorrekturcodes praktisch angewendet werden. Diese Codes verdeutlichen die grundlegende Idee, logische Information durch Kodierung über mehrere physikalische Qubits und durch Syndrommessungen zu schützen.

Um zu zeigen, wie die in Abschnitt 5.2 besprochenen Grundideen von Redundanz und Syndrommessung in der Praxis funktionieren, können wir uns einfache Fehlerkorrekturcodes ansehen. Der Drei-Qubit-Bitflip-Code ist so ein Beispiel. Hier wird ein logisches Qubit, z.B. $\alpha|0\rangle_L + \beta|1\rangle_L$, als $\alpha|000\rangle+\beta|111\rangle$ auf drei physikalischen Qubits gespeichert. Tritt beispielsweise ein Bitflip-Fehler auf einem der drei physikalischen Qubits auf (z.B. wird aus $|000\rangle$ der Zustand $|100\rangle$), kann dieser mittels spezifischer Prüfoperationen – der Messung von Stabilisatoroperatoren wie $Z_1Z_2$ und $Z_2Z_3$ – erkannt werden. Diese Messungen, deren Grundprinzip in Abschnitt 5.2 erläutert wurde, verraten, ob benachbarte Qubits gleich sind oder nicht, ohne den gespeicherten logischen Zustand selbst preiszugeben. Das Ergebnis dieser Syndrommessung identifiziert das fehlerhafte Qubit, woraufhin ein entsprechender X-Operator zur Fehlerkorrektur angewendet werden kann.\cite[Seite 427-430]{nielsen_quantum_2010}\\

Analog dazu existiert der Drei-Qubit-Phasenflip-Code, der Phasenfehler korrigieren kann. Hierbei wird der Zustand beispielsweise als $\alpha\ket{+++}+\beta\ket{---}$ kodiert, wobei $\ket{+}=(\ket{0}+\ket{1})/\sqrt{2}$ und $\ket{-}=(\ket{0}-\ket{1})/\sqrt{2}$. Durch Anwendung von Hadamard-Transformationen vor und nach einem Kanal, der Phasenfehler verursacht, kann dieser Code Phasenfehler effektiv in Bitfehler umwandeln, die dann analog zum Bitflip-Code korrigiert werden können.\cite[Seite 430-431]{nielsen_quantum_2010}\\

Diese einfachen Codes können jedoch jeweils nur einen spezifischen Fehlertyp (entweder Bitflip oder Phasenflip) auf einem einzelnen Qubit korrigieren. Um allgemeine Fehler, d.h. beliebige Kombinationen von Bit- und Phasenflips, auf mehreren Qubits zu korrigieren, sind komplexere Codes notwendig. Beispiele hierfür sind der von Peter Shor entwickelte Shor-Code, der ein logisches Qubit in neun physikalischen Qubits kodiert und einen beliebigen einzelnen Fehler korrigieren kann, oder die bereits erwähnten Oberflächencodes, die aufgrund ihrer praktischen Vorteile im nächsten Abschnitt detaillierter behandelt werden. Diese komplexeren Codes bauen auf den gleichen Grundprinzipien auf, nutzen aber weiter entwickelte Kodierungs- und Syndrommessverfahren.\cite[Seite 432ff]{nielsen_quantum_2010}\\
Die Notwendigkeit spezifischer Quantenfehlerkorrekturverfahren und die Komplexität ihrer praktischen Umsetzung ergeben sich somit direkt aus den in Kapiteln ~\ref{chap:QEC1} und ~\ref{chap:QEC2} diskutierten quantenmechanischen Gegebenheiten. Die folgenden Abschnitte vertiefen die Realisierung anhand leistungsfähigerer Code-Familien


\subsection{Der Fehlerkorrekturzyklus mit am Beispiel von Oberflächencodes}

Oberflächencodes (Surface Codes) gelten aktuell als einer der besten Ansätze für die praktische Quantenfehlerkorrektur. Sie sind deshalb so interessant, weil sie relativ hohe Fehlerraten erlauben (wie durch das in \ref{chap:QEC2} erläuterte Threshold-Theorem vorhergesagt) und gut zu vielen gängigen Qubit-Bauweisen passen.\\

\textbf{Einführung in Oberflächencodes (Surface Codes)}



\begin{itemize}
    \item \textbf{Der QEC-Zyklus -- ein Überblick:} Kodieren $\rightarrow$ Fehler passieren $\rightarrow$ Fehler finden $\rightarrow$ Fehler deute $\rightarrow$ Fehler beheben.
    \item \textbf{1. Logische Qubits kodieren:}
    \item ...
    \item \textbf{2. Fehler aufspüren (Syndrommessung):}
    \item ...
    \item \textbf{3. Fehler interpretieren (Dekodierung):}
    \item ...
    \item \textbf{4. Fehler beheben (Korrekturoperation):}
    \item ...
\end{itemize}

\subsection{Aktuelle Hürden und was die Zukunft bringt}
\begin{itemize}
    \item \textbf{Herausforderungen \& Ausblick -- ein Überblick:} Großer Qubit-Bedarf, Zeitaufwand, interne Fehlertoleranz des QEC-Prozesses, Dekoder-Performance, reale Fehlermodelle und die Suche nach besseren Lösungen.
    \item \textbf{Hürde: Massiver Qubit-Overhead (z.B. $O(d^2)$)}
    \item ...
    \item \textbf{Hürde: Zeitlicher Overhead und Latenz der Zyklen}
    \item ...
    \item \textbf{Hürde: Fehlertoleranz des Fehlerkorrekturprozesses selbst}
    \item ...
    \item \textbf{Hürde: Komplexität und Geschwindigkeit der Dekoder}
    \item ...
    \item \textbf{Hürde: Korrelierte Fehler und Abweichungen von idealen Fehlermodellen}
    \item ...
    \item \textbf{Ausblick: Entwicklung effizienterer Codes, Hardware-Verbesserungen, Co-Design, Fortschritte bei fehlertoleranten Gattern}
    \item ...
\end{itemize}

\section{Praxisbeispiel: Ein einfacher Quantenfehlerkorrekturcode}
\begin{itemize}
    \item Mit Qiskit Library und qiskit.providers.aer Rauschen simulieren und die Notwendigkeit von Fehlerkorrektur anhand eines Vergleichs aufzeigen
    \item Verschiedene Algorithmen zur Fehlerminimierung verwenden
    \item Verschiedene Rauschen und Ansätze vergleichen und kontrastisieren
    \item Qiskit Visualization Library erlaubt Darstellung von Zusammenhängen
\end{itemize}



\printbibliography
