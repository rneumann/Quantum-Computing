%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Fehlerkorrektur}
\label{error_correction} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\chapterauthor{Niklas Bodfeld, Tim Boschert, Manuel Meixner, Ann-Kathrin Wenzel}

\abstract{some abstract}

\section{Fehlertypen in Quantenrechnern}\label{chap:QEC1}

\subsection{Die Herausforderung von Fehlern in Quantencomputern}
Quantencomputer gelten als vielversprechende Technologie der Zukunft, da sie bestimmte Probleme deutlich effizienter lösen können als klassische Rechner. Ihre praktische Realisierung steht jedoch vor einer grundlegenden Hürde. Die Fehler. Diese treten in Quantensystemen häufiger auf als in klassischen Rechnern. Zudem sind sie viel schwerer zu kontrollieren.

Während klassische Systeme mit extrem niedrigen Fehlerraten arbeiten (unter 10⁻¹⁷), liegen diese bei heutigen Quantenprozessoren um Größenordnungen zwischen 10⁻³ und 10⁻¹. Der Grund dafür liegt unteranderem in der hohen Empfindlichkeit von Qubits gegenüber Umwelteinflüssen wie elektromagnetischen Feldern, Temperaturfluktuationen oder Strahlungen. Hinzu kommen Fehler durch ungenaue Hardware, Crosstalk zwischen benachbarten Qubits und unzuverlässige Mess- oder Initialisierungsprozessen.

Die Auswirkungen dieser Fehler sind tiefgreifend. Schon ein einzelner Fehler kann sich durch Verschränkung auf das gesamte System ausbreiten. Besonders kritisch sind Phasenfehler, da sie die Interferenzmuster zerstören können, auf denen viele Quantenalgorithmen beruhen.

Deshalb ist die Entwicklung effektiver Fehlerkorrekturmethoden zentral für die Zukunft des Quantencomputings. Nur wenn solche Mechanismen greifen, kann der theoretische Nutzen in der Praxis ausgeschöpft werden.


\subsection{Physikalische Fehlerursachen in Quantenrechnern}
Quantencomputer arbeiten nicht mit Bits sondern mit Qubits, die sich gleichzeitig in mehreren Zuständen befinden können. Dies ist ein Phänomen, das man als Superposition bezeichnet. Diese Superpositionen und die damit verbundenen quantenmechanischen Effekte machen Quantencomputer so deutlich leistungsstärker als normale Rechner. Gleichzeitig machen sie die Systeme aber auch äußerst empfindlich gegenüber äußeren Einflüssen.

Der bedeutendste physikalische Fehlermechanismus ist dabei die sogenannte Dekohärenz. Diese wird so beschreiben, dass in dem ein Qubit durch den Kontakt mit seiner Umgebung „gestört“ wird. Dabei verliert das Qubit seine Fähigkeit, in einem Überlagerungszustand zu bleiben, und verhält sich mehr wie ein klassisches Bit. Dies hat zur Folge, dass Informationen, welche eigentlich im Zustand des Qubits gespeichert waren, verloren gehen. Diese Art von Fehler entsteht nicht durch eine bestimmte Fehlbedienung, sondern ist ein ganz natürlicher Effekt, sobald ein Quantensystem mit seiner Umgebung in Kontakt tritt (Nielsen & Chuang, 2010, Kap. 8.3).

Man unterscheidet dabei zwei Hauptarten von Dekohärenz. Zum einen die Phasendekohärenz und die Amplitudendekohärenz. Bei der Phasendekohärenz bleibt das Qubit grundsätzlich im selben Zustand. Zum Beispiel 
∣0⟩ oder ∣1⟩,
aber die sogenannte Phase zwischen den Zuständen verändert sich. Das hat  weitreichende Folgen. Die Phase ist entscheidend dafür, ob Quanteninterferenzen funktionieren, also ob die verschiedenen Zustände eines Qubits sinnvoll zusammenwirken können. Ist die Phase gestört, kann der Quantencomputer falsche oder gar keine Ergebnisse mehr liefern.

Amplitudendekohärenz dagegen beschreibt eine Veränderung des Energiezustands des Qubits. Das tritt häufig dann auf, wenn das Qubit aus einem angeregten Zustand wieder in seinen Grundzustand „zurückfällt“. Dieser Vorgang entspricht einem Energieverlust. Dies passiert zum Beispiel durch spontane Emission von Strahlung oder durch Wärmeabgabe an die Umgebung. In vielen physischen Systemen, etwa bei supraleitenden Qubits oder bei Ionenfallen, ist dieser Prozess besonders häufig zu beobachten (Rieffel \& Polak, 2011, Kap. 10.4.4).

Doch Dekohärenz ist nicht die einzige Fehlerquelle in Quantencomputern. Weitere Ursachen liegen in verschiedenen Rauschprozessen, also zufälligen oder unkontrollierten Störungen aus der Umgebung. Dazu zählen zum Beispiel thermisches Rauschen, das durch Temperaturunterschiede entstehen kann, oder elektromagnetische Fluktuationen, etwa von Stromleitungen, elektronischen Geräten oder sogar Mobilfunkstrahlung. Auch mechanische Vibrationen. Diese können unteranderem durch Pumpen oder Bewegungen im Kühlapparat entstehen. Diese können sich auf die empfindlichen Qubits übertragen.

Ein besonderes Problem ist die unzureichende Isolation der Qubits. Idealerweise sollte ein Qubit vollständig von seiner Umgebung abgeschottet sein. In der Realität ist das kaum möglich. Selbst im Vakuum oder bei extrem tiefen Temperaturen gibt es noch mikroskopisch kleine Wechselwirkungen mit anderen Teilchen, Materialien oder Defekten. Manche Materialien enthalten zum Beispiel sogenannte Zwei-Niveau-Systeme, die mit den Qubits interagieren können und als zusätzliche Störquellen fungieren. Auch kosmische Strahlung oder Magnetfelder aus der Umgebung können Störungen verursachen (Tutschku et al., 2023).

Aus all diesen Gründen ist es extrem wichtig, die Quantenhardware so gut wie möglich gegen äußere Einflüsse zu schützen. Das geschieht zum Beispiel durch ultratiefe Temperaturen im Millikelvin-Bereich, durch spezielle Abschirmungen gegen elektromagnetische Wellen, durch mechanische Entkopplung der Geräte und durch den Einsatz besonders reiner Materialien ohne Defekte. Trotzdem lässt sich nie ganz vermeiden, dass äußere Einflüsse auf das System wirken. Daher ist es essenziell, zusätzlich auf softwareseitige Verfahren zur Fehlerkorrektur zu setzen – etwa durch den Einsatz logischer Qubits oder redundanter Kodierungen.

Quantenfehler sind also ein Zusammenspiel aus unvermeidbaren physikalischen Prozessen, technologischen Grenzen und Umwelteinflüssen. Sie zu verstehen und zu kontrollieren ist eine der größten Herausforderungen beim Bau und Betrieb eines funktionierenden Quantencomputers.

\subsection{Klassifizierung von Quantenfehlern}
Die Klassifikation von Quantenfehlern stellt eine essenzielle Grundlage für die Entwicklung robuster Fehlerkorrekturverfahren im Quantencomputing dar. Aufgrund der Superposition können Fehler nicht nur den Zustand selbst, sondern auch die relative Phase, Amplitude oder die Kohärenzeigenschaften eines Qubits beeinflussen. Die gebräuchlichste Einteilung unterscheidet zwischen diskreten, kontinuierlichen, zufälligen und systematischen Fehlern sowie zwischen Einzelqubit- und Mehrqubitfehlern.

Die Diskrete Fehlerarten sind die Pauli-Fehler. Die einfachste und zugleich mathematisch fundamentale Klasse von Quantenfehlern basiert auf den sogenannten Pauli-Fehlern, benannt nach den drei Pauli-Matrizen 
X, Y und Z. Diese Operatoren stellen Basisoperationen im zweidimensionalen Qubit-Zustandsraum dar und bilden eine vollständige Fehlerbasis für beliebige Ein-Qubit-Störungen

Der Bit-Flip-Fehler (X-Fehler): Ein Bit-Flip entspricht einer Umkehrung des Qubitzustands von ∣0⟩ nach ∣1⟩ oder umgekehrt. Dieser Fehler kann zum Beispiel durch thermische Anregung entstehen, etwa wenn ein Qubit spontan aus dem Grundzustand in einen angeregten Zustand übergeht. In Formeln entspricht dies der Anwendung des X-Operators: 
X∣0⟩=∣1⟩ (Nielsen & Chuang, 2010, S. 449).

Der Phase-Flip-Fehler (Z-Fehler): Beim Phase-Flip verändert sich die relative Phase eines Qubits. So wird etwa der Zustand 
∣+⟩=(∣0⟩+∣1⟩)/ 
2
​	
  durch einen Z-Operator in  ∣−⟩=(∣0⟩−∣1⟩)/ 2
​	
 überführt. Dieser Fehler beeinträchtigt Interferenzeffekte und ist besonders kritisch für Algorithmen, die auf kohärenter Phasenevolution basieren (ebd., S.449).
 
Kombinierte Bit- und Phase-Fehler (Y-Fehler): Der Y-Operator kombiniert Bit- und Phase-Flip gleichzeitig: Y=iXZ. Solche Fehler treten häufig bei komplexeren physikalischen Störungen auf, wie zum Beispiel durch Crosstalk zwischen Qubits oder bei nichtlokalen Umwelteinflüssen (Rieffel & Polak, 2011, S.375).

Diese drei Operatoren bilden eine vollständige Fehlerbasis für Ein-Qubit-Störungen. Jeder beliebige Fehler auf einem Qubit kann mathematisch als Linearkombination von 
I, X, Y und Z dargestellt werden. Diese Eigenschaft ist von zentraler Bedeutung für den Aufbau von Quantenfehlerkorrekturcodes wie dem Shor-Code oder dem Steane-Code, welche gezielt auf Pauli-Fehler reagieren (Nielsen & Chuang, 2010, S.449–451).


Neben diskreten Fehlern gibt es auch kontinuierliche Fehlerprozesse, die sich nicht durch einzelne Pauli-Operatoren darstellen lassen, sondern über sogenannte Quantenkanäle modelliert werden. Die zwei wichtigsten Beispiele sind Amplitudendämpfung und die Phasendämpfung.

Amplitudendämpfung: Diese beschreibt den Verlust von Energie durch das Qubit, etwa infolge spontaner Emission oder thermischer Relaxation. Der Zustand ∣1⟩ relaxiert dabei mit einer gewissen Wahrscheinlichkeit 
γ in den Grundzustand ∣0⟩. Die Kraus-Darstellung des Amplitudendämpfungskanals lautet:
 BILD EINFÜGEN 
(Nielsen & Chuang, 2010, S.376–377)

Phasendämpfung (Dephasing): Hierbei geht keine Energie verloren, aber Kohärenz. Die Off-Diagonal-Elemente der Dichtematrix, also die, die Superpositionen erzeugen, werden gedämpft. Dies geschieht durch Umwelteinflüsse wie Fluktuationen elektromagnetischer Felder. In der Praxis ist Phasendämpfung oft die dominante Dekohärenzquelle (Rieffel & Polak, 2011, S.374–375).
Die Modellierung dieser kontinuierlichen Prozesse ist entscheidend für die realistische Simulation von Quantensystemen und wird durch Kraus-Operatoren und Lindblad-Gleichungen mathematisch beschrieben.

Quantenfehler lassen sich nicht nur nach ihrer physikalischen Form, sondern auch nach ihrer Entstehungsart klassifizieren. Hierzu zählen zufällige Fehler und systematische Fehler.

Zufällige Fehler (stochastisch): Diese Fehler entstehen unvorhersehbar durch thermisches Rauschen, Photonen-Einfälle, kosmische Strahlung oder spontane Kopplungen an die Umgebung. Sie sind oft kurzzeitig, unkorreliert und lassen sich statistisch modellieren, z. B. über Randomized Benchmarking (Tutschku et al., 2023, S. 50).
Systematische Fehler: Diese beruhen auf wiederholbaren, deterministischen Einflüssen, wie falscher Kalibrierung von Pulssequenzen, ungenauen Gatterzeiten oder falsch modellierter Kopplung. Solche Fehler summieren sich über Zeit auf und können ganze Rechenprozesse verfälschen. Sie sind schwerer zu korrigieren, da sie nicht durch Mittelwertbildung „herausrauschen“ (Google Quantum AI, 2024, S. 3–4).
Die Unterscheidung ist für die Architekturplanung essenziell, da systematische Fehler oft durch verbesserte Hardware, kontrollierte Steuerungen oder modellgestützte Korrektur vermieden werden können.

Ein wachsendes Problem in größeren Quantenprozessoren ist das Auftreten von korrelierten Fehlern. Solche Fehler beeinflussen mehrere Qubits gleichzeitig und können durch gemeinsame Störquellen oder durch ungewollte Kopplungen entstehen. Anders als unabhängige Fehler breiten sie sich nicht lokal aus, sondern erzeugen komplexe Fehlerbilder, die von klassischen Fehlerkorrekturmodellen oft nicht erfasst werden.


\subsection{Hardwarebedingte Fehler und Systematische Grenzen}
Neben den oben beschriebenen Fehlermechanismen spielen auch hardwarebedingte Fehler eine zentrale Rolle in der Fehlertheorie des Quantencomputings. Diese entstehen durch Unzulänglichkeiten in der technischen Umsetzung der Steuerung, der Auslese und der physikalischen Qubit-Architektur. In realen Quantenprozessoren, wie supraleitenden Schaltkreisen, Ionenfallen oder spinbasierten Systemen, sind diese Fehlerquellen ein wesentlicher limitierender Faktor für die Skalierbarkeit und Zuverlässigkeit der Systeme.

Ein wesentliches Problem sind fehlerhafte Gatteroperationen. Idealerweise sollen Quantenlogikgatter wie das CNOT- oder Hadamard-Gate eine definierte unitäre Transformation auf den Zustand der Qubits ausführen. In der Praxis weichen die implementierten Operationen von diesem Ideal ab. Ursachen dafür sind unter anderem folgende Fehler:

Timing-Fehler: Ungenauigkeiten in der Pulslänge oder -frequenz führen zu nicht vollständig abgeschlossenen Rotationen.
Crosstalk: Eine ungewollte Kopplung zwischen benachbarten Qubits oder Steuerleitungen kann zu Störungen führen, die nicht auf das Zielqubit beschränkt bleiben.
Nichtlinearitäten in der Pulselektronik, die insbesondere bei stark skalierten Systemen auftreten.
Diese Gatterfehler akkumulieren sich über tiefe Schaltkreise hinweg und führen dazu, dass die logische Fehlerrate mit zunehmender Schaltungstiefe exponentiell ansteigt (Nielsen & Chuang, 2010, S.390–391; Tutschku et al., 2023, S.51). Selbst bei modernen Quantenprozessoren liegt die mittlere Gatterfidelität, also die Wahrscheinlichkeit, mit der ein Gatter korrekt ausgeführt wird, typischerweise nur bei 99–99,9% (Google Quantum AI, 2024, S.3).

Ein weiterer kritischer Punkt sind Messfehler. Die Messung eines Qubits erfolgt meist über eine Verstärkung und Auswertung eines quantenmechanischen Signals (z.B. dispersive Kopplung an einen Resonator bei supraleitenden Qubits oder Fluoreszenz in Ionenfallen). Dabei kann es zu Fehlern kommen durch:

Rauschen in der Verstärkerschaltung
Überlappung von Signalverteilungen für 
∣0⟩ und ∣1⟩
Verzögerungen oder Sättigungseffekte in der Ausleseelektronik
Diese Fehler können zu falscher Interpretation des Qubit-Zustands führen und beeinträchtigen damit die Zuverlässigkeit von Algorithmen und insbesondere von Fehlerkorrekturcodes, die sich auf korrektes Auslesen von Syndromen verlassen (Nielsen & Chuang, 2010, S.414; Tutschku et al., 2023, S.50).

Die Initialisierung von Qubits ist ein notwendiger erster Schritt jeder Quantenberechnung. Ziel ist es, alle Qubits zuverlässig in den Grundzustand ∣0⟩ zu setzen. In der Praxis gelingt dies nicht immer mit perfekter Genauigkeit. Ursachen sind unter anderem:

Thermische Anregung bei unzureichender Kühlung
Unvollständiges Relaxieren aus dem angeregten Zustand
Restkopplungen an Resonatoren oder Nachbarqubits
Fehlerhafte Initialisierungen wirken sich direkt auf die korrekte Ausführung der ersten Gatteroperationen aus und können sich durch den Schaltkreis fortpflanzen (Rieffel & Polak, 2011, S.378; Google Quantum AI, 2024, S.3).

In skalierbaren Architekturen, vor allem in zweidimensionalen Gittern supraleitender Qubits, sind nicht alle Qubits direkt miteinander verbunden. Diese eingeschränkte Konnektivität führt dazu, dass logische Operationen, die Qubits an weit entfernten Positionen betreffen, durch zusätzliche SWAP-Gatter oder Relaisoperationen realisiert werden müssen. Diese zusätzlichen Schritte erhöhen nicht nur die Rechenzeit, sondern fügen dem System zusätzliche Fehlerquellen hinzu. Besonders kritisch ist dies in tiefen Schaltkreisen oder bei komplexen Algorithmen wie QAOA oder QFT (Tutschku et al., 2023, S.49).

Alle oben genannten Fehlerarten werden zusätzlich durch Kalibrierfehler verstärkt. Diese entstehen, wenn die physikalischen Parameter des Systems, wie Frequenzen, Kopplungsstärken, Pulsamplituden oder Dämpfungsraten nicht exakt bekannt oder nicht stabil sind. Selbst kleine Abweichungen in der Kalibrierung können über viele Rechenzyklen hinweg zu signifikanten Abweichungen im Endergebnis führen.

Studien zeigen, dass Kalibrierfehler insbesondere bei nicht regelmäßig durchgeführten Rekalibrierungszyklen zu systematischer Fehlentwicklung der Qubit-Zustände führen (Google Quantum AI, 2024, S.4–5).

\subsection{Quantifizierung und Modellierung von Quantenfehlern}
Die präzise Modellierung und Quantifizierung von Fehlern in Quantencomputern ist essenziell, um deren Auswirkungen auf die Informationsverarbeitung zu verstehen und wirksame Fehlerkorrekturstrategien zu entwickeln. Aufgrund der Besonderheiten quantenmechanischer Systeme, insbesondere Superposition, Nichtklassikalität und Unitarität, ist eine adäquate Beschreibung von Fehlerprozessen nur durch formalisierte mathematische Modelle möglich, die über klassische Störmodelle weit hinausgehen. In der Theorie offener Quantensysteme werden solche Fehler durch Quantenkanäle beschrieben, deren Wirkung über sogenannte Krausoperatoren modelliert wird.

Fehlerkanäle und Krausoperatoren

Ein Quantenkanal beschreibt die Wirkung eines Umgebungseinflusses auf ein Quantensystem, das sich dadurch in einen neuen Zustand überführt. Formal wird ein Quantenkanal (E) durch eine completely positive, trace-preserving (CPTP) Abbildung auf dem Raum der Dichtematrizen definiert. Die Wirkung eines solchen Kanals auf einen Zustand ρ wird durch die Kraus-Zerlegung beschrieben:

FORMEL BILD EINFÜGEN
Hierbei sind  E k E  k die Krausoperatoren, die jeweils eine mögliche Fehlerwirkung modellieren. Diese Darstellung ist besonders mächtig, da sie sowohl diskrete als auch kontinuierliche Fehlermechanismen integriert. Je nach Art des Rauschprozesses ergeben sich unterschiedliche konkrete Kanalformen:
Amplitudendämpfungskanal: Modelliert Energieverluste wie spontane Emission. Dabei relaxiert das Qubit mit Wahrscheinlichkeit 
γ vom angeregten Zustand ∣1⟩ in den Grundzustand ∣0⟩. Die zugehörigen Krausoperatoren lauten:

(BILD EINFÜGEN DER FORMEL)
(Nielsen & Chuang, 2010, S.377)
Depolarisationskanal: Beschreibt symmetrisches Rauschen, bei dem mit Wahrscheinlichkeit p ein zufälliger Pauli-Fehler (X, Y, Z) auftritt:
(BILD VOPN FORMEL EINFÜGEN)
 (XρX+YρY+ZρZ)
(ebd., S.376)
Diese Modelle bilden die Grundlage für theoretische Fehleranalysen und sind integraler Bestandteil moderner Simulationen von Quantenalgorithmen.

Fehlerraten

Neben der qualitativen Modellierung von Fehlerprozessen ist deren quantitative Beschreibung durch Fehlerraten entscheidend. Die Fehlerrate gibt an, mit welcher Wahrscheinlichkeit eine Operation – z.B. ein Quanten-Gatter oder eine Messung, fehlerhaft ausgeführt wird. Sie stellt eine wichtige Benchmarkgröße dar, insbesondere im Kontext der Skalierbarkeit und Fehlerkorrektur.

Typische Fehlerraten in heutigen NISQ-Systemen (Noisy Intermediate-Scale Quantum):

Ein-Qubit-Gatterfehler: ca. 10−4 bis 10−3
 
Zwei-Qubit-Gatterfehler: ca. 
10−3 bis 10−2
 
Messfehler: typischerweise zwischen  0.5% und 1% je nach Architektur (Tutschku et al., 2023, S. 51–52)
Eine häufig verwendete Metrik ist die average gate fidelity, welche angibt, wie nahe die tatsächlich implementierte Gatteroperation im Mittel an der idealen, unitären Operation liegt.

Fehlervermessung: Methoden zur Fehlerquantifizierung

Um Fehlermodelle nicht nur theoretisch anzusetzen, sondern empirisch zu validieren und zu quantifizieren, kommen in der experimentellen Quanteninformatik verschiedene Messverfahren zum Einsatz:

Quantum Process Tomography (QPT): Ermöglicht die vollständige Rekonstruktion des Quantenkanals durch systematische Zustandsvorbereitungen und Messungen. Aufgrund des exponentiellen Aufwands bei wachsender Qubit-Anzahl ist QPT auf wenige Qubits beschränkt (Nielsen & Chuang, 2010, S.388).
Randomized Benchmarking (RB): Verwendet zufällig gewählte Gatterfolgen aus einer Gruppe (typischerweise Clifford-Gruppenoperationen), um statistisch robuste Aussagen über mittlere Fehlerraten zu treffen. Der Vorteil von RB liegt in seiner Unempfindlichkeit gegenüber SPAM-Fehlern (State Preparation and Measurement) (Tutschku et al., 2023, S.52).
Cross-Entropy Benchmarking (XEB): Ein Verfahren, das besonders bei großen supraleitenden Qubit-Systemen wie Sycamore oder Bristlecone zum Einsatz kommt. Es vergleicht die gemessene Ausgabeverteilung eines komplexen Quantenkreises mit der theoretisch erwarteten Verteilung aus klassischer Simulation. Die Übereinstimmung (Cross-Entropy) gibt Rückschluss auf die Gesamteffizienz und Fehlerhäufigkeit des Systems (Google Quantum AI, 2024, S.3–4)

\subsection{Auswirkungen von Fehlern auf Quantenalgorithmen und Systemarchitektur}
Fehler in Quantencomputern wirken sich nicht isoliert auf einzelne Operationen aus, sondern haben umfassende Konsequenzen für die Stabilität, Zuverlässigkeit und Effizienz ganzer Quantenalgorithmen. Aufgrund der spezifischen Eigenschaften von Quanteninformation, vor allem der Verwendung von Superposition und Interferenz, kann bereits ein einziger Fehler in einem Quantenregister dazu führen, dass das Rechenergebnis vollständig verfälscht oder unbrauchbar wird. Das No-Cloning-Theorem schließt zusätzlich aus, dass fehleranfällige Quanteninformationen einfach dupliziert oder redundant gespeichert werden können (Nielsen & Chuang, 2010, S.466). Damit sind Fehlerkorrektur und Systemtoleranz nicht nur Ergänzungen, sondern fundamentale Voraussetzungen für zuverlässige Quantenverarbeitung.

Rechenabbrüche und Fehlerakkumulation

In praktisch ausgeführten Algorithmen, wie bei Grover’s Algorithmus, Shor’s Faktorisierungsverfahren oder variationalen Methoden wie VQE (Variational Quantum Eigensolver), kann ein einzelner Fehler eine Verkettung von Folgefehlern auslösen. Dies führt dazu, dass die Zustandsamplituden falsche Interferenzen erzeugen oder sich logisch fehlerhafte Zustände im System ausbreiten. Besonders problematisch ist dabei die Akkumulation von Fehlern über viele Rechenzyklen hinweg. Bei jedem Gatteraufruf, jeder Qubit-Wechselwirkung und jeder Messung steigt die Wahrscheinlichkeit, dass sich ein Fehler einschleicht (Tutschku et al., 2023, S.53).

Rechenabbrüche treten dann auf, wenn die Fehlerzahl oder ihre Art eine erfolgreiche Fortsetzung oder Interpretation des Algorithmus unmöglich macht. Ursachen hierfür sind unter anderem:

Timing-Fehler oder falsche Pulslängen bei Zwei-Qubit-Gattern, wodurch die beabsichtigte Kopplung nicht korrekt implementiert wird.
Überschreiten der Kohärenzzeiten T1 (Relaxation) und T2 (Dephasierung), was in tief verschachtelten Algorithmen fast unausweichlich ist, wenn keine Fehlerkorrektur erfolgt.
Leckagefehler, bei denen ein Qubit aus dem definierten Zustandsraum (z.B. ∣0⟩, ∣1⟩) in einen höheren Energiezustand übergeht, oder Crosstalk, bei dem Qubit A durch das Ansteuern von Qubit B unbeabsichtigt beeinflusst wird (Tutschku et al., 2023, S.53; Google Quantum AI, 2024, S.3).

Fehlerbudget und Fehlertoleranz
Zur planvollen Kontrolle dieser Risiken wird in der Architekturentwicklung ein sogenanntes Fehlerbudget verwendet. Dieses definiert, wie viele und welche Arten von Fehlern ein System in einer gegebenen Operationstiefe oder Zeitdauer tolerieren kann, ohne dass die logische Konsistenz der Berechnung verloren geht. Entscheidend dafür ist die Fehlerschwelle (fault-tolerance threshold), also die maximale physikalische Fehlerrate, bei der eine effektive Fehlerkorrektur noch möglich ist.

Liegt die effektive Fehlerrate pro Gatteroperation oder Qubit unterhalb dieser Schwelle, so kann durch wiederholte Korrekturzyklen ein logisch fehlerfreier Zustand aufrechterhalten werden. Wird sie überschritten, kann auch ein noch so ausgeklügelter Fehlerkorrekturcode das System nicht mehr retten (Nielsen & Chuang, 2010, S.452; Preskill, 1998, S.226).

Beispiele für Fehlerschwellen:

Surface Codes: Schwelle bei ca. 1% pro Gatteroperation, gelten als besonders robust, da sie nur lokale Interaktionen benötigen (Google Quantum AI, 2024, S.2–4).
Steane- und Bacon-Shor-Codes: Schwelle bei etwa 10 −3, dafür mit höherem logischem Overhead (Rieffel & Polak, 2011, S.393–395).
Um unterhalb dieser Schwelle zu operieren, ist ein signifikanter Hardware-Overhead erforderlich. Für ein einziges logisch geschütztes Qubit werden, je nach verwendetem Code und Fehlertoleranz, zwischen 50 und über 1.000 physikalische Qubits benötigt (Tutschku et al., 2023, S.53). Diese Relation bestimmt maßgeblich die Skalierungsgrenzen aktueller Quantenprozessoren.

Architekturstrategien für Fehlertoleranz

Die Gestaltung der Quantenhardware und -architektur muss daher darauf ausgerichtet sein, mit einer begrenzten Anzahl physikalischer Qubits möglichst effektive Fehlerresistenz zu gewährleisten. Dazu gehören:

Fehlertolerante Layouts, z.B. zweidimensionale Qubit-Gitter mit lokaler Nachbarschaftsstruktur (wie im Surface Code), die einfache Syndrome-Messung und Korrektur ermöglichen.
On-Chip-Feedbacksysteme, die Fehler während der Berechnung in Echtzeit detektieren und sofort korrigierend eingreifen können – etwa durch Mikrocontroller oder FPGA-Logik in der Steuerungselektronik.
Adaptives Routing, das es erlaubt, defekte oder fehlerhafte Qubits dynamisch zu umgehen, ohne die gesamte Berechnung zu unterbrechen (Google Quantum AI, 2024, S.3–5).
Zukünftige Entwicklungen konzentrieren sich zudem auf intelligente Dekodierer, die mithilfe maschinellen Lernens in der Lage sind, nicht nur klassische, sondern auch korrelierte und systematische Fehler zu erkennen und zu kompensieren.


\section{Grundprinzipien in Quantenfehlerkorrektur}\label{chap:QEC2}
Die Quanten-Fehlerkorrektur (Quantum Error Correction, QEC) stellt eine zentrale Voraussetzung für den praktischen Einsatz von Quantencomputern dar. Aufgrund der hohen Störanfälligkeit von Qubits und der Unmöglichkeit, Quantenzustände direkt zu kopieren oder verlustfrei zu messen, bedarf es spezieller Konzepte, um Quantensysteme vor Dekohärenz und Rechenfehlern zu schützen.

Im Folgenden werden sechs theoretische Grundprinzipien vorgestellt, die das Fundament fehlertoleranter Quantenarchitekturen bilden. Diese Prinzipien definieren sowohl die strukturellen Anforderungen an Fehlerkorrekturverfahren als auch zentrale Mechanismen zu ihrer technischen Umsetzung:

\begin{itemize}
    \item Prinzip der Fehlerkennung ohne Zustandsmessung
    \item Prinzip der redundanten Informationsverteilung
    \item Prinzip der Unterscheidbarkeit von Fehlern
    \item Prinzip der reversiblen Fehlerkorrektur
    \item Prinzip der lokalen Fehlerbegrenzung
    \item Fehlerschwellenprinzip
\end{itemize}

Diese Prinzipien greifen konzeptionell ineinander und schaffen gemeinsam die Voraussetzung dafür, dass selbst bei wiederholtem Auftreten physikalischer Fehler eine stabile und skalierbare Quanteninformationsverarbeitung möglich bleibt.

\subsection{Prinzip der Fehlerkennung ohne Zustandsmessung}
Eines der zentralen Konzepte der Quanten-Fehlerkorrektur ist, dass man Fehler erkennen kann, ohne die kodierte Quanteninformation zu messen und somit zu zerstören. In klassischen Systemen könnten wir etwa redundante Bitmuster verwenden, um Fehler aufgrund direkter Messung zu erkennen und zu korrigieren. In einem quantenmechanischen System ist es jedoch nicht so einfach möglich, eine Messung durchzuführen, da ein solcher Schritt den Superpositionszustand des Qubits kollabieren ließe und damit die gespeicherte Information unter Umständen irreversibel zerstören könnte. \cite{nielsen_quantum_2010}

Um das zu umgehen, führt man im Bereich der Quanten-Fehlerkorrektur sogenannte Syndrommessungen durch. Dabei handelt es sich um Messungen, bei denen nicht der Zustand des Qubits direkt gemessen wird, sondern nur Information über das Eintreten von Fehlern. Dieses Prinzip lässt sich mit einer Prüfungsaufsicht vergleichen: Die Aufsicht erkennt zwar, wenn ein Prüfling abschreibt und somit einen Täuschungsversuch begeht. Sie kennt jedoch nicht den Inhalt der Klausur. In der Quantenfehlerkorrektur misst man hierbei den sogenannten Stabilisator-Operator, der zu einem speziellen Quanten-Fehlerkorrekturcode gehört. Diese Operatoren definieren eine Menge erlaubter Zustände – sogenannte +1-Eigenzustände der Stabilizer-Gruppe. Tritt ein Fehler auf, so verändert sich die Eigenwertstruktur des Stabilisators. Ein Wechsel zwischen \(+1\) und \(-1\) ist zum Beispiel ein  Zeichen dafür, dass ein bestimmter Fehler passiert. \cite[Seite 444-446]{nielsen_quantum_2010}

Die Syndrommessung erfolgt in der Praxis meist so, dass man Hilfsqubits (Ancillae) einführt, auf denen man den Stabilisator wirken lässt und so Informationen darüber erhält, was der Stabilizer auf den gekodeten Zustand angewendet hätte, ohne hierbei den gekodeten Zustand zu verändern. So lassen sich ernsthaft Fehler zum Beispiel durch die sogenannten kontrollierten Operatoren auch zu dem Hilfs-Qubit "weiterleiten" und damit dort auslesen. Dabei bleibt die gekodete Quanteninformation erhalten. Man nutzt dies dazu, das sogenannte Fehlersyndrom zu bestimmen um dann gezielt unitäre Korrektur-Operatoren durchzuführen (siehe Kapitel \ref{chap:QEC3}.). \cite[Seite 444-446]{nielsen_quantum_2010}

Ein Schlüsselelement des fehlertoleranten Quantencomputings ist die Möglichkeit, einen Quantenzustand messen zu können, ohne dass er in seinen Ursprungszustand zu zwei Drittelwahrscheinlichkeit kollabiert. Dank dieser Fähigkeit können Codes konstruiert werden, welche die Integrität von Quanteninformation auch dann noch lange Zeit sicherstellen, wenn wiederkehrende Fehler laufend erkannt und ausgebessert werden.

\subsection{Prinzip der redundanten Informationsverteilung}
Quanten-Fehlerkorrektur beruht auf zentraler Idee, Quantenzustände vor Fehlern zu schützen, indem Redundanz eingefügt wird. Das bedeutet, ein einzelnes logisches Qubit wird nicht durch ein einziges physikalisches Qubit dargestellt, sondern auf mehrere physikalische Qubits verteilt. Genau durch diese zusätzliche Kodierungskapazität können Fehler eintreten, ohne dass dabei die logische Quanteninformation verloren geht. Ein passendes Beispiel ist eine wichtige Nachricht, die Sie einem Freund dreimal schicken – per E-Mail, per WhatsApp und als Sprachnachricht. Selbst wenn eine Variante verloren geht oder fehlerhaft ist, kann er den Inhalt rekonstruieren. Genauso wird in der Quanten-Fehlerkorrektur ein logisches Qubit auf mehrere physikalische Qubits verteilt, um Fehler zu überstehen.

Im Unterschied zur klassischen Fehlerkorrektur, bei der Redundanz verwendet wird (z.B. dreifache Paritätsbits), muss in der Quantenwelt solch ein Ansatz jedoch mit Bedacht gewählt werden. Denn das No-Cloning-Theorem verhindert, dass man auch einfach unbekannte Quantenzustände duplizieren kann. \cite{nielsen_quantum_2010} Stattdessen wird die Kodierung mithilfe einer kohärenten Verschränkung mehrerer Qubits erreicht, sodass die Quanteninformation nicht in einem einzelnen Qubit, sondern in einem höherdimensionalen Unterraum des Gesamtsystems gespeichert wird.

Ein bekanntes Beispiel ist der Shor-Code, bei dem ein logisches Qubit auf neun physikalische Qubits abgebildet wird. Dieser Code schützt sowohl vor Bit-Flip- als auch vor Phase-Flip-Fehlern, indem er erst eine dreifache Kopie erstellt, um Bit-Flips zu entdecken, dann jede dieser Kopien nochmals mit einer Phase-Flip-Korrektur kodiert. \cite{shor_scheme_1995}
Ebenso gut ist der Steane-Code, der sieben Qubits zum Darstellen eines logischen Qubits verwendet und den klassischen (7, 4, 3)-Hamming-Code verwendet. \cite{steane_error_1996}

Mit Hilfe einer solchen Kodierung kann Quanteninformation in einem sogenannten Code-Raum abgelegt werden, der durch eine Gruppe von Stabilisator-Operatoren definiert ist. Tritt ein Fehler auf, so wird der Zustand in einen orthogonalen Unterraum verschoben, der außerhalb des Code-Raums liegt. Dieses Umschichten lässt sich über die Syndrommessung identifizieren – und zwar ohne, dass der ursprüngliche Zustand vermessen wird. Wesentlich ist, dass die Redundanz dabei hilft, Fehler zu lokalisieren und Korrekturoperationen durchzuführen, ohne die Quantenkohärenz zu stören.

Die Redundanz durch Kodierung ist somit die Voraussetzung für alle Quanten-Fehlerkorrekturverfahren: Denn nur indem man Information auf viele Qubits verteilt, kann man dem System Fehler verzeihen, ohne dass es aus funktionalem Zerfallen lässt.

\subsection{Prinzip der Unterscheidbarkeit von Fehlern}
Ein entscheidendes Konzept in der Quantenfehlerkorrektur ist, dass Fehler auf einfache Weise voneinander unterscheidbar sein müssen. Nur wenn Fehler Zustände auf unterschiedliche Weise beschädigen, hat man eine Chance, festzustellen welcher der Fehler aufgetreten ist, mit dem Ziel diesen gezielt zu korrigieren ohne die Quanteninformation zu verlieren. Stellen Sie sich vor, Sie bekommen zwei Fehlermeldungen auf Ihr Handy - eine lautet "kein Netz", die andere "Flugmodus aktiviert". Nur wenn Sie beide unterscheiden können, wissen Sie, wie Sie das Problem beheben. Wenn beide Fehlermeldungen gleich heißen würden, hätten Sie Schwierigkeiten das Problem zu lösen. 

In der Sprache der Quanteninformatik bedeutet dies, dass die durch Fehler erstellten Zustände orthogonal zueinander sein müssen, beziehungsweise so aufgebaut, dass man durch Messung eines fehlerhaften Zustands auf den Code-Raum schließen kann. Andernfalls besteht die Gefahr, dass Fehler, die zu gleichem Messausgang führen, nicht unterschieden werden können und die Messung zu einer falschen Korrektur und dem Verlust von Information führt. \cite[Seite 449–451]{nielsen_quantum_2010}

Dieses Konzept wird durch das sogenannte Fehlerkorrekturkriterium mathematisch präzisiert. Für eine Menge von Fehlern \(\left\{E_{i}\right\}\) , die von einem Quantencode korrigiert werden sollen, gilt:
\begin{equation}
    \left\langle\psi_{a}\right| E_{i}^{\dagger} E_{j}\left|\psi_{b}\right\rangle=C_{i j} \delta_{a b}
\end{equation}
für alle Zustände \(
    \left|\psi_{a}\right\rangle,\left|\psi_{b}\right\rangle
\)  im Code-Raum. Hierbei hängt die Konstante \(
    C_{i j}
\) vom Fehler,  nicht jedoch vom kodierten Zustand ab. Dieses Kriterium ist notwendig um sicherzustellen, dass die Wirkung eines Fehlers unabhängig von der gespeicherten Information eindeutig bestimmbar ist.

Aber aufgepasst: Wenn zwei unterschiedliche Fehler denselben Effekt auf den Code haben – also nicht unterscheidbar sind –, kann keine gezielte Korrektur erfolgen. Die Fähigkeit zur Unterscheidung solcher Fehlerzustände ist also die Voraussetzung für \textit{irgendeine} syndrombasierte Fehlerkorrektur. Nur dadurch können beispielsweise Look-Up-Tabellen oder automatische Fehlerkorrekturmechanismen sicher die \textit{einzig mögliche} Korrektur wählen.

Das Prinzip hat übrigens eine Parallele zur Konstruktion von Stabilizer-Codes, bei denen unterschiedliche Fehler unterschiedliche Syndrommuster hinterlassen. Durch geschickte Wahl der Stabilisatorgruppe kann man erreichen, dass in einem Stabilizer-Code alle Fehler innerhalb der Toleranzgrenze eindeutige Syndrome haben. \cite{gottesmann Stabilizer Codes}

\subsection{Prinzip der reversiblen Fehlerkorrektur}
Ein entscheidendes Prinzip der Quanten-Fehlerkorrektur ist es, dass sämtliche eingesetzten Operationen im Fehlerkorrekturprozess umkehrbar sein müssen. Vergleichbar mit dem Schreiben eines Textes in Word, bei dem Sie über STRG + Z den vorherigen Zustand des Textes wiederherstellen können.  In der Quantenmechanik entsprechen sämtliche möglichen zeitlichen Entwicklungen von isolierten Systemen unitären Transformationen – also Operationen, die völlig umkehrbar sind und keine Information vernichten. Diese Forderung gilt es auch für die Fehlerkorrektur durchzusetzen, da eine unwiderrufliche Fehlerkorrektur notwendigerweise den kodierten Quantenzustand zerstören und die Kohärenz der Quanteninformation verlieren würde. \cite[Seite 450-451]{nielsen_quantum_2010}

Anders ausgedrückt: Wenn bei einer Kette von Ereignissen ein Fehler durch das Syndrom erkannt wurde, erfolgt ihre Korrektur durch eine bestimmte, unitäre Operation, welche uns den Fehlerzustand wieder ins Urteil des Codes zurückführt. Ist dieser Zustand erreicht, ist unser Fehler nicht nur korrigiert, sondern auch so, dass wir ihn kohärent rekonstruieren können – und dies ist für weiteres Quantenrechnen und für fehlerresistente Quantenrechner notwendig.

Dieses Erfordernis der Rückkehr zur Ausgangslage besitzt auch eine sehr klare und bequeme theoretische Beschreibung: Das sogenannte Nicht-Trivialitätskriterium für Fehlerkorrektur sagt uns, dass für jede nichttriviale Fehlerwirkung \(E_i\) eine zugehörige Korrekturoperation \(R_i\) existiert, so dass 
\begin{equation}
    R_{i} E_{i}|\psi\rangle=|\psi\rangle \quad \text { für alle }|\psi\rangle \in \mathcal{C}
\end{equation}
Die Fehlererkennung und -korrektur insgesamt bildet somit einen Prozess, bei dem der Quanteninformationsträger lediglich entlang eines Umwegs durch Code-Hilfsraum entlanggeführt wird und die Logikqubit-Effektivität erhalten bleibt, um in der Lage zu sein, auch weitere Quantengatter auf diese anzuwenden.

Darum unterscheidet sich die Quanten-Fehlerkorrektur deutlich z.B. von klassischen Korrekturverfahren, welche nicht unbedingt reversibel sein müssen. In der Quanteninformatik wiederum ist Reversibilität jedoch ein \textbf{fundamentales physikalisches Prinzip}, das sowohl theoretisch als auch bei der Praxis der Implementierung beachtet werden muss.

\subsection{Prinzip der lokalen Fehlerbegrenzung}
Ein weiteres wichtiges Konzept, das hinter der Quanten-Fehlerkorrektur steckt, ist das der Fehlerlokalisierung. Damit ist gemeint, dass ein aufgetretener Fehler am besten so gefunden und behandelt werden kann, dass er sowohl räumlich als auch logisch eng eingegrenzt ist, und dass seine Folgen sich nicht unkontrolliert über das gesamte System ausbreiten. Ein anschauliches Beispiel für dieses Prinzip ist der Austausch einer defekten Glühbirne in einer Wohnung. Tritt ein Fehler – etwa ein Kurzschluss oder das Durchbrennen einer einzelnen Lampe – auf, betrifft dies lediglich den konkreten Leuchtkörper. Die restliche Stromversorgung bleibt davon unberührt. Die Reparatur kann somit gezielt und lokal erfolgen, ohne dass andere Teile des Systems beeinträchtigt oder untersucht werden müssen. Nur wenn ein Fehler örtlich identifiziert und korrigiert werden kann, ist eine skalierbare und robuste Fehlerkorrektur möglich.\cite[Seite 451-452]{nielsen_quantum_2010}

Fehler können in einem Quantencomputer verschiedene Ursprünge haben – z. B. Dekohärenz, Wechselwirkung mit der Umwelt oder Fehler bei Gates. Nach dem Prinzip der Fehlerlokalisierung sollte jeder dieser Fehler anfänglich lediglich eine begrenzte Anzahl physikalischer Qubits beeinflussen. Diese Voraussetzung soll es ermöglichen, mittels sogenannter Syndrommessungen zu erkennen, wo ein Fehler aufgetreten ist, ohne dabei den globalen Quantenzustand zu zerstören.

Dieses Konzept ist eng mit der Verwendung sogenannter lokaler Fehlerkorrektur-Codes wie z. B. topologischen Codes wie dem Surface Code verbunden. Bei diesen Codes sind die Qubits in einer regelmäßigen, zweidimensionalen Gitteranordnung angeordnet, bei der jeder Stabilizer-Operator nur Qubits betrachtet, die benachbart sind. Ein auftretender Fehler produziert ein lokales Syndrom, das sich aus den Nachbarn des fehlerhaften Qubits ergibt. Dadurch kann der Fehler nicht nur gefunden, sondern auch eingegrenzt und so gezielt korrigiert werden. \cite{fowler_surface_2012}

Insbesondere für große, skalierbare Quantenrechner ist die Fähigkeit zur Fehlerlokalisierung essentiell: Denn ohne diese müssten sämtliche Qubits miteinander verschränkt und abgefragt werden, was einen exponentiellen Kontroll- und Ressourcenaufwand bedeuten würde. Bei lokalisierbaren Fehlern hingegen können lineare oder sogar konstante Skalierungsstrategien verfolgt werden – wodurch fehlertolerante Quantencomputer in greifbare Nähe rücken.

Zusammengefasst: Fehlerlokalisierung sorgt dafür, dass weder einzelne Fehler den gesamten Code-Raum beeinträchtigen, noch langfristige Korrekturprozesse destabilisieren. Sie ist damit eine notwendige Voraussetzung für praktische, modulare und skalierbare Quanten-Architekturen.

\subsection{Fehlerschwellenprinzip (Threshhold Theorem)}
Ein fundamentaler Mechanismus, um fehlertolerante Quantencomputer auch technisch umsetzen zu können, ist das Fehlerschwellenprinzip. Dieses besagt, dass sich Quanten-Fehlerkorrektur auf lange Sicht nur dann lohnt, wenn bei den einzelnen Qubits und Operationen die Wahrscheinlichkeit eines physikalischen Fehlers unter einem bestimmten Schwellenwert liegt, der als Fehlerschwelle bezeichnet wird. Wird die Schwelle unterschritten, so können beliebig exakte Rechnungen dadurch durchgeführt werden, dass man die Fehlerkorrektur immer wieder einsetzt – selbst wenn fortwährend physikalische Fehler passieren. Ein anschauliches Beispiel für dieses Prinzip liefert der Alltag mit technischen Geräten: Ein Smartphone, das nur einmal pro Monat abstürzt, bleibt in der Regel gut nutzbar. Tritt der Fehler hingegen alle fünf Minuten auf, wird das Gerät praktisch unbrauchbar – selbst wenn die Software grundsätzlich korrekt arbeitet. Es gibt also eine kritische Grenze, ab der die Häufigkeit der Fehler das Systemverhalten dominiert und keine sinnvolle Nutzung mehr möglich ist.\cite[Seite 454-456]{nielsen_quantum_2010}

Dieses Prinzip wurde im Rahmen des Threshold Theorems auch rigoros bewiesen \cite{aharonov_fault-tolerant_1997}. Das Theorem besagt, dass bei einer hinreichend kleinen Fehlerrate jedes Quanten-Computing mit beliebiger Genauigkeit durchgeführt werden kann – vorausgesetzt, es gibt genügend Redundanz und Korrekturschritte. Oder anders ausgedrückt: wenn Fehler selten genug und die Möglichkeit zur Fehlerkorrektur groß genug sind, dann können Fehler „überlebt“ werden.

Der genaue Wert der Fehlerschwelle hängt vom gewählten Code, der Architektur und der Art der Fehler ab. Er beträgt typischerweise  Wert zwischen \(
    10^{-4}
\)  und \(
    10^{-2}
\) und zählt derzeit Surface Codes zu den robustesten Verfahren. Deren experimentelle Fehlerschwelle liegt bei etwa \(1 \%\) für 
ein Gatemodell. \cite{fowler_surface_2012}

Von zentraler Bedeutung ist das Fehlerschwellenprinzip für die Entwicklung von Quantenhardware und -architektur, da es ermöglicht Fehler unterhalb einer beherrschbaren Schwelle und nicht als absolutes Hindernis aufzufassen. Es ist damit im Grunde eine theoretische Grundlage für das fehlertolerante Quantencomputing: Man kann Rechner auch mit unzuverlässigen Bauteilen aufbauen, solange die Wahrscheinlichkeit von Fehlern unterhalb der Fehlerschwelle liegt.

Zusammenfassend bilden die sechs vorgestellten Prinzipien das konzeptionelle Fundament der Quanten-Fehlerkorrektur. Sie definieren zentrale Anforderungen an die Struktur, Korrekturfähigkeit und Stabilität von Quantensystemen unter realistischen Bedingungen. Während einige Prinzipien – wie etwa die reversiblen Korrekturoperationen oder die Fehlerschwelle – als theoretische Mindestbedingungen gelten, beschreiben andere konkrete Strategien zur Fehlerbehandlung, wie die redundante Informationsverteilung oder die lokale Fehlerbegrenzung. Erst im Zusammenspiel dieser Aspekte wird es möglich, fehleranfällige physikalische Qubits zu robusten logischen Einheiten zusammenzufassen. Wie diese Prinzipien in der Praxis umgesetzt werden – etwa durch Syndrommessung, Stabilizer-Codes oder topologische Architekturen – wird im folgenden Kapitel näher erläutert.

\section{Praktische Realisierung der Fehlerkorrektur}\label{chap:QEC3}

Dieses Kapitel konzentriert sich auf die praktische Umsetzung der Fehlerkorrektur.
Zuerst betrachten wir einige grundlegende Code-Beispiele, die auf den in Kapitel 5.2 erläuterten Prinzipien aufbauen (Abschnitt 5.3.1). Anschließend wird der Fehlerkorrekturzyklus detailliert am Beispiel der vielversprechenden Oberflächencodes dargestellt (Abschnitt 5.3.2). Abschließend werden aktuelle Schwierigkeiten auf dem Weg zu fehlertoleranten Quantencomputern diskutiert und ein Ausblick auf zukünftige Entwicklungen gegeben (Abschnitt 5.3.3). Der Fokus liegt dabei auf Aspekten der praktischen Umsetzung sowie den damit verbundenen Problemen und Lösungen.

\subsection{Anwendung von QEC-Prinzipien: Erste Code-Beispiele}

Wie in Abschnitt 1.1 ausführlich dargestellt wurde, sind Qubits anfällig für diverse
Fehler, und wie in Abschnitt 1.2 erläutert, erfordern die grundlegenden Prinzipien der
Quantenmechanik (wie das No-Cloning-Theorem und das Messproblem) spezielle
Strategien zur Fehlerkorrektur. Anstatt diese Prinzipien hier erneut zu vertiefen,
konzentrieren wir uns darauf, wie sie in ersten, einfachen Fehlerkorrekturcodes
praktisch angewendet werden. Diese Codes verdeutlichen die grundlegende Idee,
logische Information durch Kodierung über mehrere physikalische Qubits und durch
Syndrommessungen zu schützen.
Erste historische Codes demonstrieren diese Prinzipien exemplarisch. Insbesondere betrachten wir den Shor-Code (9-Qubit-Code) und den Steane-Code (7-Qubit-Code) als frühe Beispiele, sowie die einfachen 3-Qubit-Codes, um die Funktionsweise und mathematischen Grundlagen der QEC zu verdeutlichen.
\cite[Seite 427-430]{nielsen_quantum_2010}\\

\paragraph{3-Qubit-Wiederholungscode (Bit-Flip Code):}
Als einfachstes Beispiel dient der 3-Qubit-Wiederholungscode, der gegen Bit-Flips schützt. Hier wird ein logisches Qubit auf drei physische Qubits verteilt, z.\,B.
\[
\lvert 0_L \rangle = \lvert 000 \rangle, \quad \lvert 1_L \rangle = \lvert 111 \rangle.
\]
Durch diese Redundanz kann ein einzelner Bitfehler erkannt werden, ohne den Logikzustand direkt zu messen. Man misst dazu sogenannte Paritätsoperatoren (Stabilisatoren), die nur verraten, ob unter den drei Qubits ein Ungleichgewicht vorliegt, aber nicht, was das logische Bit ist. Konkret prüft man, ob alle drei Qubits gleich sind (\(\lvert 000 \rangle\) oder \(\lvert 111 \rangle\)) oder ob eines abweicht. Diese Syndrommessungen liefern einen Fehlerindikator (z.\,B. welches Qubit anders ist), ohne die Superposition zu zerstören. Anschließend kann durch eine entsprechende Pauli-\(X\)-Operation der Fehler rückgängig gemacht werden.

Der 3-Qubit-Code illustriert somit die Prinzipien der Fehlererkennung und -korrektur: Die Fehler sind durch orthogonale Syndrome unterscheidbar, und die Korrekturoperation (Bit-Flip auf dem identifizierten fehlerhaften Qubit) stellt den ursprünglichen Zustand wieder her (Reversibilität). Allerdings kann dieser einfache Code nur einen Bit-Flip-Fehler korrigieren und versagt, falls zwei oder mehr Qubits gleichzeitig flippen.
\cite[Seite 430-431]{nielsen_quantum_2010}\\

\paragraph{3-Qubit-Phasenwiederholungscode (Phase-Flip Code):}
Analog dazu existiert der Drei-Qubit-Phasenflip-Code, der Phasenfehler korrigieren kann. Hierbei wird der Zustand beispielsweise als
\[
\alpha \lvert{+++}\rangle + \beta \lvert{---}\rangle
\]
kodiert, wobei
\[
\lvert + \rangle = \frac{1}{\sqrt{2}}(\lvert 0 \rangle + \lvert 1 \rangle), \quad
\lvert - \rangle = \frac{1}{\sqrt{2}}(\lvert 0 \rangle - \lvert 1 \rangle).
\]
Durch Anwendung von Hadamard-Transformationen vor und nach einem Kanal, der Phasenfehler verursacht, kann dieser Code Phasenfehler effektiv in Bitfehler umwandeln, die dann analog zum Bitflip-Code korrigiert werden können.
\cite[Seite 430-431]{nielsen_quantum_2010}\\

\paragraph{Shor-Code (9-Qubit-Code):}

Peter Shor entwickelte 1995 den ersten Quantenfehlerkorrekturcode, der beliebige Ein-Qubit-Fehler – Bit-Flip (\(X\)), Phasen-Flip (\(Z\)) oder eine Kombination daraus (\(Y = iXZ\)) – korrigieren kann. Der Code kombiniert zwei einfache Schutzmechanismen: einen gegen Bitflips und einen gegen Phasenflips.

Ein logisches Qubit wird dabei auf neun physikalische Qubits verteilt. Zuerst wird der Zustand dreifach wiederholt, um Bitfehler erkennen zu können. Danach wird jede dieser drei Gruppen so angepasst, dass auch Phasenfehler erkennbar werden. Dadurch entsteht ein codierter Zustand wie:
\[
\lvert 0_L \rangle = \frac{1}{2\sqrt{2}} \left( ( \lvert 000 \rangle + \lvert 111 \rangle )^{\otimes 3} \right),
\quad
\lvert 1_L \rangle = \frac{1}{2\sqrt{2}} \left( ( \lvert 000 \rangle - \lvert 111 \rangle )^{\otimes 3} \right).
\]

Der Code nutzt sogenannte Stabilizer – das sind spezielle Messungen, die anzeigen, ob und wo ein Fehler passiert ist, ohne die gespeicherte Information zu zerstören. So kann man z.\,B. erkennen, auf welchem der neun Qubits ein Fehler aufgetreten ist, und ihn gezielt mit einem Pauli-Operator (\(X\) oder \(Z\)) korrigieren. Da \(Y\)-Fehler eine Kombination aus \(X\) und \(Z\) sind, genügt es, beide einzeln zu korrigieren.

Der Shor-Code hat die Parameter \([[9,1,3]]\): Er speichert ein logisches Qubit in neun physischen und hat eine Distanz von 3. Das bedeutet: Solange nicht mehr als zwei Qubits gleichzeitig fehlerhaft sind, kann man den ursprünglichen Zustand eindeutig erkennen und im Fall eines einzelnen Fehlers korrekt wiederherstellen.

Auch wenn der Code in der Praxis aufwendig ist, war er ein Meilenstein: Er zeigte erstmals, dass sich Quanteninformation durch geschickte Kodierung und Messung zuverlässig schützen lässt.\\

\paragraph{Steane-Code (7-Qubit-Code):}

Andrew Steane entwickelte einen effizienteren Quantenfehlerkorrekturcode, der wie der Shor-Code beliebige Ein-Qubit-Fehler korrigieren kann – jedoch mit nur 7 physikalischen Qubits. Der Steane-Code ist ein Beispiel eines CSS-Codes (Calderbank-Shor-Steane), der zwei klassische Codes kombiniert: einen gegen Bitfehler (\(X\)) und einen gegen Phasenfehler (\(Z\)).

Der Steane-Code ist dem Shor-Code in der Fehlerkorrektur gleichwertig, benötigt aber weniger Qubits. Außerdem lassen sich viele logische Operationen direkt auf den einzelnen Qubits ausführen, was die Umsetzung fehlertoleranter Quantenschaltungen erleichtert. Er ist damit ein kompaktes und praxisnahes Beispiel für die Prinzipien der Quantenfehlerkorrektur.\\

\paragraph{Ausblick auf fortgeschrittene Codes:}

Die bisher vorgestellten Codes – wie der Drei-Qubit-Code, der Shor-Code und der Steane-Code – zeigen, wie sich einzelne Bit- oder Phasenfehler gezielt erkennen und korrigieren lassen. Während sie bereits vollständige Quantenfehlerkorrektur auf einem logischen Qubit ermöglichen, stoßen sie bei wachsender Qubit-Anzahl und praktischer Realisierung an ihre Grenzen.

Für den Aufbau skalierbarer Quantencomputer werden daher weiterentwickelte Codefamilien benötigt, die neben der Korrektur beliebiger Fehler auch architektonische Vorteile bieten – etwa durch lokale Wechselwirkungen und effiziente Layouts. Ein prominentes Beispiel dafür sind die Oberflächencodes, die im folgenden Abschnitt behandelt werden. Sie beruhen auf denselben Grundprinzipien, setzen jedoch auf topologische Strukturen und fehlertolerante Operationen im Gitter.

Die Notwendigkeit solcher Verfahren ergibt sich unmittelbar aus den quantenmechanischen Eigenschaften, die in den Abschnitten 1.1 und 1.2 beschrieben wurden – insbesondere der Fehleranfälligkeit von Qubits und den Einschränkungen bei direkten Messungen.

\subsection{Der Fehlerkorrekturzyklus am Beispiel von Oberflächencodes}

Der Oberflächencode basiert auf einem zweidimensionalen Gitter aus physikalischen Qubits, typischerweise in Form eines rechteckigen Flächenstücks (\emph{Patch}). Dabei unterscheidet man zwei Arten von Qubits: \emph{Daten-Qubits}, welche die eigentliche logische Quanteninformation tragen, und \emph{Mess-Qubits} (auch \emph{Ancilla-Qubits} genannt), die zur Fehlererkennung verwendet werden.

Man kann sich die Struktur wie ein Schachbrett vorstellen: Die Daten-Qubits liegen auf Kanten des Gitters, während die Mess-Qubits zwischen ihnen positioniert sind. Diese Mess-Qubits überwachen jeweils vier benachbarte Daten-Qubits. Dabei gibt es zwei Typen von Stabilizer-Operatoren:
\begin{itemize}
  \item \textbf{\(Z\)-Stabilizer} (auch \emph{Plaquette-Operatoren}\footnote{Plaquettes bezeichnen kleine quadratische Flächen im Gitter. Ein \(Z\)-Stabilizer wirkt auf die vier Daten-Qubits an den Ecken einer solchen Plaquette und erkennt Bitfehler.}) überprüfen die Parität in Bezug auf Bit-Flip-Fehler.
  \item \textbf{\(X\)-Stabilizer} (auch \emph{Stern-Operatoren}\footnote{Stern-Operatoren wirken auf die vier Daten-Qubits, die an einem Gitterpunkt zusammenlaufen, und erkennen Phasenfehler.}) erkennen Phasenfehler.
\end{itemize}

\paragraph{Syndrommessung und Fehlererkennung:}

Die Fehlerkorrektur erfolgt zyklisch. In jedem Zyklus werden alle \(X\)- und \(Z\)-Stabilizer gemessen. Dazu wird jedes Mess-Qubit mit den vier zugehörigen Daten-Qubits durch eine Reihe kontrollierter Gatter verschränkt, z.\,B. mit CNOT-Operationen. Nach dieser Verschaltung enthält das Mess-Qubit die Paritätsinformation: Bei \(Z\)-Stabilizern etwa, ob eine gerade oder ungerade Anzahl von \( |1\rangle \)-Zuständen in der Gruppe vorliegt. Anschließend wird das Mess-Qubit gemessen – in der \(Z\)-Basis für \(Z\)-Stabilizer und in der \(X\)-Basis für \(X\)-Stabilizer.

Das Ergebnis der Messung – 0 (Eigenwert \(+1\)) oder 1 (Eigenwert \(-1\)) – wird als \emph{Syndrom-Bit} bezeichnet. Es zeigt an, ob sich seit der letzten Messung ein Fehler ereignet hat. Wichtig ist: Diese Messungen beeinflussen den logischen Zustand nicht. Sie liefern nur Informationen über potenzielle Fehler, ohne die kodierte Quanteninformation direkt zu zerstören.

\paragraph{Decodierung und Fehlerlokalisierung:}

Die Menge aller Syndrom-Bits ergibt ein \emph{Syndrommuster}, aus dem auf die wahrscheinlichsten Fehler geschlossen werden kann. Ein einzelner Bitfehler an einem Daten-Qubit führt typischerweise dazu, dass zwei benachbarte \(Z\)-Stabilizer von 0 auf 1 wechseln. Analog dazu führt ein Phasenfehler zu Abweichungen bei zwei benachbarten \(X\)-Stabilizern.

Diese lokalen Muster bilden die Grundlage für die sogenannte \emph{Decodierung}: Ein Algorithmus analysiert die Verteilung der Fehlerhinweise und rekonstruiert den wahrscheinlichsten Fehlerpfad. Häufig wird dazu ein Graph aufgebaut, in dem Syndromabweichungen als Knoten erscheinen. Der Decoder sucht dann eine möglichst kurze Verbindung dieser Knoten – z.\,B. mithilfe eines Minimum-Weight-Perfect-Matching-Algorithmus. Auch modernere Verfahren wie Union-Find oder maschinelles Lernen kommen zum Einsatz.

Da auch Messungen fehlerhaft sein können, betrachtet man oft mehrere Zyklen hintereinander und baut einen 3D-Graphen mit der Zeitachse als dritter Dimension. Inkonstistente Syndrome über die Zeit hinweg deuten dann auf Messfehler hin.

\paragraph{Korrekturoperation:}

Nach der Decodierung steht eine Hypothese über die Art und Position der Fehler zur Verfügung. Man könnte nun direkt eine Pauli-Korrekturoperation auf das betroffene Qubit anwenden – etwa ein weiteres \(X\), um einen Bitflip rückgängig zu machen.

In der Praxis verwendet man häufig stattdessen ein sogenanntes \emph{Pauli-Frame}\footnote{Ein Pauli-Frame ist ein rein klassisches Register, in dem alle erkannten Fehler notiert werden. Die tatsächliche physische Korrektur wird dadurch vermieden und stattdessen bei der Interpretation der Messdaten berücksichtigt. So spart man potenziell fehleranfällige Operationen.}. Dabei wird die Fehlerinformation nicht physisch korrigiert, sondern nur logisch „mitgeführt“ und am Ende beim Auslesen berücksichtigt. Das spart Ressourcen und reduziert zusätzliche Fehlerquellen.

Nach Abschluss eines Fehlerkorrekturzyklus beginnt sofort der nächste – denn in einem realen Quantencomputer können ständig neue Fehler auftreten. Die kontinuierliche Wiederholung des Zyklus ermöglicht daher langfristig stabile und fehlertolerante Quanteninformation.\\

\paragraph{Einfacher Überblick: So funktioniert ein Fehlerkorrekturzyklus im Oberflächencode}Damit ein Quantencomputer trotz fehleranfälliger Qubits zuverlässig funktioniert, wird in kurzen Abständen automatisch überprüft, ob Fehler aufgetreten sind. Der Ablauf sieht dabei so aus:

\begin{enumerate}
  \item \textbf{Start:}  
  Alle Qubits werden in bekannte Anfangszustände gebracht. Die sogenannten Mess-Qubits sind bereit, nach Fehlern zu suchen.

  \item \textbf{Fehlersuche:}  
  Die Mess-Qubits „fragen“ ihre Nachbarn (die Daten-Qubits), ob alles in Ordnung ist. Aus ihren Antworten entsteht ein Muster aus Nullen und Einsen – das sogenannte \emph{Syndrom}.

  \item \textbf{Auswertung:}  
  Ein klassischer Computer schaut sich das Syndrom an und erkennt daraus, wo wahrscheinlich ein Fehler passiert ist.

  \item \textbf{Reaktion:}  
  Der Computer entscheidet, ob und wie ein Fehler korrigiert werden muss. Oft merkt man sich den Fehler einfach und berücksichtigt ihn später automatisch.

  \item \textbf{Neustart:}  
  Der nächste Zyklus beginnt sofort. So werden ständig neue Fehler früh erkannt – auch während der Quantencomputer rechnet.
\end{enumerate}

\subsection{Aktuelle H\"urden und was die Zukunft bringt}

Trotz erster erfolgreicher Implementierungen steht die Quantenfehlerkorrektur (QEC) noch am Anfang ihrer praktischen Nutzbarkeit. Der Weg zu einem fehlertoleranten Quantencomputer ist gepflastert mit technischen und konzeptionellen Herausforderungen. In diesem Abschnitt beleuchten wir die zentralen H\"urden und geben einen Ausblick auf vielversprechende Entwicklungen.\\

\paragraph{Qubit-Qualit\"at und Fehlerschwellen} 
Ein zentrales Problem ist die Qualit\"at der physikalischen Qubits selbst. Quantenfehlerkorrektur wirkt nur, wenn die zugrunde liegenden Fehlerraten hinreichend niedrig sind. Theoretisch existiert eine Fehlerschwelle, unterhalb derer Redundanz die Zuverl\"assigkeit erh\"oht\footnote{Siehe das Threshold-Theorem in Abschnitt~\ref{chap:QEC2}}. Praktisch liegt diese Schwelle typischerweise bei Fehlerraten $\lesssim 10^{-3}$. Viele Architekturen, wie supraleitende Qubits oder Ionenfallen, erreichen in Labordemonstrationen bereits zweiqubit-Gates mit Fehlerraten im niedrigen Promillebereich, was vielversprechend ist. Doch um lange Quantenalgorithmen (z.B. Faktorisierung mit Shor bei $n=2048$) fehlerfrei auszuf\"uhren, m\"ussen logische Qubits \emph{stundenlang} koh\"arent bleiben -- ein Ziel, das nur mit sehr stabilen physikalischen Qubits erreichbar ist. Korrelierte Fehler, Leakage und seltene St\"orereignisse (z.B. kosmische Teilchen) stellen dabei zus\"atzliche Risiken dar.

\paragraph{Massiver Qubit-Overhead}
Fehlerkorrektur ist teuer: Selbst einfache Codes wie der Oberfl\"achencode ben\"otigen $O(d^2)$ physikalische Qubits f\"ur ein logisches Qubit mit Distanz $d$. Realistische Anwendungen erfordern daher \emph{Millionen} physikalischer Qubits. Derzeitige Systeme bewegen sich im Bereich von wenigen Hundert bis Tausend Qubits -- die Skalenl\"ucke ist also noch immens. Jeder weitere Qubit bringt nicht nur Nutzen, sondern auch neue Herausforderungen: Verkabelung, Kalibrierung und thermische Isolation m\"ussen f\"ur jedes Qubit mitgedacht werden.

\paragraph{Technische Limitierungen bei Kryogenik und Auslese}
Besonders supraleitende Systeme m\"ussen bei 10--20\,mK betrieben werden. Hier stoßen Forscher schnell an Grenzen: Die K\"uhlung gro\ss er Quantenprozessoren erfordert aufw\"andige Verd\"unnungsk\"uhler, und die Zahl der nach innen f\"uhrenden Kabel wird schnell zum Flaschenhals. Auch die Kontrolle \"uber klassische Elektronik ist herausfordernd: Steuereinheiten befinden sich oft au\ss erhalb des K\"uhlsystems und m\"ussen mit tausenden Kan\"alen kommunizieren -- ein thermisches und technisches Problem. Ans\"atze wie Cryo-CMOS (Steuerelektronik bei tiefen Temperaturen) werden intensiv erforscht.

\paragraph{Dekodierung: Komplex und latenzkritisch}
Selbst wenn alle Syndromdaten vorliegen, ist die Decodierung (Fehlerdiagnose und Korrekturvorschlag) ein nichttriviales Problem. Bei QEC-Zyklen im Mikrosekundenbereich und Millionen von Qubits entstehen Datenraten im Bereich hunderter Terabyte pro Sekunde. Diese m\"ussen in Echtzeit verarbeitet werden, idealerweise mit Latenzen $<1\,\mu$s. Daher ben\"otigt man spezialisierte Hardware-Decoder\footnote{Siehe z.B. Riverlane's Decodierchips oder FPGA-basierte Systeme.} und ausgekl\"ugelte Algorithmen wie Minimum Weight Matching oder Machine-Learning-gest\"utzte Ans\"atze.

\paragraph{Nicht-ideale Fehler und Modellabweichungen}
Die meisten QEC-Codes beruhen auf idealisierten Fehlermodellen (z.B. unkorrelierte, Pauli-artige Fehler). In realen Systemen treten jedoch korrelierte Fehler, Kaskadeneffekte und Leakage auf, die nicht leicht mit bestehenden Codes zu behandeln sind. Solche Abweichungen senken die effektive Fehlerschwelle und machen eine robuste Fehlerkorrektur schwieriger.\\

\paragraph{Ausblick und neue Ans\"atze}
Die Forschung reagiert mit vielf\"altigen Strategien:
\begin{itemize}
    \item \textbf{Hardwareverbesserung:} Längere Kohärenzzeiten, stabilere Qubits und besseres Materialdesign sind zentrale Entwicklungsziele.
    \item \textbf{Topologische Qubits:} Systeme wie Majorana-Moden versprechen inhärente Fehlerresistenz, sind aber noch experimentell.
    \item \textbf{Modulare Architekturen:} Mehrere kleine Qubit-Module könnten vernetzt werden, um skalierbare Systeme aufzubauen.
    \item \textbf{Neue Codes:} Quantum-LDPC-Codes, Floquet-Codes oder Color Codes bieten bessere Raten und Flexibilität, sind aber schwerer umzusetzen.
    \item \textbf{Ko-Design:} Eine enge Verzahnung von Hardware, Software und Fehlermodellen ist notwendig, um das Gesamtsystem zu optimieren.
\end{itemize}

\paragraph{Fazit} 
Die Quantenfehlerkorrektur hat sich in den letzten Jahren von einem theoretischen Konzept zu einem praktischen Forschungsfeld mit greifbaren Fortschritten entwickelt. Dennoch sind viele zentrale Herausforderungen ungel\"ost. Die kommenden Jahre werden entscheidend sein, um aus den heutigen Demonstratoren robuste, fehlertolerante Quantencomputer zu formen. Dabei ist klar: Ohne QEC ist skalierbares Quantencomputing unm\"oglich -- mit ihr aber wird es eines Tages m\"oglich sein, Probleme zu l\"osen, die klassisch unzug\"anglich bleiben.


\section{Praxisbeispiel: Ein einfacher Quantenfehlerkorrekturcode}
\begin{itemize}
    \item Mit Qiskit Library und qiskit.providers.aer Rauschen simulieren und die Notwendigkeit von Fehlerkorrektur anhand eines Vergleichs aufzeigen
    \item Verschiedene Algorithmen zur Fehlerminimierung verwenden
    \item Verschiedene Rauschen und Ansätze vergleichen und kontrastisieren
    \item Qiskit Visualization Library erlaubt Darstellung von Zusammenhängen
\end{itemize}



\printbibliography
