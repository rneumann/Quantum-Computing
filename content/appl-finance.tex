%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Anwendungsgebiete im Finanzbereich}
\label{trends} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\chapterauthor{Lola Bankai, Felix Goos}

\section{Top Technologien und Algorithmen}

Quantencomputer beruhen auf unterschiedlichen technologischen Prinzipien, die jeweils eigene algorithmische Strategien erfordern. Neben gate-basierten Universalarchitekturen existieren spezialisierte Modelle wie Quantum Annealing oder variationale Hybridverfahren, die auf bestimmte Problemklassen zugeschnitten sind.

Dieses Kapitel gibt einen systematischen Überblick über die zentralen technologischen Ansätze und Quantenalgorithmen. Im Fokus stehen deren Funktionsprinzipien, technische Voraussetzungen sowie typische Einsatzbereiche. Ziel ist es, die strukturelle Vielfalt aktueller Quantencomputing-Ansätze herauszuarbeiten und deren Relevanz für verschiedene Anwendungsszenarien einzuordnen.

Zu den derzeit zentralen quantenbasierten Methoden im Finanzkontext gehören:

\begin{itemize}
  \item Quantum Annealing (QA)
  \item Gate-basierte Algorithmen
    \begin{itemize}
      \item Quantum Approximate Optimization Algorithm (QAOA)
      \item Variational Quantum Eigensolver und verwandte Variationsansätze (VQE, VQA)
    \end{itemize}
  \item Quantum Amplitude Estimation (QAE)
\end{itemize}

Die folgenden Abschnitte analysieren diese Technologien aus technischer Perspektive. Sie zeigen, welche Stärken und Schwächen mit den jeweiligen Ansätzen verbunden sind und welche praktischen Voraussetzungen für deren erfolgreichen Einsatz im Finanzwesen erfüllt sein müssen.

\subsection{Quantum Annealing (QA)}

\subparagraph{Herkunft und Entwicklung}
Quantum Annealing (QA) ist ein quantenmechanisches Optimierungsverfahren, das auf dem Konzept der adiabatischen Quantenberechnung basiert. Die Idee wurde Anfang der 2000er-Jahre insbesondere durch Farhi et al. und D-Wave Systems weiterentwickelt und kommerzialisiert \cite{orus_quantum_2019}. Während universelle Quantencomputer auf Gattermodellen beruhen, fokussiert sich QA auf die Lösung kombinatorischer Optimierungsprobleme durch sukzessive Energie-Minimierung in einem physikalischen System.

\subparagraph{Grundidee und Aufbau}
QA nutzt quantenmechanische Effekte wie Superposition und insbesondere Tunneling, um das globale Minimum einer Zielfunktion zu finden. Das Verfahren startet mit einem bekannten Anfangszustand (Anfangshamiltonian) und transformiert diesen langsam in den Zielzustand (Problemhamiltonian), wobei das System idealerweise stets im Grundzustand bleibt. Das zugrundeliegende Optimierungsproblem wird in ein sogenanntes \emph{Quadratic Unconstrained Binary Optimization} (QUBO) oder Ising-Modell übersetzt \cite{orus_quantum_2019, mugel_dynamic_2022}.

\subparagraph{Technologische Basis}
Die Technologie basiert auf dem adiabatischen Theorem: Wird ein System hinreichend langsam verändert, verbleibt es mit hoher Wahrscheinlichkeit im Grundzustand \cite{orus_quantum_2019}. Zur Umsetzung wird die Problemstruktur als QUBO-Modell kodiert, wobei binäre Variablen Entscheidungsoptionen (z.\,B. Asset-Auswahl) und deren Wechselwirkungen Risiken, Renditen oder Nebenbedingungen repräsentieren. Die physische Umsetzung erfolgt auf Hardware wie dem D-Wave-Quantenprozessor, wobei Qubits binären Variablen und Kopplungen den Interaktionen entsprechen \cite{mugel_dynamic_2022}.

\subparagraph{Algorithmisches Vorgehen}
Zu Beginn befindet sich das System in einem leicht vorbereitbaren Grundzustand. Während des Annealing-Prozesses wird der Hamiltonian kontinuierlich in Richtung des Problems deformiert. Wenn dieser Übergang langsam genug erfolgt, bleibt das System im energetischen Minimum, dem optimalen Lösungskandidaten \cite{orus_quantum_2019}. Dieses Vorgehen erlaubt es, auch in hochdimensionalen Problemräumen durch Quanten-Tunneling lokale Minima zu überwinden und globale Lösungen zu erreichen \cite{mugel_dynamic_2022, Rosenberg 2016}.

\subparagraph{Modellierungsvoraussetzungen}
Eine Voraussetzung für QA ist die Transformation des Problems in eine QUBO-Form. Für viele finanzielle Anwendungen, insbesondere Portfoliooptimierung – ist dies gut möglich, da viele Constraints und Zielfunktionen binär darstellbar sind \cite{mugel_dynamic_2022}. Einschränkungen ergeben sich jedoch bei komplexen, nichtlinear verknüpften Nebenbedingungen, deren QUBO-Formulierung zusätzlicher Modellierungsarbeit bedarf \cite{sakuler_real-world_2025}.

\subparagraph{Chancen und Stärken}
QA bietet insbesondere bei kombinatorischen, binären Problemen eine attraktive Alternative zu klassischen Heuristiken. Vorteile sind die natürliche Hardware-Umsetzung auf spezialisierten Systemen wie D-Wave, die Fähigkeit zur Überwindung lokaler Minima durch Tunneling sowie eine schnelle Konvergenz bei kleinen bis mittelgroßen Problemgrößen \cite{mugel_dynamic_2022, sakuler_real-world_2025}. Zudem zeigt sich in Studien, dass QA insbesondere bei realitätsnahen Portfolios mit echten Marktdaten effiziente Lösungen liefern kann \cite{mugel_dynamic_2022}.

\subparagraph{Grenzen und Herausforderungen}
Ein zentrales Problem besteht in der begrenzten Skalierbarkeit aktueller Systeme: Sowohl die Anzahl als auch die Konnektivität der Qubits limitiert die Größe und Komplexität der darstellbaren Probleme \cite{sakuler_real-world_2025}. Auch ist die Einbettung realer Constraints in die QUBO-Form nicht trivial und verlangt sorgfältiges Penalty-Tuning. Zudem ist die aktuelle Hardware noch anfällig für Rauschen, was insbesondere bei tiefer „Temperatur“ zu Fehlern führen kann \cite{Rosenberg2016, sakuler_real-world_2025}.

\subsection{Gate-basierte Ansätze (QAOA und VQE)}

Gate-basierte Quantencomputer arbeiten auf Basis von quantenmechanischen Schaltkreisen. Diese Systeme ermöglichen die Implementierung sogenannter Variationsalgorithmen, bei denen ein parametrischer Quantenschaltkreis durch einen klassischen Optimierer so angepasst wird, dass eine Zielfunktion minimiert oder maximiert wird. Zu den bekanntesten Methoden gehören der Quantum Approximate Optimization Algorithm (QAOA) und der Variational Quantum Eigensolver (VQE) \cite{buonaiuto_best_2023, brandhofer_benchmarking_2022}. Beide Verfahren finden Anwendung in der Portfoliooptimierung, erfordern jedoch eine geeignete Problemformulierung in Form eines Ising-Hamiltonians.

\subsubsection*{Quantum Approximate Optimization Algorithm (QAOA)}

\subparagraph{Herkunft und Entwicklung}
QAOA wurde ursprünglich 2014 von Farhi et al. entwickelt \cite{farhi_quantum_2014} und stellt eine hybride Methode dar, die speziell für kombinatorische Optimierungsprobleme konzipiert wurde. Im Gegensatz zu Quantum Annealing arbeitet QAOA nicht auf einem physikalisch kontinuierlichen Prozess, sondern diskret über parametrische Gate-Schaltungen. Es zählt zu den vielversprechendsten Algorithmen für sogenannte Noisy Intermediate-Scale Quantum (NISQ)-Geräte.

\subparagraph{Grundidee und Aufbau}
Der QAOA-Ansatz kombiniert zwei zentrale Operationen: eine Phasenoperation basierend auf dem Problemhamiltonian und eine Mischoperation über den sogenannten Mixer-Hamiltonian. Durch abwechselnde Anwendung dieser beiden Operatoren und Variation der Parameter wird ein Quantenzustand erzeugt, der eine möglichst hohe Wahrscheinlichkeit hat, die optimale Lösung zu repräsentieren \cite{brandhofer_benchmarking_2022}.

\subparagraph{Technologische Basis}
Für QAOA muss das Optimierungsproblem binär formuliert und in ein Ising-Modell überführt werden. Dabei codieren Qubit-Zustände Entscheidungsvariablen, während Kopplungsterme die Interaktionen (z.\,B. Risiko-Rendite) abbilden. Die Implementierung erfolgt auf gate-basierten Quantenprozessoren, typischerweise innerhalb von Frameworks wie Qiskit oder Cirq \cite{buonaiuto_best_2023}.

\subparagraph{Algorithmisches Vorgehen}
Der Algorithmus initialisiert einen Quantenzustand (meist im Gleichgewichtszustand) und führt anschließend $p$-mal abwechselnd zwei Schichten aus: eine Phase-Schicht $U_C(\gamma)$, die auf dem Problemhamiltonian basiert, und eine Mixer-Schicht $U_B(\beta)$, die eine Rotation im Zustandsspektrum bewirkt. Nach jeder Iteration werden die Parameter $\gamma$ und $\beta$ durch einen klassischen Optimierer angepasst. Ziel ist es, die Messwahrscheinlichkeit für optimale Lösungen zu maximieren \cite{brandhofer_benchmarking_2022}.

\subparagraph{Voraussetzungen und Modellierung}
Ein geeignetes QUBO- oder Ising-Modell ist Voraussetzung. Besonders die Abbildung harter Constraints stellt eine Herausforderung dar, da diese mit Penalty-Terms implementiert werden müssen. Spezialisierte Mixer können hierbei die Ergebnisqualität und Konvergenz verbessern, wie etwa in der Studie von Brandhofer et al. gezeigt \cite{brandhofer_benchmarking_2022}.

\subparagraph{Chancen und Stärken}
QAOA ist besonders gut geeignet für kombinatorische Probleme mit binärer Struktur. Die geringe Tiefe der Schaltungen macht es für NISQ-Hardware attraktiv. Die Möglichkeit, domänenspezifische Mixer zu integrieren, bietet zusätzliche Flexibilität \cite{brandhofer_benchmarking_2022}.

\subparagraph{Grenzen und Herausforderungen}
Die praktische Leistungsfähigkeit ist stark von der Wahl der Parameter, der Problemgröße und dem Hardware-Backend abhängig. Mit zunehmender Tiefe steigt die Empfindlichkeit gegenüber Rauschen. Zudem erfordert die Parametereinstellung oft umfangreiche Hyperparameter-Tuning-Prozesse \cite{buonaiuto_best_2023}.

\subsubsection*{Variational Quantum Eigensolver (VQE)}

\subparagraph{Herkunft und Entwicklung}
Der Variational Quantum Eigensolver (VQE) wurde 2014 eingeführt und war eine der ersten Methoden zur Nutzung von NISQ-Geräten für Optimierungsprobleme in Physik und Chemie. Anders als QAOA richtet sich VQE nicht ausschließlich an kombinatorische Probleme, sondern zielt allgemein auf das Auffinden von Grundzuständen komplexer Hamiltonians ab \cite{buonaiuto_best_2023}.

\subparagraph{Grundidee und Aufbau}
VQE basiert auf dem variationalen Prinzip der Quantenmechanik: Der Erwartungswert des Hamiltonians wird auf einem parametrischen Quantenzustand minimiert. Dieser Zustand wird durch einen sogenannten Ansatz erzeugt, ein vorgegebener Schaltkreis mit variierbaren Parametern. Die Optimierung erfolgt iterativ mit Hilfe eines klassischen Optimierungsalgorithmus \cite{buonaiuto_best_2023}.

\subparagraph{Technologische Basis}
Das zugrunde liegende Optimierungsproblem wird zunächst in ein QUBO-Modell transformiert, welches dann in ein Ising-Hamiltonian überführt wird. Über die Wahl des Ansatzes und die Operatorbasis (z.\,B. Pauli-Zerlegung) wird der Hamiltonian auf einem realen Quantenprozessor gemessen. Die Messwerte dienen als Eingabe für die klassische Optimierung.

\subparagraph{Algorithmisches Vorgehen}
VQE setzt auf eine wiederholte Schleife aus Quanten- und Klassikprozessor: Zunächst wird der parametrische Schaltkreis vorbereitet und gemessen. Dann schätzt der klassische Teil den Erwartungswert des Hamiltonians und passt die Parameter des Ansatzes an. Dieser Vorgang wird solange wiederholt, bis eine Konvergenz zum Energie-Minimum erreicht ist \cite{buonaiuto_best_2023}.

\subparagraph{Voraussetzungen und Modellierung}
Für die Portfoliooptimierung wird das ursprüngliche Problem zunächst in ein binäres Quadratic Programming-Modell transformiert. Constraints wie Budgetgrenzen werden in die Zielfunktion eingebettet, indem sie als Strafterme gewichtet werden. Die Effizienz hängt stark von der richtigen Wahl des Ansatzes, des Optimierers sowie der Penalty-Koeffizienten ab \cite{buonaiuto_best_2023}.

\subparagraph{Chancen und Stärken}
VQE zeigt große Flexibilität hinsichtlich der Wahl des Ansatzes und eignet sich besonders gut für kontinuierliche oder strukturierte Optimierungsprobleme. Studien zeigen, dass bei richtiger Parametrisierung auf realer Hardware (z.\,B. IBM Q) sehr gute Ergebnisse erzielt werden können, auch ohne aufwändige Fehlerkor


\subsection{Quantum Amplitude Estimation (QAE)}

\subparagraph{Herkunft und Entwicklung}
Quantum Amplitude Estimation (QAE) wurde im Jahr 2000 von Brassard et al. eingeführt und gehört zur Klasse der quantenmechanischen Algorithmen, die auf der Grover-Suche aufbauen \cite{quantumjournal2020}. Das Ziel war, klassische Monte-Carlo-Verfahren durch quadratische Geschwindigkeitsvorteile zu übertreffen. Im Unterschied zu Ansätzen wie Quantum Annealing oder VQE, die auf Optimierung abzielen, adressiert QAE das Problem der präzisen Erwartungsschätzung – ein zentraler Aspekt in stochastischen Simulationen.

\subparagraph{Grundidee und Aufbau}
QAE zielt darauf ab, Wahrscheinlichkeiten bzw. Erwartungswerte effizient zu schätzen. Dazu nutzt es die quantenmechanische Superposition sowie Amplitudeninterferenz, um Zustände mit hoher Relevanz zu verstärken. Die Wahrscheinlichkeit für das Auftreten eines gewünschten Ergebnisses wird dabei indirekt über Messungen in speziell vorbereiteten Zuständen ermittelt. Im Gegensatz zu klassischen Monte-Carlo-Verfahren, bei denen zur Erreichung einer Genauigkeit $\varepsilon$ eine Komplexität von $O(1/\varepsilon^2)$ notwendig ist, erreicht QAE dies mit nur $O(1/\varepsilon)$ Abfragen \cite{quantumjournal2020, rebentrost2018}.

\subparagraph{Technologische Basis}
QAE kombiniert mehrere quantentechnische Bausteine: Vorbereitung eines Superpositionszustands, Anwendung einer Oracle-Funktion zur Kodierung des Problems, kontrollierte Grover-Operatoren zur Amplitudenverstärkung sowie eine abschließende Quanten-Fourier-Transformation zur Schätzung der Zielamplitude \cite{martin2022}. Die Berechnungen erfolgen auf gate-basierten Quantencomputern. Die zugrunde liegende Struktur ist auf tiefe und komplexe Quantenschaltkreise angewiesen, was besondere Anforderungen an Kohärenzzeiten und Fehlerkorrektur stellt.

\subparagraph{Algorithmisches Vorgehen}
Im ersten Schritt wird ein Zustand vorbereitet, der die Lösungswahrscheinlichkeit in seiner Amplitude trägt. Anschließend erfolgt die wiederholte Anwendung des Grover-Operators, der die Wahrscheinlichkeit des Zielzustands periodisch verstärkt. Die Anzahl der Grover-Schritte variiert kontrolliert, sodass über Quanteninterferenz ein Phasenintervall entsteht, welches über die inverse Quanten-Fourier-Transformation präzise ausgelesen werden kann. Das Ergebnis ist eine Schätzung der Zielwahrscheinlichkeit mit quadratisch höherer Effizienz im Vergleich zur klassischen Simulation \cite{quantumjournal2020, martin2022}.

\subparagraph{Voraussetzungen und Modellierung}
Damit QAE zur Anwendung kommen kann, muss das zugrunde liegende Problem so formuliert werden, dass es sich als Wahrscheinlichkeitsverteilung über einem Quantenzustand ausdrücken lässt. In der Praxis bedeutet dies, dass die Bewertung (z.\,B. eines Finanzinstruments) als Erwartungswert über Zufallsvariablen formuliert sein muss. Die Komplexität der Problemabbildung liegt vor allem in der Konstruktion des Orakels und der effizienten Kodierung der stochastischen Prozesse in die Amplituden \cite{rebentrost2018}.

\subparagraph{Chancen und Stärken}
QAE eignet sich besonders für Probleme, bei denen Erwartungswerte über große Stichprobenräume geschätzt werden müssen – etwa bei der Bewertung komplexer, pfadabhängiger Optionen oder der Berechnung von Value-at-Risk (VaR) \cite{orus_quantum_2019}. Die quadratische Beschleunigung gegenüber Monte-Carlo-Methoden führt zu erheblichen Effizienzgewinnen. Studien zeigen, dass bereits bei moderaten Genauigkeitsanforderungen signifikante Reduktionen im Rechenaufwand möglich sind \cite{rebentrost2018, egger2020}. Darüber hinaus eröffnet QAE neue Möglichkeiten zur Modellierung realitätsnaher Szenarien, die mit klassischen Methoden bislang nicht praktikabel waren.

\subparagraph{Grenzen und Herausforderungen}
Die größte Herausforderung in der praktischen Umsetzung liegt in den hardwareseitigen Anforderungen. QAE benötigt tiefe Quantenschaltkreise, die auf heutigen NISQ-Geräten aufgrund limitierter Qubit-Anzahl, kurzer Kohärenzzeiten und hoher Fehlerraten nur schwer zuverlässig realisierbar sind \cite{bouland_prospects_2020, martin2022}. Auch die Konstruktion eines effizienten Orakels ist nicht trivial und hängt stark vom Anwendungsfall ab. Darüber hinaus ist die statistische Robustheit des Algorithmus empfindlich gegenüber Messfehlern, was die praktische Anwendbarkeit einschränkt.

\subparagraph{Praktische Perspektiven und Entwicklungen}
Trotz dieser Hürden gibt es vielversprechende Entwicklungen: Kooperationen wie jene zwischen JPMorgan und IBM oder Forschungsprojekte von Google zeigen, dass QAE in der Finanzbranche ernsthaft evaluiert wird \cite{egger2020}. Erste Proof-of-Concepts für die Optionsbewertung auf simulierten und realen Quantenplattformen sind bereits veröffentlicht worden. Die zukünftige Skalierung hängt maßgeblich von Fortschritten in der Hardwareentwicklung und der Fehlerkorrektur ab.

\subsection{Vergleich der Technologien}

Zur Einordnung der vorgestellten quantenmechanischen Verfahren: Quantum Annealing (QA), Quantum Approximate Optimization Algorithm (QAOA), Variational Quantum Eigensolver (VQE) sowie Quantum Amplitude Estimation (QAE) bietet sich ein zweistufiger Vergleich an. Zunächst erfolgt ein technologischer Vergleich anhand funktionaler und struktureller Merkmale, gefolgt von einer allgemeinen Bewertung hinsichtlich Reifegrad und industrieller Perspektiven.

\subparagraph*{Legende der Vergleichskriterien}
Die Tabelle vergleicht vier zentrale quantenmechanische Verfahren anhand technologisch-funktionaler Merkmale. Im Fokus stehen strukturelle Unterschiede, Hardwareanforderungen und algorithmische Eigenheiten, die für die praktische Implementierung und Modellierbarkeit entscheidend sind.


\begin{itemize}
    \item \textbf{Plattform}: Art der zugrundeliegenden Quantenarchitektur (adiabatisch oder gate-basiert).
    \item \textbf{Zieltyp}: Ob das Verfahren auf Optimierung oder Erwartungsschätzung ausgerichtet ist.
    \item \textbf{Formulierung}: Mathematische Struktur, in die Probleme überführt werden müssen (z.\,B. QUBO, Hamiltonian).
    \item \textbf{Ressourcen}: Erforderliche Quantenschaltkreise, Qubit-Kopplungen oder komplexe Operatoren.
    \item \textbf{Struktur}: Art des Algorithmusaufbaus – z.\,B. statisch, flexibel oder zyklisch.
    \item \textbf{Fehlertoleranz}: Robustheit gegenüber Rauschen und Hardwareunschärfen.
    \item \textbf{Qubitbedarf}: Geschätzter Ressourcenbedarf bezogen auf Qubit-Zahl und Konnektivität.
\end{itemize}

\subparagraph{Legende der Einstufungen}

\begin{itemize}
    \item \textbf{hoch}: Verfahren ist etabliert, gut erforscht oder praxistauglich.
    \item \textbf{mittel}: Verfahren ist teilweise anwendbar, unterliegt aber Einschränkungen.
    \item \textbf{niedrig}: Aktuell kaum praktikabel oder nur experimentell umsetzbar.
    \item \textbf{aufstrebend / wachsend / sehr hoch}: Verfahren mit stark wachsendem Potenzial, aber begrenztem Stand heute.
\end{itemize}

\subsubsection*{Technologischer Vergleich}

\begin{table}[H]
\centering
\caption{Technologisch-struktureller Vergleich der Methoden}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{0.26\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|}
\hline
\textbf{Kriterium} & \textbf{QA} & \textbf{QAOA} & \textbf{VQE} & \textbf{QAE} \\
\hline
Plattform & Adiabatisch & Gate-basiert & Gate-basiert & Gate-basiert \\
\hline
Zieltyp & Optimierung & Optimierung & Optimierung & Schätzung \\
\hline
Formulierung & QUBO/Ising & Ising-Modell & Hamiltonian & Orakel \\
\hline
Ressourcen & Kopplungen & Tiefe Gates & Variabler Ansatz & QFT, Gates \\
\hline
Struktur & Hardware-nah & Alternierend & Flexibel & Interferenz \\
\hline
Fehlersensitivität & Gering & Hoch & Hoch & Sehr hoch \\
\hline
Qubitbedarf & Niedrig & Mittel & Mittel & Hoch \\
\hline
\end{tabular}
\end{table}

\subsubsection*{Bewertung und Analyse: Technologisch-struktureller Vergleich}

\paragraph{Analyse ausgewählter Aspekte}

Bereits der erste Eintrag zeigt eine klare Trennung: Während Quantum Annealing (QA) auf einem adiabatischen Hardwaremodell basiert, setzen alle anderen Verfahren auf gate-basierte Systeme. Dies macht QA besonders hardware-nah und ressourcenschonend, da es keine komplexen Schaltungen benötigt \cite{orus_quantum_2019, mugel_dynamic_2022}. Die Kopplungen zwischen Qubits übernehmen direkt die Problemstruktur. Im Gegensatz dazu erfordern QAOA, VQE und QAE komplexe Gate-Sequenzen, die eine präzise Steuerung voraussetzen \cite{buonaiuto_best_2023}.

Ein auffälliger Unterschied zeigt sich beim Zieltyp: Während QA, QAOA und VQE für Optimierungsprobleme entwickelt wurden, dient QAE der Schätzung von Wahrscheinlichkeiten, etwa bei Monte-Carlo-Simulationen oder Value-at-Risk-Modellen \cite{rebentrost_quantum_2018, quantumjournal2020}. Daraus ergeben sich tiefgreifende Unterschiede in der Struktur und Ressourcennutzung. Insbesondere QAE benötigt tiefe, kontrollierte Quantenschaltkreise inklusive Quanten-Fourier-Transformation, was es zu einer der anspruchsvollsten Methoden macht \cite{martin2022}.

Der Punkt „Struktur“ verdeutlicht, wie unterschiedlich die Algorithmen designt sind: QAOA arbeitet mit alternierenden Operatoren (Problem- und Mixer-Gates), während VQE auf einen frei wählbaren Ansatz basiert, der auf die Hardware optimiert sein kann \cite{brandhofer_benchmarking_2022}. QA hingegen nutzt eine kontinuierliche Energielandschaft, was eine ganz andere physikalische Interpretation erlaubt.

In Bezug auf Fehlertoleranz und Qubitbedarf zeigen sich wieder zwei Extreme: QA benötigt vergleichsweise wenige Qubits mit eingeschränkter Konnektivität, ist dafür aber relativ robust gegenüber Fehlern \cite{sakuler_real-world_2025}. QAE hingegen ist stark fehleranfällig und benötigt eine hohe Zahl kohärenter Qubits \cite{bouland_prospects_2020}, was es aktuell noch limitiert.

Der technologische Vergleich zeigt klar, dass sich die Verfahren nicht nur im Ziel, sondern auch in ihrer Hardware- und Ressourcenausrichtung stark unterscheiden. Während QA eine spezialisierte, robuste Lösung für bestimmte Optimierungsprobleme bietet, sind VQE, QAOA und QAE flexibler, aber in ihrer aktuellen Form noch stark abhängig von der Hardwareentwicklung.




\subsubsection*{Reifegrad und Anwendungsperspektiven}

\begin{table}[H]
\centering
\caption{Bewertung der Verfahren nach Reifegrad und industrieller Perspektive}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{0.24\linewidth}|p{0.19\linewidth}|p{0.19\linewidth}|p{0.19\linewidth}|p{0.19\linewidth}|}
\hline
\textbf{Kriterium} & \textbf{QA} & \textbf{QAOA} & \textbf{VQE} & \textbf{QAE} \\
\hline
Reifegrad & hoch & mittel & mittel & niedrig \\
\hline
Hardware-Kompatibilität & hoch & mittel & mittel & niedrig \\
\hline
Industrielle Relevanz & hoch & mittel & mittel & aufstrebend \\
\hline
Skalierbarkeit & gering & mittel & mittel & gering \\
\hline
Fehlerkorrektur & gering & hoch & hoch & sehr hoch \\
\hline
Forschungstiefe & hoch & hoch & sehr hoch & wachsend \\
\hline
Zukunftspotenzial & mittel & hoch & hoch & sehr hoch \\
\hline
\end{tabular}
\end{table}


\subsubsection*{Bewertung und Analyse: Reifegrad und Anwendungsperspektiven}

Die Tabelle gibt einen überblicksartigen Vergleich der vier betrachteten quantenmechanischen Verfahren entlang zentraler Anwendungskriterien. Dabei geht es weniger um eine vollständige Bewertung, sondern vielmehr um eine Orientierung entlang von Stärken und Schwächen in Bezug auf praktische Umsetzbarkeit, Forschungsstand und Zukunftspotenzial.



\paragraph{Analyse ausgewählter Kriterien}

Besonders auffällig ist die hohe Bewertung von Quantum Annealing (QA) im Hinblick auf den technologischen Reifegrad und die praktische Umsetzbarkeit. Diese Einschätzung basiert auf der industriellen Nutzung durch Anbieter wie D-Wave sowie reale Anwendungen etwa in der Portfolio- oder Supply-Chain-Optimierung \cite{mugel_dynamic_2022, sakuler_real-world_2025}. Dennoch zeigt QA gleichzeitig eine \emph{geringe Skalierbarkeit}, was mit der begrenzten Qubit-Konnektivität aktueller Annealer zusammenhängt \cite{sakuler_real-world_2025}.

Quantum Amplitude Estimation (QAE) weist ein gegenteiliges Profil auf: Es besitzt ein \emph{sehr hohes Zukunftspotenzial}, kann aber derzeit aufgrund der tiefen Schaltkreise und des Bedarfs an Fehlerkorrektur kaum praktisch eingesetzt werden \cite{bouland_prospects_2020, martin2022}. Die „niedrig“-Bewertung bei Umsetzbarkeit entspricht dem aktuellen Stand auf NISQ-Geräten.

Variational Quantum Eigensolver (VQE) wird in der Literatur häufig für chemische und finanzielle Modellierungen eingesetzt, insbesondere auf IBM-Q-Systemen \cite{buonaiuto_best_2023}. Die Kombination aus mittelmäßigem Reifegrad und hohem Forschungsstand ist hier konsistent: Die Methode ist anpassbar, aber hardwareintensiv.

Der QAOA-Ansatz liegt in fast allen Dimensionen zwischen QA und VQE, was durch seine modulare Gate-Struktur begründet ist. Studien zeigen jedoch, dass bei sorgfältiger Parametrisierung und Spezialisierung deutliche Fortschritte erzielt werden können \cite{brandhofer_benchmarking_2022}.

Zusammenfassend zeigt sich, dass aktuell nur QA als produktionsnah gelten kann, während QAOA und VQE im mittleren Reifegrad operieren. QAE hingegen steht als vielversprechender, aber technisch herausfordernder Spezialfall am Anfang seiner Entwicklung.







\abstract{Dieses Kapitel untersucht das Potenzial von Quantencomputing für den Einsatz im Finanzwesen. Im Zentrum stehen drei besonders rechenintensive Anwendungsfelder: Portfoliooptimierung, Monte-Carlo-Simulationen und Risikomessung. Anhand aktueller Forschungsergebnisse und Pilotprojekte wird analysiert, wie Quantenalgorithmen klassische Verfahren ergänzen oder perspektivisch ablösen könnten. Dabei werden sowohl Quantum Annealing als auch gate-basierte Algorithmen wie QAOA, VQE und QAE betrachtet. Darüber hinaus beleuchtet das Kapitel die Rolle führender Technologieunternehmen, Finanzinstitute und internationaler Forschungsinitiativen. Abschließend erfolgt eine kritische Bewertung der technologischen Entwicklungen mit Blick auf konkrete Einsatzmöglichkeiten, bestehende Herausforderungen und strategische Implikationen für die Finanzbranche.}

\section{Einleitung}
Das Finanzwesen zählt zu den datenintensivsten und rechentechnisch anspruchsvollsten Bereichen der modernen Wirtschaft. Täglich werden riesige Datenmengen analysiert, komplexe Modelle berechnet und Entscheidungen unter Unsicherheit getroffen. Klassische Computersysteme stoßen hierbei zunehmend an ihre Leistungsgrenzen, insbesondere wenn es um die Lösung kombinatorischer Optimierungsprobleme, stochastischer Simulationen oder die Verarbeitung hochdimensionaler Datenstrukturen geht. In diesem Kontext rücken Quantencomputer als potenziell transformative Technologie in den Fokus. Sie beruhen auf den Prinzipien der Quantenmechanik wie Superposition, Verschränkung und Interferenz und ermöglichen damit neue Arten der Informationsverarbeitung, die bei bestimmten Problemklassen eine exponentielle Beschleunigung gegenüber klassischen Verfahren versprechen.

Erste Forschungsarbeiten und Pilotprojekte belegen, dass Quantenalgorithmen insbesondere in der Portfoliooptimierung, in der Bewertung komplexer Finanzinstrumente sowie im Risikomanagement erhebliche Vorteile bringen könnten. Dabei geht es nicht nur um Rechengeschwindigkeit, sondern auch um die Möglichkeit, realistischere Modelle zu verwenden, die bislang aufgrund ihrer Komplexität nicht praktikabel waren. Gleichzeitig stellt die wachsende Leistungsfähigkeit von Quantencomputern auch eine Bedrohung für klassische kryptografische Verfahren dar, was die Notwendigkeit neuer Sicherheitsstrategien aufwirft.

Im weiteren Verlauf dieses Kapitels werden zunächst die zentralen Herausforderungen und Chancen des Quantencomputings im Finanzwesen dargestellt. Anschließend werden die drei wichtigsten Anwendungsfelder beleuchtet: Portfoliooptimierung, Optionsbewertung und Risikomessung. Darauf folgt eine Übersicht über die zugrunde liegenden Technologien und Algorithmen. Zudem werden führende Akteure, bedeutende Unternehmensinitiativen sowie ausgewählte Forschungsprojekte vorgestellt. Den Abschluss bildet eine Bewertung der Entwicklungen und ein Teilfazit mit Blick auf zukünftige Potenziale.

\section{Relevanz und Problemstellung}
Zu den datenintensivsten und rechenaufwendigsten Bereichen der modernen Wirtschaft zählt das Finanzwesen. Jeden Tag müssen enorme Datenmengen ausgewertet, komplizierte Modelle beurteilt und Entscheidungen mit Risiko getroffen werden   häufig unter Bedingungen von Unsicherheit und in dynamischen Märkten. Bei der Bewältigung solcher Aufgaben geraten herkömmliche Computer zunehmend an ihre Grenzen. Der Rechenaufwand in Bereichen wie Optimierung, Simulation oder probabilistischer Risikobewertung wächst insbesondere mit der Komplexität von Finanzprodukten und -märkten exponentiell an \cite{springer2025,plos2024}.

Quantencomputer stellen ein neuartiges Paradigma dar: Sie verarbeiten Informationen auf der Grundlage quantenmechanischer Zustände, was die parallele Bearbeitung bestimmter mathematischer Probleme mit einem erheblichen Geschwindigkeitsvorteil ermöglicht. Neue Lösungsräume eröffnen sich insbesondere bei kombinatorischen Optimierungsproblemen, wie sie beispielsweise in der Portfoliozusammensetzung oder im Optionspricing vorkommen, sowie bei stochastischen Simulationen. Erste Untersuchungen haben ergeben, dass Quantenalgorithmen diese Aufgaben mit einer deutlich höheren Effizienz bewältigen können als klassische Verfahren \cite{quantumjournal2020,orus2019}.

Zugleich bringt die Verwendung von Quantencomputern im Finanzsektor neue Herausforderungen mit sich. Vor allem die Gefahr, dass klassische kryptographische Verfahren brechen könnten, steht neben der noch nicht voll ausgereiften Hardware im Raum. Es geht dabei nicht nur um den Schutz sensibler Finanzdaten, sondern auch um die Stabilität ganzer Systeme. Das sogenannte „Harvest Now, Decrypt Later“-Szenario ist besonders kritisch. Dabei werden heute verschlüsselte Daten abgefangen, um sie später mit leistungsfähigen Quantencomputern zu entschlüsseln \cite{finance21net}.

Damit befindet sich der Finanzsektor an einem Wendepunkt: Auf der einen Seite bieten Quantencomputer enorme Möglichkeiten zur Effizienzsteigerung und zur Verbesserung bestehender Verfahren   wie bei Risikomodellen, Investmentstrategien oder der Integration mit künstlicher Intelligenz \cite{finance21net}. Auf der anderen Seite ist es notwendig, dass bestehende Infrastrukturen rechtzeitig gegen neue Bedrohungen geschützt werden. Die Chance-Risiko-Dualität macht Quantencomputing für den Finanzsektor äußerst relevant und zeigt die Notwendigkeit praxisnaher Forschung und strategischer Vorausschau auf \cite{springer2025,orus2019}.

!!hier fehlen noch die dinger in zotero

\section{Top 3 Anwendungsfelder (Praxis \& Theorie)}

Die konkrete Anwendbarkeit von Quantencomputern im Finanzsektor lässt sich exemplarisch an drei besonders rechenintensiven und praxisrelevanten Bereichen aufzeigen. Diese Felder zeichnen sich durch hohe algorithmische Komplexität und umfangreiche Datenanforderungen aus und gelten daher als vielversprechende Einsatzgebiete für quantenbasierte Verfahren. Im Folgenden wird untersucht, in welchem Ausmaß Quantenalgorithmen klassische Methoden ergänzen oder perspektivisch ablösen könnten.

Die drei zentralen Anwendungsfelder sind:

\begin{itemize}
    \item{Portfoliooptimierung}
   
    \item{Monte-Carlo-Simulationen}

    \begin{itemize}
        \item{Optionsbewertung}
        \item{Risikomessung}
    \end{itemize}
\end{itemize}

Diese Abschnitte beleuchten sowohl die theoretischen Grundlagen als auch erste praktische Implementierungen auf aktueller Quantenhardware. Ziel ist es, das Potenzial quantenbasierter Verfahren im Vergleich zu klassischen Methoden realistisch einzuordnen und mögliche technologische Durchbrüche zu identifizieren.

\subsection{Portfoliooptimierung}
Die Portfoliotheorie ist ein Konzept der Finanzwirtschaft und beschreibt die Bestimmung eines optimalen Portfolios durch die Zusammensetzung von mehreren Kapitalanlagen. Kapitalanlagen beschreiben Investitionen in zahlreiche, langfristig orientierte Anlagen, die in Form von Aktien, Anleihen, Währungen, Immobilien oder Rohstoffen über Jahre gehalten und nicht verkauft werden \cite{orus_quantum_2019, sakuler_real-world_2025}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{bildname.png}
  \caption{efficient frontier}
  \label{fig:beispielbild}
\end{figure}

Harry Markowitz widmete sich der Frage, in welcher Form ein optimales Portfolio unter Berücksichtigung der Marktbedingungen zusammengestellt werden sollte. Die Effizienzkurve repräsentiert das zentrale Konzept und stellt jene Portfolios dar, die die maximale Rendite zu einem gegebenen Risikoniveau, beziehungsweise umgekehrt das minimale Risiko für eine Zielrendite darstellen. Markowitz (1959) modelliert daher ein Portfolio mittels des Erwartungswert der zukünftigen Renditen, sowie der Varianz (bzw. Standardabweichung) als Maß für das Risiko. Markowitz entdeckte, dass die Varianz des Portfolios durch Diversifikation, also der Gewichtung unterschiedlicher Anlageklassen mit geringer Korrelation, minimiert werden kann. Hierdurch entsteht die gekrümmte Form der Effizienzkurve. Ein rational handelnder Investor würde nun ein Portfolio auf der Kurve wählen. Portfolios rechts oder unterhalb der Effizienzkurve weisen entweder eine geringere Rendite, zu dem selben Risiko oder eine identische Rendite zu mehr Risiko auf.

Die Optimierung nach Markowitz lässt sich in einem kontinuierlichen, quadratischen Optimierungsproblem schreiben, das mithilfe von quadratischer Programmierung lösbar ist
\cite{Markowitz 1959}.

In der Praxis müssen jedoch oft diskrete Kauf- oder Nicht-Kauf-Entscheidungen getroffen werden, die das Problem zu einem kombinatorischen Optimierungsproblem machen. Solche Probleme lassen sich dann in einem Quadratic Unconstrained Binary Optimization (QUBO) Formulierung überführen. Die daraus entstehende Lösungsmenge wächst dabei exponentiell mit der Anzahl der hinzugefügten Assets.
Trotz der theoretischen Stärke der Portfoliotheorie stößt sie schnell bei solch wachsender Komplexität an Ihre Grenzen \cite{sakuler_real-world_2025, orus_quantum_2019}.

\subsubsection*{Quantenbasierte Methoden zur Portfoliotheorie}

An dieser Stelle kommen Quantencomputer ins Spiel. Die Idee dahinter ist, das kombinatorische Problem direkt auf Quantenbits abzubilden und damit die Suche nach einer effizienten Lösung zu optimieren. Zwei Technologien stechen in den letzten Jahren der Forschung besonders heraus: Quantum Annealing und gate-basierte Quantenalgorithmen \cite{mugel_dynamic_2022, orus_quantum_2019}.

\subsubsection*{Quantum Annealing}
 
Zahlreiche kombinatorische Optimierungsprobleme, wie zum Beispiel eine Portfolioauswahl von 0/1-Entscheidungen, gehören zu den sehr schwer lösbaren Problemen. Diese lassen sich in eine Quadratic Unconstrained Binary Optimization (QUBO-Form) überführen. Quantum Annealer sind spezialisierte Quantencomputer und nutzen diese QUBO-Probleme und leiten sie durch quantenmechanische Tunnel, um Ihre Minimums Lösung zu finden. 

Die Übersetzung auf die Portfoliotheorie ist wie folgt: Jede mögliche Portfolioentscheidung wird als Qubit abgebildet, während die Markowitzfunktion, also die Kombination aus Rendite und Risiko als effektives Hamiltonian formuliert wird. Hamiltonian ist die mathematische Form der Energie des Modells. Diese dient zur Identifikation des optimalen Portfolios, also den Zustand mit der geringstmöglichen Energie erzeugt durch das stetige Herunterkühlens des Quantenannealers. Details zu dieser Technologie in Kapitel 4.4.1 \cite{mugel_dynamic_2022}.
 
Erste praktische Untersuchungen zeigen, dass kleine Portfolioinstanzen erfolgreich mithilfe von Quantum Annealing gelöst werden können. So gelang es, reale Portfolios mit etwa 9 bis 11 Assets (bestehend aus Aktien, Anleihen und Geldmarktinstrumenten) unter Berücksichtigung von Budget- und Volatilitätsrestriktionen als QUBO-Probleme zu formulieren und auf Quantenhardware auszuführen. Die Optimierungsläufe lieferten innerhalb von Sekunden bis wenigen Minuten Lösungen, die qualitativ mit klassischen Verfahren übereinstimmten \cite{sakuler_real-world_2025}rosenberg.
 
Dabei zeigten sich insbesondere bei hybriden Annealing-Methoden, also der Kombination aus klassischer Vorverarbeitung und quantenbasierter Optimierung robuste Ergebnisse mit hoher Reproduzierbarkeit. Herausfordernd bleiben allerdings die begrenzte Konnektivität der Qubits im Annealer und die damit verbundene Notwendigkeit, Problemstrukturen auf die zugrundeliegende Hardware-Topologie zu projizieren. Ebenso erweist sich die Auswahl geeigneter Modellparameter wie Strafterme oder Normierungen als kritisch für die Qualität der Lösung \cite{sakuler_real-world_2025}.
 
Auch in umfangreichen Testfällen mit deutlich größeren Portfolios und längeren historischen Datenreihen zeigten sich skalierbare Resultate. Die besten Ergebnisse wurden dabei mit hybriden oder quanteninspirierten Verfahren erzielt, die klassische Heuristiken mit quantum-naher Optimierung kombinierten. Ein echter Quantenvorteil gegenüber klassischen Methoden konnte bislang jedoch noch nicht nachgewiesen werden, wenngleich die Technologie als reif für kleinere, strukturierte Probleme gilt \cite{BBVA, 2020}.!!

\subparagraph{Chancen und Herausforderungen}

Quantum Annealing gilt als besonders geeigneter Ansatz zur Lösung kombinatorischer Optimierungsprobleme wie der Portfolioallokation. Die praktische Umsetzbarkeit wurde in mehreren Pilotstudien nachgewiesen, etwa durch Formulierung von realistischen Portfolios mit bis zu elf Assets als QUBO-Probleme \cite[S. 6]{orus_quantum_2019}. Dabei konnten innerhalb kurzer Rechenzeiten robuste Lösungen gefunden werden, die qualitativ mit klassischen Verfahren übereinstimmen. Insbesondere hybride Methoden, welche klassische Vorverarbeitung mit quantenmechanischer Lösungsfindung kombinieren, zeigten eine hohe Reproduzierbarkeit der Ergebnisse.

Herausforderungen bestehen jedoch in der begrenzten Konnektivität der Qubits in aktuellen Annealing-Hardwarearchitekturen (z.\,B. D-Wave), was eine komplexe Problemembedding-Strategie erfordert \cite[S. 9]{orus_quantum_2019}. Zusätzlich beeinflussen Parameter wie Strafterme und Normalisierungen die Lösungssuche stark, was eine sorgfältige Modellierung notwendig macht \cite[S. 10]{sakuler_real-world_2025}. Obwohl Quantum Annealing keine universelle Quantenlösung darstellt, bieten erste Resultate auf praktischen Geräten vielversprechende Hinweise auf eine sinnvolle Anwendbarkeit in kleinen bis mittelgroßen Portfolioinstanzen.

\subsubsection*{Gate-basierte-Algorithmen}
 
Gate-basierte Algorithmen wie QAOA und VQE lassen sich auf Portfoliooptimierungsprobleme anwenden, indem diese zunächst in ein Ising-Modell oder einen geeigneten Hamiltonoperator überführt werden. In dieser Darstellung entsprechen die Qubits binären Portfolioentscheidungen, während die Zielfunktion aus Risiko- und Ertragsanteilen als Energieausdruck kodiert wird. Der Quantenalgorithmus sucht dann durch parametrische Variationen jenen Zustand mit minimaler Energie, der einer optimalen Allokation entspricht \cite{buonaiuto_best_2023, brandhofer_benchmarking_2022}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{EfficientFrontier_VQE.png}
    \caption{Beispiel einer Efficient Frontier in der Portfolio-Optimierung (simuliertes 6-Asset-Portfolio). Die blaue Kurve zeigt die optimale Risiko-Rendite-Kurve (klassische Exhaustivsuche), die grünen Dreiecke markieren Lösungen des Quantum-VQE-Algorithmus. Man sieht, dass die Quantenlösungen die Effizienzgrenze eng nachzeichnen (Rendite $\mu^T x$ und Risiko $x^T \Sigma x$ sind in willkürlichen Einheiten). \cite[Abb. 8]{egger_quantum_2020}}
    \label{fig:efficient_frontier_vqe}
\end{figure}


Abb. \ref{fig:efficient_frontier_vqe} illustriert, dass bereits heutige gate-basierte Quantenalgorithmen wie der VQE in der Lage sind, für kleine Portfolios (hier ein simuliertes 6-Asset-Portfolio) ähnlich effiziente Lösungen wie klassische Exhaustivsuche zu erzeugen. Die grünen Dreiecke zeigen, dass die vom VQE generierten Lösungen eng entlang der klassischen Efficient Frontier liegen. Dies verdeutlicht das Potenzial quantenbasierter Optimierung in Kombination mit VQE, insbesondere für risikoertragsbasierte Portfolioentscheidungen. Die zugrunde liegende Studie von Egger et al. (2020) demonstriert damit erfolgreich die prinzipielle Machbarkeit dieser Methode auf simulierten Quantencomputern. Einschränkungen bestehen jedoch weiterhin in Bezug auf Skalierbarkeit, da größere Portfolios tiefere Schaltkreise und längere Optimierung erfordern.
 
Gleichzeitig zeigen erste Studien jedoch, dass unter kontrollierten Bedingungen bereits vielversprechende Ergebnisse erzielt werden können. So testeten mehrere Arbeiten die Wirksamkeit von VQE-Pipelines. Die Ergebnisse lieferten, dass der VQE auf einem 5-Qubit-Gerät eine fast identische Lösung zum klassischen Optimum generieren konnte. Die Qualität der Lösung variierte jedoch mit der Qualität des Quantenchips. Größere und weniger von Rausch behaftete Geräte lieferten verbesserte Ergebnisse als kleinere. Somit schmälern Rauschen und die begrenzte Qubit-Zahlen zwar noch den Nutzen, obwohl die Algorithmen prinzipiell eine optimale Lösung bieten \cite{buonaiuto_best_2023}.
Der Einsatz von QAOA mit der Fixierung der Anzahl der enthaltenen Assets zeigte, dass die Anzahl benötigter Quantengatter und Messungen, die für mittlere Problemgrößen benötigt werden, die NISQ-Geräte an ihre Grenzen bringt. Für kleinere Portfolios konnten jedoch annähernd optimale Ergebnisse erzielt werden \cite{brandhofer_benchmarking_2022}.
 
Beide Ansätze QAOA/VQE glänzen in Ihrer Flexibilität. Es können leichter zusätzliche Nebenbedingungen eingebaut und dynamische Portfolio-Probleme formuliert werden. Nachteile spiegeln sich in der Limitation durch Rauschen und Dekohärenz wider, die ohne notwendige Fehlerkorrektur nicht behoben werden können. Aufgrund dieser Einschränkungen bleiben die Anwendungen der gate-basierten Ansätze noch auf rein simulierter Ebene oder auf kleinen Hardware-Experimenten beschränkt.

\subparagraph{Chancen und Herausforderungen}

Gate-basierte Algorithmen wie VQE bieten ein hohes Maß an Anpassungsfähigkeit für Portfoliooptimierungsprobleme, insbesondere im Umgang mit komplexen Nebenbedingungen. Erste Tests auf realer Hardware zeigen, dass bei kleinen Portfolios bereits Lösungen in Nähe des klassischen Optimums erzielt werden können \cite[S. 2–4]{buonaiuto_best_2023}. Dabei hängt die Qualität stark von den gewählten Hyperparametern und der verwendeten Hardware ab. Je größer und rauschärmer das Quantenchip, desto näher die Lösung an der klassischen Referenz. Allerdings bleiben signifikante Herausforderungen bestehen: Skalierungsprobleme, Rauschanfälligkeit und die Auswahl geeigneter Optimierer beeinflussen die Ergebnisse maßgeblich. Trotz des hohen Potenzials für künftige Verbesserungen ist ein praktischer Vorteil gegenüber klassischen Methoden bislang nicht eindeutig belegt.

\subsection{ Monte Carlo Simulation}

Die Bewertung von Anlagen und Portfolios bedingt eine enorme Komplexität durch die
unvollkommene Verteilung von Informationen auf Finanzmärkten. Entscheidungen über Risikostrategien, Bewertungen und Investitionen müssen häufig in Situationen getroffen werden, in denen wirtschaftliche Verläufe nicht direkt einsichtig sind. Hierbei greifen Marktteilnehmer auf Schätzungen, Wahrscheinlichkeiten und historische Daten zurück, um zukünftige Entwicklungen zu bewerten. 
\\
 Als grundlegendes Instrument wird in diesem Bereich die Monte-Carlo-Simulation in Betracht gezogen. Mithilfe der Monte-Carlo-Simulation können die Lösungen von analytisch komplexen und schwer lösbaren mathematischen Problemen geschätzt werden. Es nutzt also statistische Simulierung und Zufallsstichproben, um wiederholt verschiedene Szenarien eines Systems zu simulieren und Erwartungswerte oder Wahrscheinlichkeiten approximativ zu bestimmen.\cite{orus_quantum_2019}.
 
Typische Anwendungsfelder dieses stochastischen Ansatzes sind: Optionsbewertung (Pricing von Derivaten, deren Auszahlung vom zukünftigen Preisverlauf eines Basiswerts abhängt) und Risikobemessung (Berechnung des Value at Risk für ein Portfolio über einen Zeithorizont). Letzteres wird in Kapitel 4.3.3 näher beleuchtet \cite{orus_quantum_2019}. Sowohl die Optionsbewertung als auch die Risikobemessung stellen zentrale Aufgaben im Finanzwesen dar, bei denen zukünftige, stochastisch unsichere Größen quantifiziert werden müssen. Aufgrund der Komplexität moderner Finanzinstrumente und der hohen Dimensionalität der zugrunde liegenden Zufallsprozesse sind analytische Lösungen häufig nicht verfügbar. Die Monte-Carlo-Simulation dient in beiden Anwendungsfeldern als methodisches Fundament, um durch zufällige Stichproben komplexe Erwartungswerte oder Verteilungsquantile numerisch zu vergleichen. Damit bildet sie die Grundlage zahlreicher Bewertungs- und Risikomodelle in Praxis und Forschung.

\subsubsection*{Optionsbewertung}
Die Bewertung von Optionen erfordert häufig numerische Verfahren, da für viele Ausgestaltungen keine geschlossenen analytischen Lösungen existieren. Besonders bei komplexeren Derivaten wie asiatischen Optionen, Barrier-Optionen oder Portfolios von Optionen muss meist numerisch simuliert werden. Optionen stellen Finanzderivate dar, wobei ein Derivat ein Instrument ist, dessen Preisentwicklung von der Entwicklung einer oder mehrerer Anlagen abhängt. Das Monte-Carlo-Verfahren nutzt hierbei eine große Menge an Zufallspfad-Simulationen generiert für die relevanten Prozesse und errechnet aus den Auszahlungen oder Verlusten statistische Kennzahlen. Faktisch wird versucht, den „fairen“ Preis einer Option zu ermitteln. Dazu werden mögliche Pfade, die der Aktienkurs einnehmen könnte, bis zur Fälligkeit simuliert, für jeden Pfad die Auszahlung berechnet und gemittelt\cite{orus_quantum_2019}.
 
Damit die Verteilungsmerkmale in der geforderten Genauigkeit erfasst werden können, müssen ausreichend Pfade generiert werden. Das Gesetz der großen Zahlen bestimmt hierbei dass die Schätzung des Erwartungswertes mit der Anzahl N der simulierten Pfade ungefähr mit $1/\sqrt{N}$ konvergiert. Diese Konvergenz ist verhältnismäßig gering. Um die Genauigkeit, um genau eine Einheit zu verbessern, muss die Zahl der Simulationsläufe um das Vierfache erhöht werden. Dadurch steigt die Zahl der Pfade schnell auf eine Millionen oder Milliarden Höhe an.
In der Praxis ist genau das eins der größten Probleme. Komplexe Berechnungen von zum Beispiel Kreditrisiko-Simulationen von Hunderten von Banken und Unternehmen können Tage dauern. Trotz der stetigen Parallelisierung stoßen herkömmliche Methoden aufgrund der Rechenzeit und Kosten an Ihre Grenzen \cite{orus_quantum_2019}.

\subsubsection*{Quantenbasierte Methoden zur Optionsbewertung}
Durch methodische Erweiterungen, wie der Quantum Amplitude Estimation (QAE) können Prozesse der klassischen Monte-Carlo Simulation beschleunigt werden. Bei dem Fall der Optionsbewertung, müssen Wahrscheinlichkeitsverteilungen über zukünftige Assetpreise quantenmechanisch kodiert werden. Diesbezüglich wird QAE angewendet, um effiziente Schätzungen des Erwartungswertes, also die Bestimmung des “fairen” Optionspreises, zu gewährleisten. Im Vergleich zur klassischen Monte-Carlo-Methode, die für eine Zielgenauigkeit von $\varepsilon$ etwa $O(1/\varepsilon^2)$ Stichproben benötigt, minimiert QAE diesen Aufwand auf $O(1/\varepsilon)$. Praktisch bedeutet das, dass statt einer Million klassischer Proben rund 1000 Quantenproben ausreichen bei gleichbleibender Akkuratesse \cite{Brassard/Lomonaco+Brandt; Montanaro, 2015}.

In ersten Bewertungs-Workflows wurde dieser theoretische Ansatz bereits auf realer Quantenhardware getestet und demonstriert. Es wurden verschieden Optionsarten mithilfe passender Quantenschaltungen modelliert und darauf folgend mit Maximum-Likelihood-Amplitude-Estimation (MLAE) bewertet. Dieses Verfahren ist vorteilhafter als das klassische QAE, da es durch eine flachere Schaltung besser für die NISQ-Geräte geeignet ist. NISQ-Geräte (Noisy Intermediate-Scale Quantum) sind gegenwärtige Quantencomputer mit einer begrenzten Anzahl von Qubits und ohne vollständige Fehlerkorrektur, die bereits erste praktische Anwendungen ermöglichen, jedoch noch anfällig für Störungen und Skalierungsprobleme sind \cite{stamatopoulos_option_2020}.

Fehler können durch Kalibrierung von Zwei-Qubit-Gattern gedämpft werden, wodurch bereits aus Drei-Qubit-Schaltungen valide Optionspreise extrahiert werden konnten. Zentrale praktische Hindernisse bleiben jedoch bestehen. Besonders das effiziente Laden von Datenquellen und klassischen Wahrscheinlichkeitsverteilungen auf Quantenhardware stellt sich als kompliziert heraus. Um diese Marktszenarien, wie Asset-Preise, auf Quantencomputern zu verarbeiten, müssen die Daten in einen entsprechenden Quantenzustand überführt werden. Hiefür benötigen die klassischen Verfahren einen exponentiell steigenden Ressourcenaufwand von $O(2^n)$. Eine Lösung bietet Quantum Generative Adversial Networks (qGans), die aus den klassischen Marktdaten die passenden Wahrscheinlichkeitsverteilungen erlernt und anschließend als polynomieller Komplexität als Quantenzustand bereitstellt \cite{zoufal_quantum_2019}.

Zusammenfassend wird deutlich, dass QAE in Kombination mit verfahren wie MLAE, iterativen QAE oder variationalen Ansätzen zu einer Beschleunigung klassischer Monte-Carlo-Simulationen führt. In Verbindung mit effizienten Datenlagern wie qGANs rückt der Einsatz solcher Verfahren auf realer Quantenhardware in greifbare Nähe.

\subparagraph{Chancen und Herausforderungen}

Die Anwendung quantenmechanischer Verfahren zur Optionsbewertung, etwa durch Quantum Amplitude Estimation (QAE) oder Maximum-Likelihood-Amplitude-Estimation (MLAE), verspricht signifikante Rechenzeitvorteile gegenüber klassischen Monte-Carlo-Simulationen. In ersten Experimenten konnten realistische Optionspreise bereits aus wenigen Qubits extrahiert werden \cite[S. 4–6]{stamatopoulos_option_2020}, was das Potenzial für eine breitere Nutzung unterstreicht. Gleichzeitig bestehen technische Hürden: Insbesondere das effiziente Laden klassischer Wahrscheinlichkeitsverteilungen in Quantenregister ist rechenintensiv. Quantum Generative Adversarial Networks (qGANs) bieten hier einen vielversprechenden Ansatz zur effizienten Quantenzustandspräparation \cite[S. 1–3]{zoufal_quantum_2019}. Die hohe Variabilität exotischer Auszahlungsstrukturen und die Unsicherheit bei Modellkalibrierungen bleiben jedoch praktische Herausforderungen.

\subsection{Risikobemessung und Value at Risk}

Eine der zentralen Herausforderungen im modernen Finanzwesen ist die Risikobemessung. Um drohende Verluste rechtzeitig zu erkennen und gezielt Gegenmaßnahmen zu ergreifen, sind Banken, Investoren und andere Marktteilnehmer in einem hoch volatilen Marktumfeld darauf angewiesen, Risiken genau zu quantifizieren und zuverlässig einschätzen zu können. 
Auch in der Risikobemessung stellt die Monte-Carlo-Simulation ein zentrales Werkzeug dar, insbesondere zur Schätzung von Risikokennzahlen wie dem Value at Risk (VaR) oder dem Conditional Value at Risk (CVaR), die auf der Analyse stochastischer Verlustverteilungen basieren. Der VaR ist ein anerkanntes und gängiges Instrument zur Quantifizierung von Risiko und zeigt den maximalen Verlust an, den ein Portfolio oder eine einzelne Anlageposition innerhalb eines bestimmten Zeitraums und mit einer festgelegten Vertrauens-Wahrscheinlichkeit (in der Regel 95\,\% oder 99\,\%) nicht überschreiten wird \cite{springer2025, plos2024}. 
Für Finanzinstitutionen ist der VaR nicht nur methodisch relevant, sondern auch regulatorisch von hoher Bedeutung. Finanzinstitute wie Banken, Versicherungen und Investmentfonds sind gesetzlich verpflichtet, Marktrisiken und Kreditrisiken regelmäßig zu quantifizieren, um ausreichende Kapitalpuffer zu erzeugen. Die Risikokennzahlen dienen darüber hinaus intern der Begrenzung von Handelspositionen, etwa durch tägliche VaR-Limits für einzelne Portfolios oder Trader. Auch in der langfristigen Risikosteuerung, etwa bei Pensionskassen oder Versicherungen, sind VaR und CVaR zentrale Steuerungsgrößen, um sicherzustellen, dass finanzielle Verpflichtungen auch unter Stressszenarien eingehalten werden können \cite{Orús et al., 2019}.
Trotz seiner weiten Verbreitung ist der VaR nicht frei von Kritik. Insbesondere die fehlende Subadditivität kann bei der Aggregation von Risiken zu fehlerhaften Einschätzungen führen. Der CVaR wurde als kohärentes Risikomaß eingeführt, um auch die Schwere der Verluste im Extrembereich zu erfassen und gilt in vielen Anwendungen als präzisere Alternative \cite{orus_quantum_2019}.In der Praxis lassen sich VaR und CVaR nur in einfachen Fällen analytisch berechnen. Komplexe Portfolios mit nicht-normalverteilten Erträgen oder Derivatebestandteilen erfordern numerische Verfahren. Die Monte-Carlo-Simulation ist hier besonders geeignet, um aus vielen simulierten Szenarien empirische Verlustverteilungen abzuleiten und daraus die relevanten Quantile zu bestimmen \cite{Zoufal et al., 2019}.

Die Monte-Carlo-Simulation gilt zwar als Standardverfahren zur Schätzung komplexer Verlustverteilungen, stößt jedoch bei steigender Modellkomplexität schnell an ihre Grenzen. Mit zunehmender Anzahl an Risikofaktoren, Anlageklassen oder eingebetteten Derivaten steigt der Rechenaufwand drastisch an. Insbesondere die Ermittlung von Extremrisiken wie dem 99{,}9\,\%\,VaR erfordert Millionen von Simulationsläufen, was selbst auf leistungsstarken Rechnern mit erheblichen Laufzeiten verbunden ist. Für verschachtelte Portfolios kommt es häufig zu sogenannten Nested-Monte-Carlo-Strukturen, bei denen für jedes Szenario zusätzliche Bewertungen durchgeführt werden müssen. Die resultierende Rechenzeit verhindert eine laufende Risikobewertung in kurzen Intervallen und erschwert eine schnelle Reaktion in volatilen Marktphasen \cite{bouland_prospects_2020, orus_quantum_2019}


\subsubsection*{Quantenbasierte Methoden zur Risikobemessung}

In Anbetracht der rechentechnischen Hürden klassischer Risikosimulationen rückt Quantum Computing als vielversprechender Lösungsansatz zunehmend in den Fokus. Besonders relevant ist hierbei der Quantum-Amplitude-Estimation-Algorithmus (QAE), der bei der Berechnung stochastischer Erwartungswerte eine quadratische Beschleunigung gegenüber klassischen Monte-Carlo-Verfahren ermöglicht \cite{quantumjournal2020, rebentrost_quantum_2018}. Der Vorteil des QAE besteht darin, dass weniger Simulationsläufe benötigt werden, um eine Zielgenauigkeit $\varepsilon$ zu erreichen, da die Komplexität von $O(1/\varepsilon^2)$ auf $O(1/\varepsilon)$ reduziert wird \cite{martin2022}. Dies ist besonders relevant bei der Schätzung seltener Ereignisse wie dem 99{,}9\,\%-VaR, bei dem klassisch Millionen bis Milliarden Pfade erforderlich wären.

Zur Anwendung von QAE im Risikokontext wird ein Orakel-Operator entworfen, der für ein bestimmtes Verlustniveau $x$ anzeigt, ob der simulierte Portfolioverlust dieses Niveau überschreitet. Mittels QAE lässt sich dann die Wahrscheinlichkeit $P(\text{Verlust} > x)$ effizient bestimmen. Durch Variation von $x$ kann so derjenige Schwellenwert ermittelt werden, für den die Überschreitungswahrscheinlichkeit einem definierten Konfidenzniveau entspricht, was exakt der Definition des Value at Risk entspricht. Analog dazu lässt sich auch der Conditional Value at Risk berechnen, indem ein modifizierter Schaltkreis verwendet wird, der den durchschnittlichen Verlust im Überschreitungsfall quantifiziert \cite{orus_quantum_2019, egger_quantum_2020}.

Die Umsetzung dieser quantenmechanischen Verfahren erfordert jedoch eine geeignete Repräsentation der Verlustverteilung auf dem Quantencomputer. Dazu müssen komplexe stochastische Prozesse, wie sie etwa durch Marktrisiken und Korrelationen zwischen verschiedenen Risikofaktoren entstehen, in einem Quantenzustand abgebildet werden. Eine direkte Kodierung solcher Verteilungen auf Quantenhardware ist jedoch exponentiell aufwendig, weshalb quantum-native Methoden wie Quantum Generative Adversarial Networks (qGANs) eingesetzt werden. Diese Netzwerke sind in der Lage, aus realen Marktdaten eine Zielverteilung zu lernen und diese in polynomieller Komplexität als Quantenzustand bereitzustellen \cite{zoufal_quantum_2019}. Kombiniert mit vereinfachten QAE-Varianten wie dem Maximum-Likelihood-Amplitude-Estimation (MLAE), die flachere Quantenschaltkreise erzeugen, können solche Verfahren auch auf heutiger NISQ-Hardware eingesetzt werden \cite{stamatopoulos_option_2020}.

Erste Proof-of-Concept-Studien zeigen, dass quantenbasierte Risikomessung prinzipiell umsetzbar ist. In einer wegweisenden Arbeit demonstrierten Woerner und Egger die Berechnung des VaR für ein vereinfachtes Anleiheportfolio mit Hilfe eines IBM-Q-Prozessors. Trotz der Beschränkung auf wenige Qubits konnten valide Resultate erzeugt werden, die die klassische Berechnung reproduzierten \cite{egger_quantum_2020}. Aufbauend darauf wurden in weiteren Studien der potenzielle Ressourcenbedarf für realistische Risikoszenarien analysiert. Diese Arbeiten kommen zu dem Ergebnis, dass für große Portfolios mit vielen Risikofaktoren und hohen Konfidenzniveaus fehlerkorrigierte Quantencomputer mit deutlich größerer Qubit-Anzahl erforderlich wären. Gleichzeitig wurden aber auch Ansätze zur Reduktion der Komplexität untersucht, etwa durch grobe Vorabschätzungen mittels klassischer Simulation oder durch analytische Näherungen innerhalb des quantenbasierten Modells \cite{martin2022}.

Auch in der Finanzindustrie wächst das Interesse an quantenbasierter Risikomodellierung. Verschiedene Banken, Technologieunternehmen und Start-ups forschen an hybriden Architekturen, bei denen klassische und Quantencomputer kombiniert werden. Ziel ist es, einzelne Komponenten wie die Quantilschätzung gezielt zu beschleunigen und so tägliche Risikoberichte effizienter zu gestalten. Langfristig wird angestrebt, komplexe Verlustverteilungen nahezu in Echtzeit zu analysieren und damit ein neues Niveau an Präzision und Reaktionsgeschwindigkeit im Risikomanagement zu erreichen \cite{orus_quantum_2019, bouland_prospects_2020}.

Insgesamt zeigt sich, dass quantenbasierte Methoden wie QAE das Potenzial haben, bestehende Grenzen klassischer Risikoberechnung deutlich zu verschieben. Zwar steht die praktische Umsetzung noch am Anfang, doch bereits heute belegen erste experimentelle Studien den theoretischen Vorteil und die prinzipielle Machbarkeit. Mit zunehmender Reife der Quantenhardware sowie verbesserter Datenkodierung und Fehlerkorrektur könnten quantenmechanische Simulationen zu einem festen Bestandteil moderner Finanzanalyse werden.

\subparagraph{Chancen und Herausforderungen}

Quantengestützte Risikomodellierung verspricht eine signifikante Beschleunigung bei der Schätzung von Value at Risk und Conditional Value at Risk, insbesondere in Szenarien mit hohen Konfidenzniveaus und komplexer Portfoliostruktur. QAE reduziert die erforderliche Anzahl an Simulationsläufen drastisch und ermöglicht präzisere Risikoberichte bei geringerem Rechenaufwand \cite[S. 6–7]{orus_quantum_2019}. Gleichzeitig sind praktische Anwendungen noch limitiert: Das Laden komplexer Verlustverteilungen ist technisch aufwendig, und aktuelle NISQ-Geräte stoßen bei umfangreichen Risikomodellen rasch an Grenzen. Auch klassische Monte-Carlo-Verfahren leiden unter ähnlichen Problemen, insbesondere bei Konvergenzgeschwindigkeit und Modellrisiken \cite[S. 1–2]{MonteCarloSim2023}. Quantenverfahren zeigen daher bislang insbesondere bei strukturierten, datengetriebenen Subproblemen einen Vorteil.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|p{0.25\linewidth}|p{0.23\linewidth}|p{0.23\linewidth}|p{0.23\linewidth}|}
\hline
\textbf{Anwendungsfeld} & \textbf{Problem} & \textbf{Quantenlösung} & \textbf{Technologie(n)} \\
\hline
\textbf{Portfolio-optimierung} & 
Kombinatorische Auswahl aus Assets mit Budget/Risiko & 
Binäre Optimierung mit Energie-Minimum-Suche & 
\begin{minipage}[t]{\linewidth}
\begin{itemize}[leftmargin=*,noitemsep]
  \item Quantum Annealing (D-Wave)
  \item QAOA/VQE (gate-basiert)
\end{itemize}
\end{minipage} \\
\hline
\textbf{Optionsbewertung} & 
Erwartungswert der Auszahlung über viele Pfade & 
Schätzung des Mittelwerts über quantisierte Wahrscheinlichkeiten & 
\begin{minipage}[t]{\linewidth}
\begin{itemize}[leftmargin=*,noitemsep]
  \item QAE
  \item MLAE (NISQ-kompatibel)
\end{itemize}
\end{minipage} \\
\hline
\textbf{Risikomessung (VaR/CVaR)} & 
Quantile aus Verlustverteilung (extreme Szenarien) & 
Amplitude-Schätzung von Überschreitungswahrscheinlichkeit & 
\begin{minipage}[t]{\linewidth}
\begin{itemize}[leftmargin=*,noitemsep]
  \item QAE
  \item QAE + qGANs
\end{itemize}
\end{minipage} \\
\hline
\end{tabular}
\caption{Übersicht der Anwendungsfelder und Quantenlösungen im Finanzbereich}
\label{tab:qc_overview}
\end{table}








\section{Wichtige Unternehmen und Akteure und Ihre Zukunftsprojekte}
Der Fortschritt im Quantencomputing für Finanzanwendungen wird wesentlich von der engen Vernetzung führender Technologieunternehmen, Finanzinstitute und Forschungsinitiativen geprägt. Einige Akteure entwickeln Hardware und Plattformen, andere arbeiten direkt an Algorithmen oder Pilotprojekten. Im Folgenden werden die wichtigsten Akteure vorgestellt und ihre Projekte zugeordnet.

\subsection{IBM und JPMorgan: Technologie trifft Anwendung}

IBM zählt zu den globalen Vorreitern im Bereich Quantencomputing. Über die Cloudplattform IBM Quantum stellt das Unternehmen supraleitende Quantenprozessoren und entsprechende Entwicklungsumgebungen bereit. Partnerunternehmen können dort eigene Algorithmen entwickeln, testen und auf realer Quantenhardware ausführen. Im Finanzsektor nimmt IBM eine zentrale Rolle ein, indem es nicht nur Infrastruktur anbietet, sondern auch aktiv an der Entwicklung domänenspezifischer Quantenlösungen mitwirkt.
Ein bedeutender Kooperationspartner ist JPMorgan Chase, eine der ersten Banken, die strategisch auf Quantencomputing setzen. Im Rahmen der IBM-Quantum-Partnerschaft forscht JPMorgan an Anwendungen zur Bewertung komplexer Finanzinstrumente und zur Risikomodellierung (Moody’s, 2024). Besonders im Fokus steht die Quantum Amplitude Estimation (QAE), ein Algorithmus, der klassische Monte-Carlo-Simulationen deutlich effizienter gestaltet. Durch QAE lässt sich der Rechenaufwand zur Berechnung von Erwartungswerten signifikant verringern, was für Finanzanwendungen wie die Derivatebewertung oder die Berechnung des Value at Risk (VaR) besonders relevant ist (Larsson & Oosterlee, 2019; Schuld, 2020).
Ein weiteres gemeinsames Forschungsfeld ist die Quantum Random Number Generation (QRNG), bei der echte Zufallszahlen aus quantenphysikalischen Effekten gewonnen werden. Solche Zufallszahlen können klassische Pseudozufallsverfahren ersetzen und liefern robustere Ergebnisse für Simulationen und stochastische Modelle (Schuld, 2020). Die Kombination aus verbesserter Genauigkeit und höherer Rechengeschwindigkeit macht quantengestützte Verfahren für den Finanzsektor besonders vielversprechend.
Die Kooperation zwischen IBM und JPMorgan steht exemplarisch für eine enge Verzahnung von technologischer Innovation und praktischer Finanzanwendung. IBM stellt die Hardware und Softwareplattform bereit, während JPMorgan konkrete Anwendungsfälle identifiziert und validiert. Dadurch entsteht ein messbarer Erkenntnisgewinn für beide Seiten und ein reales Testfeld für die Leistungsfähigkeit aktueller Quantenalgorithmen im Finanzkontext (Moody’s, 2024).

\subsection{Microsoft und Alphabet: Infrastruktur ohne konkreten Finanzfokus}

Microsoft und Alphabet zählen zu den technologisch führenden Unternehmen im Bereich des Quantencomputings. Beide Konzerne investieren umfangreich in Forschung und Entwicklung und betreiben eigene Quantenplattformen, die den Zugang zu verschiedenen Hardwarearchitekturen ermöglichen. Im Gegensatz zu IBM fokussieren sich Microsoft und Alphabet bislang jedoch primär auf die infrastrukturelle Ebene. Konkrete, öffentlich dokumentierte Finanzanwendungen oder Partnerschaften mit Banken sind derzeit nicht bekannt.
Microsoft verfolgt mit der Plattform Azure Quantum einen Multi-Backend-Ansatz. Nutzer können über eine einheitliche Cloud-Umgebung auf verschiedene Quantenprozessoren zugreifen, darunter Anbieter wie IonQ, Quantinuum und Rigetti. Die Plattform richtet sich an Entwickler, Forschungseinrichtungen und Unternehmen, die eigene Quantenanwendungen testen oder simulieren möchten. Azure Quantum stellt damit eine flexible und leistungsstarke Infrastruktur zur Verfügung, die theoretisch auch für Finanzanwendungen geeignet ist. Bisher liegen jedoch keine spezifischen Kooperationen mit Finanzinstituten vor, in denen Microsoft aktiv quantenbasierte Lösungen für Risikoanalyse, Portfoliooptimierung oder Simulation mitgestaltet hätte (Microsoft Azure Quantum, 2024).
Alphabet verfolgt mit dem Quantum AI Lab einen stärker forschungsgetriebenen Ansatz. Der Fokus liegt auf der Weiterentwicklung supraleitender Quantenprozessoren sowie auf algorithmischen Innovationen, etwa im Bereich des maschinellen Lernens. Im Jahr 2019 erreichte Alphabet mit einem experimentellen Quantenprozessor einen wichtigen Meilenstein: die Demonstration der sogenannten Quantenvorherrschaft. Dabei wurde eine Aufgabe gelöst, die auf klassischen Computern kaum noch effizient berechnet werden kann (Arute et al., 2019). Trotz dieses Fortschritts beschränken sich die Aktivitäten von Alphabet bislang auf die Grundlagenforschung. Finanzanwendungen oder Kooperationen mit Banken wurden bisher nicht veröffentlicht (Google Quantum AI, 2024).
Sowohl Microsoft als auch Alphabet spielen eine zentrale Rolle beim Aufbau eines globalen Quantenökosystems. Ihre Plattformen tragen dazu bei, eine technologische Grundlage zu schaffen, auf der verschiedenste Branchen eigene Use Cases entwickeln können. Für den Finanzsektor bieten beide Unternehmen damit eine potenziell wertvolle Infrastruktur. Im Unterschied zu IBM engagieren sie sich bisher jedoch nicht direkt in der Entwicklung oder Validierung quantenspezifischer Anwendungen für finanzwirtschaftliche Problemstellungen.

\subsection{Europäische Perspektive: NEASQC und interdisziplinäre Zusammenarbeit}
Während große US-amerikanische Technologieunternehmen den Markt für Quantencomputing dominieren, verfolgt die Europäische Union mit gezielten Förder-programmen das Ziel, eigene Anwendungen für die NISQ-Ära zu entwickeln und gleichzeitig ihre technologische Souveränität zu stärken. Ein zentrales Vorhaben in diesem Kontext ist das Forschungsprojekt NEASQC (Next Applications of Quantum Computing), das im Rahmen des Horizon-2020-Programms der EU gefördert wird.
NEASQC bringt ein interdisziplinäres Konsortium aus Forschungseinrichtungen, Technologieunternehmen und Industriepartnern zusammen. Zu den beteiligten Organisationen gehören unter anderem Atos, das französische Forschungsinstitut CEA sowie als einziger Finanzakteur die Großbank HSBC. Ziel des Projekts ist es, praxistaugliche Algorithmen für aktuelle Quantenhardware zu entwickeln und anwendungsnah zu testen. Der Fokus liegt dabei auf sogenannten NISQ-Geräten, also Quantencomputern mit begrenzter Qubit-Anzahl und ohne vollständige Fehlerkorrektur (NEASQC, 2025).
Im Finanzbereich konzentriert sich NEASQC auf drei zentrale Anwendungsfelder: Monte-Carlo-Simulation, Portfolioanalyse und Hauptkomponentenanalyse (PCA). Dabei werden klassische Verfahren durch quantenmechanische Ansätze ersetzt oder ergänzt, etwa mithilfe von Quantum Amplitude Estimation, Quantum Machine Learning oder variationalen Algorithmen. In einem konkreten Anwendungsfall arbeitet HSBC gemeinsam mit Partnern an der Beschleunigung stochastischer Risikomodelle und der Entwicklung von Quantenschemata zur Optionsbewertung (Manzano et al., 2024; Zenodo, 2025).
Besonderes Gewicht liegt im Projekt auf der offenen Veröffentlichung von Ergebnissen und Tools. NEASQC verfolgt eine Open-Source-Strategie, bei der entwickelte Algorithmen und Testumgebungen für Dritte verfügbar gemacht werden, um Transparenz, Anschlussfähigkeit und Transfer in andere Anwendungsbereiche zu ermöglichen (NEASQC, 2025).
Das Projekt gilt als wichtiger Baustein der europäischen Quantenstrategie, da es nicht nur technische Entwicklungen fördert, sondern auch den Dialog zwischen Wissenschaft, Wirtschaft und öffentlicher Hand institutionalisiert. Für den Finanzsektor bietet NEASQC damit die Möglichkeit, die Anwendbarkeit quantenbasierter Methoden unter realen Bedingungen frühzeitig zu evaluieren und europaweit nutzbare Standards und Vorgehensweisen mitzugestalten (Finextra, 2020).

\section{Bewertung anhand der Kriterien}

Auf Basis der in diesem Kapitel analysierten Anwendungsfelder erfolgt im Folgenden eine vergleichende Bewertung der drei zentralen Einsatzbereiche von Quantencomputing im Finanzwesen: Portfoliooptimierung, Optionsbewertung und Risikomessung (Value at Risk / Conditional Value at Risk). Die Bewertung orientiert sich an fünf übergeordneten Kriterien: technologischer Reifegrad, wirtschaftliche Nutzbarkeit, gesellschaftlicher Nutzen, Forschungspotenzial sowie Risiken und ethische Implikationen.

Ziel dieser Gegenüberstellung ist es, die unterschiedlichen Entwicklungsstände, Chancen und Herausforderungen der betrachteten Anwendungsfelder systematisch einzuordnen. Dabei wird deutlich, in welchen Bereichen bereits erste praxisnahe Anwendungen existieren und wo insbesondere aus wissenschaftlicher Perspektive weiterführende Forschung notwendig ist, um das volle Potenzial quantenbasierter Methoden auszuschöpfen.



\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|p{0.25\linewidth}|p{0.23\linewidth}|p{0.23\linewidth}|p{0.23\linewidth}|}
\hline
\textbf{Kriterium} & \textbf{Portfoliooptimierung} & \textbf{Optionsbewertung} & \textbf{Risikobemessung (VaR/CVaR)} \\
\hline
Technologischer Reifegrad & mittel & niedrig & niedrig \\
\hline
Marktrelevanz & hoch & hoch & sehr hoch \\
\hline
Gesellschaftlicher Nutzen & mittel & mittel & hoch \\
\hline
Forschungspotenzial & hoch & sehr hoch & sehr hoch \\
\hline
Risiken und Ethik & gering & mittel & hoch \\
\hline
\end{tabular}
\caption{Bewertung der drei Hauptanwendungsfelder von Quantencomputing im Finanzwesen}
\label{tab:anwendungsbewertung}
\end{table}


\subsection*{Erläuterung der Bewertungskriterien}

Im Folgenden werden die fünf Bewertungskriterien der Tabelle detailliert erläutert und jeweils für die drei Anwendungsfelder \textit{Portfoliooptimierung}, \textit{Optionsbewertung} und \textit{Risikobemessung (Value at Risk / Conditional Value at Risk)} eingeordnet.

\subsubsection*{Technologischer Reifegrad}

Die \textit{Portfoliooptimierung} weist den höchsten Reifegrad auf. Quantum Annealing wurde bereits in praktischen Studien erfolgreich angewendet (z.\,B.\ mit D-Wave-Systemen), insbesondere bei kleinen bis mittelgroßen Portfolios \cite[S.~6–7]{rosenberg2016}, \cite[S.~3]{mugel_dynamic_2022}. Gate-basierte Verfahren wie VQE wurden zudem erfolgreich simuliert \cite[S.~8]{egger2020}.

Die \textit{Optionsbewertung} ist in einem früheren Stadium. Zwar konnte Quantum Amplitude Estimation (QAE) auf realen Quantenchips wie IBM Tokyo erfolgreich getestet werden, etwa zur Bewertung von Barrier-Optionen \cite[S.~2–5]{stamatopoulos_option_2020}, doch das effiziente Laden von Wahrscheinlichkeitsverteilungen bleibt eine Herausforderung.

Für die \textit{Risikobemessung} (VaR/CVaR) existieren erste Proof-of-Concepts wie bei Woerner und Egger \cite[S.~6]{orus_quantum_2019}, jedoch ist die Skalierbarkeit auf realistische Portfolios bislang technisch limitiert. Die notwendige Datenkodierung erfordert komplexe Vorverarbeitung \cite[S.~2]{zoufal_quantum_2019}.

\subsubsection*{Marktrelevanz / Wirtschaftliche Nutzbarkeit}

Die \textit{Portfoliooptimierung} ist ökonomisch relevant für institutionelle Anleger, Robo-Advisors und Asset Manager. Optimierte Allokationen können direkte Renditeeffekte erzielen \cite[S.~2]{sakuler_real-world_2025}, \cite[S.~3]{mugel_dynamic_2022}.

Die \textit{Optionsbewertung} hat ebenso hohe wirtschaftliche Bedeutung, da Derivate eine zentrale Rolle im Risikomanagement und der Preisfindung an Finanzmärkten spielen \cite[S.~1]{stamatopoulos_option_2020}.

Die \textit{Risikobemessung} besitzt die höchste Relevanz, da regulatorische Anforderungen (Basel III/IV) tägliche Berechnung von VaR und CVaR verlangen. Jede Effizienzsteigerung hat direkte Auswirkungen auf Kapitalanforderungen \cite[S.~2]{orus_quantum_2019}, \cite[S.~5]{bouland_prospects_2020}.

\subsubsection*{Gesellschaftlicher Nutzen}

Die \textit{Portfoliooptimierung} wirkt nur indirekt gesellschaftlich, etwa durch effizientere Kapitalmärkte und stabilere Anlagestrategien.

Bei der \textit{Optionsbewertung} ist der Nutzen ebenfalls mittelbar: Genauere Preisfindung verhindert Fehlbewertungen und trägt zur Marktstabilität bei \cite[S.~3]{stamatopoulos_option_2020}.

Die \textit{Risikobemessung} hat den größten gesellschaftlichen Wert: Sie dient der Früherkennung systemischer Risiken und damit dem Schutz von Volkswirtschaften vor Finanzkrisen \cite[S.~6]{orus_quantum_2019}, \cite[S.~4]{zoufal_quantum_2019}.

\subsubsection*{Forschungspotenzial / Innovationsgrad}

Die \textit{Portfoliooptimierung} bleibt ein aktives Forschungsfeld: Neue hybride Algorithmen (z.\,B.\ QAOA mit Constraint-Encoding) bieten Raum für methodische Innovationen \cite[S.~4]{buonaiuto_best_2023}.

Die \textit{Optionsbewertung} zeigt sehr hohe Innovationskraft: Die Kombination aus QAE, variationalen Methoden und qGANs zur Datenkodierung bietet vielfältige Ansätze für algorithmische Forschung \cite[S.~2–3]{zoufal_quantum_2019}, \cite[S.~5]{stamatopoulos_option_2020}.

Auch die \textit{Risikobemessung} ist forschungsintensiv. Sie vereint Themen wie Tail-Risiko, stochastische Prozesse und Quantum Machine Learning \cite[S.~3]{egger2020}, \cite[S.~5]{martin2022}.

\subsubsection*{Risiken und ethische Implikationen}

Die \textit{Portfoliooptimierung} ist relativ risikoarm. Fehlentscheidungen betreffen vor allem institutionelle Investoren und lassen sich in Simulationen evaluieren \cite[S.~3]{sakuler_real-world_2025}.

Die \textit{Optionsbewertung} birgt mittlere Risiken. Unzureichend getestete Modelle könnten Fehlbewertungen verursachen. Zudem ist die Nachvollziehbarkeit von QAE-Methoden begrenzt \cite[S.~6]{stamatopoulos_option_2020}.

Die \textit{Risikobemessung} bringt die größten ethischen Herausforderungen mit sich. Fehlerhafte Modellierung von Extremszenarien kann zu systemischen Risiken führen. Die Black-Box-Natur mancher Quantenmodelle erschwert die Auditierbarkeit \cite[S.~6]{orus_quantum_2019}, \cite[S.~4]{zoufal_quantum_2019}.

\section{Teilfazit}

Die Analyse der drei zentralen Anwendungsfelder von Quantencomputing im Finanzwesen zeigt ein differenziertes Bild hinsichtlich technologischer Reife, wirtschaftlicher Relevanz und langfristiger Forschungsperspektiven.

Die Portfoliooptimierung ist derzeit das technologisch am weitesten entwickelte Feld. Erste realweltliche Anwendungen mit D-Wave-Systemen zeigen die praktische Umsetzbarkeit von Quantum Annealing auf kleinen bis mittleren Portfolioinstanzen \cite[S.~1–2]{sakuler_real-world_2025}. Die erzielten Ergebnisse sind bereits vergleichbar mit klassischen Lösungen, wobei hybride Strategien aktuell die besten Resultate liefern \cite[S.~5]{sakuler_real-world_2025}. Dennoch sind Skalierung und Parametertuning weiterhin forschungsintensive Herausforderungen.

In der Optionsbewertung sind vor allem Methoden wie Quantum Amplitude Estimation (QAE) in Kombination mit qGANs vielversprechend. Studien zeigen, dass QAE im Vergleich zur klassischen Monte-Carlo-Simulation eine quadratische Beschleunigung bei vergleichbarer Genauigkeit erzielen kann \cite[S.~6]{zoufal_quantum_2019}, \cite[S.~4]{stamatopoulos_option_2020}. Die praktische Umsetzung auf Quantenhardware erfordert jedoch verbesserte Kohärenzzeiten und Gate-Fidelitäten \cite[S.~7]{zoufal_quantum_2019}.

Die Risikobemessung auf Basis von Value at Risk (VaR) und Conditional Value at Risk (CVaR) ist konzeptionell gut erforscht und bietet laut Woerner und Egger ein erhebliches Effizienzpotenzial durch QAE \cite[S.~5]{orus_quantum_2019}, \cite[S.~6]{zhou_quantum_2025}. In realen Szenarien ist jedoch die Datenintegration und Modellkalibrierung anspruchsvoll. Zudem bestehen regulatorische Hürden, insbesondere im Hinblick auf Modelltransparenz und Auditierbarkeit \cite[S.~5]{egger2020}.

In allen drei Bereichen besteht ein hohes Forschungspotenzial, insbesondere in Bezug auf hybride Algorithmen, probabilistische Modellierung und domänenspezifische Datenkodierung \cite[S.~6]{zoufal_quantum_2019}, \cite[S.~3]{zhou_quantum_2025}. Dabei zeigt sich, dass interdisziplinäre Zusammenarbeit zwischen Quanteninformatik, Finanzmathematik und Regulierung entscheidend für den Fortschritt ist.

Zusammenfassend lässt sich festhalten, dass Quantencomputing im Finanzbereich derzeit primär als exploratives Werkzeug für spezielle Teilprobleme dient. Das größte Anwendungspotenzial liegt dort, wo klassische Methoden an ihre Komplexitätsgrenzen stoßen. Ein produktiver Einsatz in der Praxis hängt jedoch maßgeblich von Fortschritten in der Hardware, der Fehlerkorrektur und der regulatorischen Anerkennung quantenbasierter Modelle ab.





\printbibliography





